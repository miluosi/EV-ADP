"""
PyTorch-based Value Function for ADP with Gym Integration

This module replaces the original Keras/TensorFlow implementation with PyTorch,
while maintaining the core ADP algorithm concepts.
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
import math
import gym
from gym import spaces
from typing import List, Tuple, Dict, Any, Optional
from abc import ABC, abstractmethod
from collections import deque
import random
import pickle
from pathlib import Path as PathlibPath
import logging

# Use fallback logger instead of tensorboard to avoid protobuf issues
TENSORBOARD_AVAILABLE = False
print("Using fallback logging instead of TensorBoard to avoid protobuf compatibility issues")

class SummaryWriter:
    """Fallback logger when TensorBoard is not available"""
    def __init__(self, *args, **kwargs):
        self.log_dir = args[0] if args else "logs"
        print(f"Fallback logger initialized for {self.log_dir}")
    
    def add_scalar(self, tag, value, step):
        print(f"[{step}] {tag}: {value:.4f}")
    
    def flush(self):
        pass
    
    def close(self):
        pass

# Import existing modules (modified for compatibility)
try:
    from LearningAgent import LearningAgent
    from Action import Action  
    from Environment import Environment
    from Experience import Experience
    from CentralAgent import CentralAgent
    from Request import Request
except ImportError:
    # Define placeholder classes if imports fail
    class LearningAgent: pass
    class Action: pass
    class Environment: pass
    class Experience: pass
    class CentralAgent: pass
    class Request: pass
    class Path: pass
    class Experience: pass
    class CentralAgent: pass
    class Request: pass


class PyTorchReplayBuffer:
    """PyTorch-based replay buffer for experience storage"""
    
    def __init__(self, capacity: int, device: str = 'cpu'):
        self.capacity = capacity
        self.device = torch.device(device)
        self.buffer = deque(maxlen=capacity)
        self.priorities = deque(maxlen=capacity)
        
    def add(self, experience: Experience, priority: float = 1.0):
        """Add experience to buffer"""
        self.buffer.append(experience)
        self.priorities.append(priority)
    
    def sample(self, batch_size: int, beta: float = 0.4):
        """Sample batch with prioritized sampling"""
        if len(self.buffer) < batch_size:
            return [], [], []
            
        # Convert priorities to probabilities
        priorities = np.array(self.priorities)
        probs = priorities ** 0.6  # Alpha parameter
        probs = probs / probs.sum()
        
        # Sample indices
        indices = np.random.choice(len(self.buffer), batch_size, p=probs)
        experiences = [self.buffer[i] for i in indices]
        
        # Importance sampling weights
        weights = (len(self.buffer) * probs[indices]) ** (-beta)
        weights = weights / weights.max()
        
        return experiences, weights, indices
    
    def update_priorities(self, indices: List[int], priorities: List[float]):
        """Update priorities for experiences"""
        for idx, priority in zip(indices, priorities):
            if 0 <= idx < len(self.priorities):
                self.priorities[idx] = priority + 1e-6
    
    def __len__(self):
        return len(self.buffer)


class PyTorchValueFunction(ABC, nn.Module):
    """Abstract base class for PyTorch-based value functions"""
    
    def __init__(self, log_dir: str, device: str = 'cpu'):
        super(PyTorchValueFunction, self).__init__()
        self.device = torch.device(device)
        
        # Setup logging
        log_path = PathlibPath(log_dir) / type(self).__name__
        log_path.mkdir(parents=True, exist_ok=True)
        self.writer = SummaryWriter(str(log_path))
        
        # Training statistics
        self.training_step = 0
        
    def add_to_logs(self, tag: str, value: float, step: int):
        """Add scalar to tensorboard logs"""
        self.writer.add_scalar(tag, value, step)
        self.writer.flush()
    
    @abstractmethod
    def get_value(self, experiences: List[Experience]) -> List[List[Tuple[Action, float]]]:
        """Get value estimates for experiences"""
        raise NotImplementedError
    
    @abstractmethod
    def update(self, central_agent: CentralAgent):
        """Update value function parameters"""
        raise NotImplementedError
    
    @abstractmethod
    def remember(self, experience: Experience):
        """Store experience for learning"""
        raise NotImplementedError


class PyTorchRewardPlusDelay(PyTorchValueFunction):
    """Simple reward + delay value function (no learning required)"""
    
    def __init__(self, delay_coefficient: float, log_dir: str = "logs/reward_plus_delay", device: str = 'cpu'):
        super().__init__(log_dir=log_dir, device=device)
        self.delay_coefficient = delay_coefficient
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"Initialized PyTorchRewardPlusDelay with delay_coefficient={delay_coefficient}")
    
    def get_value(self, experiences: List[Experience]) -> List[List[Tuple[Action, float]]]:
        """Compute value as immediate reward plus delay bonus"""
        scored_actions_all_agents = []
        
        for experience in experiences:
            for feasible_actions in experience.feasible_actions_all_agents:
                scored_actions = []
                for action in feasible_actions:
                    if hasattr(action, 'new_path') and action.new_path:
                        immediate_reward = sum([getattr(request, 'value', 0) 
                                              for request in getattr(action, 'requests', [])])
                        delay_bonus = self.delay_coefficient * getattr(action.new_path, 'total_delay', 0)
                        score = immediate_reward + delay_bonus
                    else:
                        score = 0.0
                    
                    scored_actions.append((action, score))
                scored_actions_all_agents.append(scored_actions)
        
        return scored_actions_all_agents
    
    def get_q_value(self, vehicle_id: int, action_type: str, vehicle_location: int, 
                   target_location: int, current_time: float = 0.0) -> float:
        """
        Simple Q-value calculation for ChargingIntegratedEnvironment
        
        Args:
            vehicle_id: ID of the vehicle
            action_type: Type of action ('assign', 'charge', 'move', 'idle')
            vehicle_location: Current location of vehicle
            target_location: Target location (request pickup or charging station)
            current_time: Current simulation time
            
        Returns:
            Q-value for the state-action pair
        """
        # Base reward calculation
        base_reward = 0.0
        
        # Distance penalty
        if hasattr(self, '_calculate_distance'):
            distance = self._calculate_distance(vehicle_location, target_location)
        else:
            # Simple Manhattan distance approximation
            distance = abs(vehicle_location - target_location)
        
        distance_penalty = distance * 0.1
        
        # Action-specific rewards
        if action_type.startswith('assign'):
            # Passenger service reward
            base_reward = 10.0  # Base reward for serving passenger
            # Add urgency bonus based on delay
            urgency_bonus = self.delay_coefficient * max(0, 300 - current_time)  # 300 is max delay
            base_reward += urgency_bonus
            
        elif action_type.startswith('charge'):
            # Charging reward - depends on battery level (if available)
            base_reward = 5.0  # Base charging benefit
            # Negative delay penalty for charging (opportunity cost)
            charging_delay_penalty = self.delay_coefficient * 30  # Assume 30 time units charging
            base_reward -= charging_delay_penalty
            
        elif action_type == 'move':
            # Movement has small cost
            base_reward = -1.0
            
        elif action_type == 'idle':
            # Idle has minimal cost
            base_reward = -0.5
        
        # Final Q-value calculation
        q_value = base_reward - distance_penalty
        
        return float(q_value)
    
    def _calculate_distance(self, loc1: int, loc2: int, grid_size: int = 10) -> float:
        """Calculate Manhattan distance between two locations"""
        x1, y1 = loc1 // grid_size, loc1 % grid_size
        x2, y2 = loc2 // grid_size, loc2 % grid_size
        return abs(x1 - x2) + abs(y1 - y2)
    
    def get_assignment_q_value(self, vehicle_id: int, target_id: int, 
                              vehicle_location: int, target_location: int, 
                              current_time: float = 0.0) -> float:
        """Get Q-value for vehicle assignment to request"""
        return self.get_q_value(vehicle_id, f"assign_{target_id}", 
                               vehicle_location, target_location, current_time)
    
    def get_charging_q_value(self, vehicle_id: int, station_id: int,
                           vehicle_location: int, station_location: int,
                           current_time: float = 0.0) -> float:
        """Get Q-value for vehicle charging decision"""
        return self.get_q_value(vehicle_id, f"charge_{station_id}",
                               vehicle_location, station_location, current_time)

    def update(self, *args, **kwargs):
        """No learning required for this value function"""
        pass
    
    def remember(self, *args, **kwargs):
        """No experience storage required"""
        pass


class PyTorchChargingValueFunction(PyTorchValueFunction):
    """Neural network-based value function for ChargingIntegratedEnvironment using PyTorchPathBasedNetwork"""
    
    def __init__(self, grid_size: int = 10, num_vehicles: int = 8, 
                 log_dir: str = "logs/charging_nn", device: str = 'cpu',
                 episode_length: int = 300, max_requests: int = 1000, env=None):
        super().__init__(log_dir=log_dir, device=device)
        
        self.grid_size = grid_size
        self.num_vehicles = num_vehicles
        self.episode_length = episode_length  # å®é™…episodeé•¿åº¦
        self.max_requests = max_requests      # æœ€å¤§é¢„æœŸè¯·æ±‚æ•°
        self.num_locations = grid_size * grid_size
        self.env = env  # å­˜å‚¨ç¯å¢ƒå¼•ç”¨
        
        # Initialize the neural network with increased capacity for complex environment
        self.network = PyTorchPathBasedNetwork(
            num_locations=self.num_locations,
            num_vehicles=num_vehicles,  # æ·»åŠ è½¦è¾†æ•°é‡å‚æ•°
            max_capacity=6,  # Increased capacity for longer paths
            embedding_dim=128,  # Larger embedding for complex environment
            lstm_hidden=256,   # Larger LSTM for complex patterns
            dense_hidden=512,   # Larger dense layer
            pretrained_embeddings=None  # Explicitly set to None to ensure gradients
        ).to(self.device)
        
        # Target network for stable DQN training
        self.target_network = PyTorchPathBasedNetwork(
            num_locations=self.num_locations,
            num_vehicles=num_vehicles,  # æ·»åŠ è½¦è¾†æ•°é‡å‚æ•°
            max_capacity=6,
            embedding_dim=128,
            lstm_hidden=256,
            dense_hidden=512,
            pretrained_embeddings=None
        ).to(self.device)
        
        # Copy weights from main network to target network
        self.target_network.load_state_dict(self.network.state_dict())
        self.target_update_frequency = 500  # Update target network every 100 steps
        
        # Ensure all parameters require gradients
        total_params = 0
        grad_params = 0
        for name, param in self.network.named_parameters():
            param.requires_grad = True
            total_params += 1
            if param.requires_grad:
                grad_params += 1
        
        print(f"   Parameters requiring gradients: {grad_params}/{total_params}")
        
        # Optimizer for training - reduced learning rate for stable learning
        self.optimizer = optim.Adam(self.network.parameters(), lr=2e-3, weight_decay=1e-5)
        self.loss_fn = nn.MSELoss()
        
        # ä¿®å¤å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼šæ›´ä¿å®ˆçš„è®¾ç½®ï¼Œé¿å…å­¦ä¹ ç‡è¿‡å¿«ä¸‹é™
        # åŸè®¾ç½®ï¼šfactor=0.7, patience=50, min_lr=1e-4 å¤ªæ¿€è¿›
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.9, patience=200, 
            min_lr=1e-3, verbose=True  # ä¿æŒæœ€å°å­¦ä¹ ç‡ä¸º1e-3ï¼Œé¿å…è¿‡åº¦é™ä½
        )
        
        # Training data buffer - increased size for more diverse experiences
        self.experience_buffer = deque(maxlen=50000)  # Doubled buffer size
        
        # Training metrics tracking
        self.training_losses = []
        self.q_values_history = []
        self.training_step = 0
        
        # Debug mode for detailed logging
        self.debug_mode = False  # è®¾ç½®ä¸ºFalseä»¥é¿å…è¿‡å¤šè¾“å‡ºï¼Œå¯ä»¥é€šè¿‡å¤–éƒ¨è®¾ç½®ä¸ºTrue
        
        # åˆå§‹åŒ–æ‹’ç»æ¦‚ç‡å­¦ä¹ ç½‘ç»œ
        self._init_rejection_predictor()
        
        print(f"âœ“ PyTorchChargingValueFunction initialized with neural network")
        print(f"   - Grid size: {grid_size}x{grid_size}")
        print(f"   - Network parameters: {sum(p.numel() for p in self.network.parameters())}")
        print(f"   - Rejection predictor parameters: {sum(p.numel() for p in self.rejection_predictor.parameters())}")
    
    def _init_rejection_predictor(self):
        """åˆå§‹åŒ–æ‹’ç»æ¦‚ç‡é¢„æµ‹ç¥ç»ç½‘ç»œ"""
        class RejectionPredictor(nn.Module):
            def __init__(self, input_dim=5, hidden_dim=64):
                super().__init__()
                self.network = nn.Sequential(
                    nn.Linear(input_dim, hidden_dim),
                    nn.ReLU(),
                    
                    nn.Linear(hidden_dim, hidden_dim // 2),
                    nn.ReLU(),
                    
                    nn.Linear(hidden_dim // 2, 1),
                    nn.Sigmoid()  # è¾“å‡º0-1ä¹‹é—´çš„æ‹’ç»æ¦‚ç‡
                )
            
            def forward(self, x):
                return self.network(x)
        
        self.rejection_predictor = RejectionPredictor().to(self.device)
        self.rejection_optimizer = optim.Adam(self.rejection_predictor.parameters(), lr=1e-3)
        self.rejection_criterion = nn.BCELoss()
        
        # æ‹’ç»æ•°æ®ç¼“å†²åŒº
        self.rejection_buffer = deque(maxlen=5000)
        self.rejection_training_losses = []
    
    def get_q_value(self, vehicle_id: int, action_type: str, vehicle_location: int, 
                   target_location: int, current_time: float = 0.0, 
                   other_vehicles: int = 0, num_requests: int = 0, 
                   battery_level: float = 1.0, request_value: float = 0.0) -> float:
        """
        Neural network-based Q-value calculation using PyTorchPathBasedNetwork
        ç°åœ¨æ”¯æŒvehicle_idã€battery_levelã€request_valueã€action_typeä»¥åŠğŸ†• target locationå’Œzone_idä¿¡æ¯
        """
        # å°†action_typeå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•°å€¼ç¼–ç 
        if action_type == 'idle':
            action_type_id = 1
        elif action_type.startswith('assign'):
            action_type_id = 2
        elif action_type.startswith('charge'):
            action_type_id = 3
        else:
            action_type_id = 2  # é»˜è®¤ä¸ºassign
        
        # ä»Environmentä¸­è·å–è½¦è¾†ç±»å‹ï¼ˆéœ€è¦ä»å¤–éƒ¨ä¼ å…¥æˆ–è€…æ¨æ–­ï¼‰
        # å‡è®¾vehicle_idä¸ºå¶æ•°æ˜¯EVï¼Œå¥‡æ•°æ˜¯AEVï¼ˆç®€åŒ–å¤„ç†ï¼‰
        # å®é™…åº”ç”¨ä¸­åº”è¯¥ä»ç¯å¢ƒæˆ–é…ç½®ä¸­è·å–
        vehicle_type_id = 1 if vehicle_id % 2 == 0 else 2  # 1=EV, 2=AEV
        
        # ğŸ†• è®¡ç®—target locationçš„æ›¼å“ˆé¡¿è·ç¦»å¹¶å½’ä¸€åŒ–
        if hasattr(self, 'env') and self.env is not None and hasattr(self.env, '_manhattan_distance_loc'):
            manhattan_distance = self.env._manhattan_distance_loc(vehicle_location, target_location)
        else:
            # Fallback: æ‰‹åŠ¨è®¡ç®—
            grid_size = self.grid_size if hasattr(self, 'grid_size') else int(math.sqrt(self.num_locations))
            vehicle_x = vehicle_location % grid_size
            vehicle_y = vehicle_location // grid_size
            target_x = target_location % grid_size
            target_y = target_location // grid_size
            manhattan_distance = abs(vehicle_x - target_x) + abs(vehicle_y - target_y)
        normalized_distance = manhattan_distance
        
        # ğŸ†• ä»environmentè·å–target locationçš„zone_id
        target_zoneid = 0  # é»˜è®¤å€¼
        if hasattr(self, 'env') and self.env is not None and hasattr(self.env, 'get_zone_id'):
            target_zoneid = self.env.get_zone_id(target_location)
        
        # ä½¿ç”¨æ”¯æŒbatteryã€request_valueã€target_distanceå’Œtarget_zoneidçš„è¾“å…¥å‡†å¤‡æ–¹æ³•
        inputs = self._prepare_network_input_with_battery(
            vehicle_location, target_location, current_time, 
            other_vehicles, num_requests, action_type, battery_level, request_value,
            normalized_distance, target_zoneid  # ğŸ†• ä¼ å…¥targetä¿¡æ¯
        )
        
        # å¤„ç†è¿”å›çš„è¾“å…¥ï¼ˆåŒ…å«batteryã€request_valueã€target_distanceå’Œtarget_zoneidï¼‰
        if len(inputs) == 9:  # ğŸ†• å®Œæ•´è¾“å…¥ï¼šåŒ…å«æ‰€æœ‰ç‰¹å¾
            path_locations, path_delays, time_tensor, others_tensor, requests_tensor, battery_tensor, value_tensor, distance_tensor, zoneid_tensor = inputs
        elif len(inputs) == 7:  # å‘åå…¼å®¹ï¼šåŒ…å«batteryå’Œrequest_value
            path_locations, path_delays, time_tensor, others_tensor, requests_tensor, battery_tensor, value_tensor = inputs
            distance_tensor = torch.tensor([[normalized_distance]], dtype=torch.float32).to(self.device)
            zoneid_tensor = torch.tensor([[target_zoneid]], dtype=torch.long).to(self.device)
        elif len(inputs) == 6:  # åªåŒ…å«battery
            path_locations, path_delays, time_tensor, others_tensor, requests_tensor, battery_tensor = inputs
            value_tensor = torch.tensor([[request_value]], dtype=torch.float32).to(self.device)
            distance_tensor = torch.tensor([[normalized_distance]], dtype=torch.float32).to(self.device)
            zoneid_tensor = torch.tensor([[target_zoneid]], dtype=torch.long).to(self.device)
        else:  # ä¸åŒ…å«batteryï¼ˆå‘åå…¼å®¹ï¼‰
            path_locations, path_delays, time_tensor, others_tensor, requests_tensor = inputs
            battery_tensor = torch.tensor([[battery_level]], dtype=torch.float32).to(self.device)
            value_tensor = torch.tensor([[request_value]], dtype=torch.float32).to(self.device)
            distance_tensor = torch.tensor([[normalized_distance]], dtype=torch.float32).to(self.device)
            zoneid_tensor = torch.tensor([[target_zoneid]], dtype=torch.long).to(self.device)
        
        # åˆ›å»ºvehicleå’Œactionç›¸å…³çš„tensors
        action_type_tensor = torch.tensor([[action_type_id]], dtype=torch.long).to(self.device)
        vehicle_id_tensor = torch.tensor([[vehicle_id + 1]], dtype=torch.long).to(self.device)  # +1å› ä¸º0æ˜¯padding
        vehicle_type_tensor = torch.tensor([[vehicle_type_id]], dtype=torch.long).to(self.device)
        
        # Forward pass through network
        self.network.eval()
        with torch.no_grad():
            q_value = self.network(
                path_locations=path_locations,
                path_delays=path_delays,
                current_time=time_tensor,
                other_agents=others_tensor,
                num_requests=requests_tensor,
                battery_level=battery_tensor,
                request_value=value_tensor,
                target_distance=distance_tensor,      # ğŸ†• ä¼ å…¥targetè·ç¦»
                target_zoneid=zoneid_tensor,          # ğŸ†• ä¼ å…¥target zone_id
                action_type=action_type_tensor,
                vehicle_id=vehicle_id_tensor,
                vehicle_type=vehicle_type_tensor
            )
            
            # Apply clipping to prevent extreme Q-values that can dominate the objective
            raw_q_value = float(q_value.item())
            
            # Clip Q-values to reasonable range to prevent optimization instability
            # This ensures Q-values don't overwhelm request values in the objective function
            # clipped_q_value = max(-50.0, min(50.0, raw_q_value))
            
            #if abs(raw_q_value - clipped_q_value) > 1e-6:  # Only log if actual clipping occurred
                #print(f"Q-value clipped: {raw_q_value:.3f} -> {clipped_q_value:.3f} for action {action_type}")
            
            return raw_q_value
    
    def _prepare_network_input(self, vehicle_location: int, target_location: int, 
                              current_time: float, other_vehicles: int, num_requests: int,
                              action_type: str):
        """Prepare input tensors for the neural network"""
        # Create path sequence: current location -> target location
        path_locations = torch.zeros(1, 3, dtype=torch.long)  # batch_size=1, seq_len=3
        path_delays = torch.zeros(1, 3, 1, dtype=torch.float32)
        
        # Set path: current -> target -> end (with boundary checking)
        # Handle coordinate tuples or integer indices
        def _convert_location_to_index(location):
            if isinstance(location, tuple) and len(location) == 2:
                # Convert coordinate tuple to location index
                x, y = location
                grid_size = int(self.num_locations ** 0.5)  # Assuming square grid
                return y * grid_size + x
            elif isinstance(location, int):
                return location
            else:
                # Fallback for unexpected types (silent handling)
                if location is None:
                    return 0
                return 0
        
        # Convert locations to indices and ensure they are within valid range [0, num_locations-1]
        safe_vehicle_location = max(0, min(_convert_location_to_index(vehicle_location), self.num_locations - 1))
        safe_target_location = max(0, min(_convert_location_to_index(target_location), self.num_locations - 1))
        
        # Debug: Log if we had to clamp any values
        # if vehicle_location != safe_vehicle_location or target_location != safe_target_location:
        #     print(f"WARNING: Clamped location indices - vehicle: {vehicle_location}->{safe_vehicle_location}, target: {target_location}->{safe_target_location}, max_allowed: {self.num_locations-1}")
        
        path_locations[0, 0] = safe_vehicle_location + 1  # +1 because 0 is padding
        path_locations[0, 1] = safe_target_location + 1
        path_locations[0, 2] = 0  # End token
        
        # Set delays based on action type
        if action_type.startswith('assign'):
            # Passenger service - delays based on urgency
            path_delays[0, 0, 0] = 0.0  # No delay at current location
            path_delays[0, 1, 0] = max(0.0, (self.episode_length - current_time) / self.episode_length)  # Normalized urgency
        elif action_type.startswith('charge'):
            # Charging action - charging time penalty
            path_delays[0, 0, 0] = 0.0
            path_delays[0, 1, 0] = 0.8  # High delay for charging (opportunity cost)
        else:
            # Movement or idle
            path_delays[0, 0, 0] = 0.0
            path_delays[0, 1, 0] = 0.1  # Small delay for movement
        
        # Normalize time (0-1 range)
        time_tensor = torch.tensor([[current_time / self.episode_length]], dtype=torch.float32)
        
        # Normalize other metrics
        others_tensor = torch.tensor([[min(other_vehicles, self.num_vehicles) / self.num_vehicles]], dtype=torch.float32)
        requests_tensor = torch.tensor([[min(num_requests, self.max_requests) / self.max_requests]], dtype=torch.float32)
        

        
        # Move to device
        return (path_locations.to(self.device), 
                path_delays.to(self.device),
                time_tensor.to(self.device),
                others_tensor.to(self.device),
                requests_tensor.to(self.device))
    
    def validate_normalization_params(self):
        """éªŒè¯å½’ä¸€åŒ–å‚æ•°çš„åˆç†æ€§"""
        print("=== Normalization Parameters Validation ===")
        print(f"Grid size: {self.grid_size}")
        print(f"Number of vehicles: {self.num_vehicles}")
        print(f"Episode length: {self.episode_length}")
        print(f"Max requests: {self.max_requests}")
        print(f"Number of locations: {self.num_locations}")
        
        # æ£€æŸ¥å‚æ•°åˆç†æ€§
        issues = []
        if self.episode_length <= 0:
            issues.append("Episode length must be positive")
        if self.num_vehicles <= 0:
            issues.append("Number of vehicles must be positive")
        if self.max_requests <= 0:
            issues.append("Max requests must be positive")
            
        if issues:
            print("âš ï¸ Issues found:")
            for issue in issues:
                print(f"  - {issue}")
        else:
            print("âœ“ All normalization parameters are valid")
        print("=" * 45)
    
    def _prepare_network_input_with_battery(self, vehicle_location: int, target_location: int, 
                                           current_time: float, other_vehicles: int, 
                                           num_requests: int, action_type: str, 
                                           battery_level: float = 1.0, request_value: float = 0.0,
                                           target_distance: float = None, target_zoneid: int = None):
        """
        Prepare input tensors for the neural network including battery, request value, and ğŸ†• target information
        
        Args:
            vehicle_location: è½¦è¾†å½“å‰ä½ç½®
            target_location: ç›®æ ‡ä½ç½®
            current_time: å½“å‰æ—¶é—´
            other_vehicles: é™„è¿‘å…¶ä»–è½¦è¾†æ•°é‡
            num_requests: å½“å‰è¯·æ±‚æ•°é‡
            action_type: åŠ¨ä½œç±»å‹
            battery_level: ç”µæ± ç”µé‡ (0-1)
            request_value: è¯·æ±‚ä»·å€¼ (åªå¯¹assignåŠ¨ä½œæœ‰æ•ˆ)
            target_distance: ğŸ†• åˆ°targetçš„æ›¼å“ˆé¡¿è·ç¦»ï¼ˆå·²å½’ä¸€åŒ–ï¼‰
            target_zoneid: ğŸ†• target locationçš„zone ID
        """
        # éªŒè¯å’Œä¿®æ­£è¾“å…¥å€¼ - å¤„ç†Noneæˆ–æ— æ•ˆå€¼ï¼ˆé™é»˜å¤„ç†ï¼‰
        if vehicle_location is None:
            vehicle_location = 0  # é»˜è®¤å€¼
        if target_location is None:
            target_location = vehicle_location if vehicle_location is not None else 0  # ä½¿ç”¨å½“å‰ä½ç½®ä½œä¸ºé»˜è®¤å€¼
        
        # ç¡®ä¿ä½ç½®å€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…
        vehicle_location = int(vehicle_location) if isinstance(vehicle_location, (int, float)) else 0
        target_location = int(target_location) if isinstance(target_location, (int, float)) else vehicle_location
        
        # æ ¹æ®åŠ¨ä½œç±»å‹é€‰æ‹©åˆé€‚çš„è¾“å…¥å‡†å¤‡æ–¹æ³•
        if action_type == 'idle':
            # å¯¹äºidleçŠ¶æ€ï¼Œå¤„ç†ç›®æ ‡ä½ç½®ä¸ºå½“å‰ä½ç½®
            path_locations = torch.zeros(1, 3, dtype=torch.long)  # batch_size=1, seq_len=3
            path_delays = torch.zeros(1, 3, 1, dtype=torch.float32)
            
            # è®¾ç½®è·¯å¾„ï¼šå½“å‰ä½ç½® -> å½“å‰ä½ç½®ï¼ˆè¡¨ç¤ºåœç•™ï¼‰-> ç»“æŸ (with boundary checking)
            # Ensure indices are within valid range [0, num_locations-1]
            safe_vehicle_location = max(0, min(vehicle_location, self.num_locations - 1))
            
            path_locations[0, 0] = safe_vehicle_location + 1  # +1 because 0 is padding
            path_locations[0, 1] = safe_vehicle_location + 1  # åŒæ ·çš„ä½ç½®è¡¨ç¤ºidle
            path_locations[0, 2] = 0  # End token
            
            # è®¾ç½®å»¶è¿Ÿ - idleçŠ¶æ€çš„å»¶è¿Ÿæ¨¡å¼
            path_delays[0, 0, 0] = 0.0  # å½“å‰ä½ç½®æ— å»¶è¿Ÿ
            path_delays[0, 1, 0] = 0.05  # idleçš„å°å»¶è¿Ÿï¼ˆç­‰å¾…æˆæœ¬ï¼‰
            path_delays[0, 2, 0] = 0.0  # ç»“æŸä½ç½®æ— å»¶è¿Ÿ
            
            # å½’ä¸€åŒ–æ—¶é—´ (0-1 range)
            time_tensor = torch.tensor([[current_time / self.episode_length]], dtype=torch.float32)
            
            # å½’ä¸€åŒ–å…¶ä»–æŒ‡æ ‡
            others_tensor = torch.tensor([[min(other_vehicles, self.num_vehicles) / self.num_vehicles]], dtype=torch.float32)
            requests_tensor = torch.tensor([[min(num_requests, self.max_requests) / self.max_requests]], dtype=torch.float32)
            

            
            # å½’ä¸€åŒ–ç”µæ± ç”µé‡
            battery_tensor = torch.tensor([[battery_level]], dtype=torch.float32)
            
            # å½’ä¸€åŒ–è¯·æ±‚ä»·å€¼ (å¯¹idleåŠ¨ä½œï¼Œrequest_valueåº”è¯¥ä¸º0)
            value_tensor = torch.tensor([[request_value / 100.0]], dtype=torch.float32)  # å‡è®¾æœ€å¤§ä»·å€¼100
            
            # ğŸ†• Targetä¿¡æ¯ (å¯¹idleåŠ¨ä½œï¼Œtarget_distanceå’Œtarget_zoneidä¸º0)
            # å½’ä¸€åŒ–distance: å‡è®¾æœ€å¤§æ›¼å“ˆé¡¿è·ç¦»ä¸º20ï¼ˆ10x10ç½‘æ ¼çš„å¯¹è§’çº¿è·ç¦»çº¦18ï¼‰
            normalized_distance = (target_distance if target_distance is not None else 0.0) 
            distance_tensor = torch.tensor([[normalized_distance]], dtype=torch.float32)
            zoneid_tensor = torch.tensor([[target_zoneid if target_zoneid is not None else 0]], dtype=torch.long)
            
            # Move to device
            return (path_locations.to(self.device), 
                    path_delays.to(self.device),
                    time_tensor.to(self.device),
                    others_tensor.to(self.device),
                    requests_tensor.to(self.device),
                    battery_tensor.to(self.device),
                    value_tensor.to(self.device),
                    distance_tensor.to(self.device),      # ğŸ†•
                    zoneid_tensor.to(self.device))        # ğŸ†•
        else:
            # å¯¹äºéidleåŠ¨ä½œï¼Œä½¿ç”¨æ ‡å‡†æ–¹æ³•å¹¶æ·»åŠ batteryå’Œrequest_valueä¿¡æ¯
            path_locations, path_delays, time_tensor, others_tensor, requests_tensor = self._prepare_network_input(
                vehicle_location, target_location, current_time, 
                other_vehicles, num_requests, action_type
            )
            
            # æ·»åŠ batteryä¿¡æ¯
            battery_tensor = torch.tensor([[battery_level]], dtype=torch.float32).to(self.device)
            
            # æ·»åŠ request_valueä¿¡æ¯ (å½’ä¸€åŒ–)
            normalized_value = request_value / 100.0 if action_type.startswith('assign') else 0.0
            value_tensor = torch.tensor([[normalized_value]], dtype=torch.float32).to(self.device)
            
            # ğŸ†• æ·»åŠ targetä¿¡æ¯ - å½’ä¸€åŒ–distance
            # å½’ä¸€åŒ–distance: å‡è®¾æœ€å¤§æ›¼å“ˆé¡¿è·ç¦»ä¸º20ï¼ˆ10x10ç½‘æ ¼çš„å¯¹è§’çº¿è·ç¦»çº¦18ï¼‰
            normalized_distance = (target_distance if target_distance is not None else 0.0) 
            distance_tensor = torch.tensor([[normalized_distance]], dtype=torch.float32).to(self.device)
            zoneid_tensor = torch.tensor([[target_zoneid if target_zoneid is not None else 0]], dtype=torch.long).to(self.device)
            
            return (path_locations, path_delays, time_tensor, 
                   others_tensor, requests_tensor, battery_tensor, value_tensor,
                   distance_tensor, zoneid_tensor)  # ğŸ†•
    
    def get_assignment_q_value(self, vehicle_id: int, target_id: int, 
                              vehicle_location: int, target_reject: int, target_location: int, 
                              current_time: float = 0.0, other_vehicles: int = 0, 
                              num_requests: int = 0, battery_level: float = 1.0,
                              request_value: float = 0.0) -> float:
        """
        Enhanced Q-value for vehicle assignment to request using neural network
        ç°åœ¨åŒ…å«æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œä¼˜åŒ–çš„è®¡ç®—é€»è¾‘ï¼Œä»¥åŠEVæ‹’ç»å­¦ä¹ æœºåˆ¶
        """
        # åŸºç¡€Qå€¼è®¡ç®—
        base_q_value = self.get_q_value(vehicle_id, f"assign_{target_id}", 
                                       vehicle_location, target_location, current_time, 
                                       other_vehicles, num_requests, battery_level, request_value)
        
        # ä¸ºEVè½¦è¾†æ·»åŠ è·ç¦»æƒ©ç½šå’Œæ‹’ç»é£é™©è¯„ä¼°
        # if hasattr(self, 'env') and self.env is not None:
        #     vehicle = self.env.vehicles.get(vehicle_id)
        #     if vehicle and vehicle.get('type') == 1:  # EVè½¦è¾†
        #         # è®¡ç®—åˆ°æ¥å®¢ç‚¹çš„è·ç¦»ï¼ˆå‡è®¾target_idå¯¹åº”pickupä½ç½®ï¼‰
        #         grid_size = self.env.grid_size if hasattr(self.env, 'grid_size') else int(math.sqrt(max(vehicle_location, target_reject)) + 1)
        #         distance = self._calculate_manhattan_distance(vehicle_location, target_reject, grid_size)
        #         print(f"Vehicle {vehicle_id} (EV) distance to request {target_id}: {distance}")
        #         # è·ç¦»æƒ©ç½šï¼šè·ç¦»è¶Šè¿œï¼ŒQå€¼è¶Šä½
        #         distance_penalty = distance * 0.15  # å¯è°ƒèŠ‚çš„è·ç¦»æƒ©ç½šå› å­
                
        #         # æ‹’ç»é£é™©æƒ©ç½šï¼šåŸºäºå†å²ç»éªŒå­¦ä¹ çš„æ‹’ç»æ¦‚ç‡
        #         rejection_penalty = self._calculate_rejection_risk_penalty(vehicle_id, distance)
                
        #         base_q_value = base_q_value - distance_penalty - rejection_penalty
                
        
        # è¿”å›è°ƒæ•´åçš„Qå€¼
        return base_q_value 
    
    def batch_get_assignment_q_value(self, batch_inputs):
        """
        æ‰¹é‡è®¡ç®—å¤šä¸ªvehicle-requestå¯¹çš„Qå€¼ï¼Œæé«˜è®¡ç®—æ•ˆç‡
        
        Args:
            batch_inputs: List of input dictionaries, each containing:
                - vehicle_id, target_id, vehicle_location, target_reject, target_location,
                - current_time, other_vehicles, num_requests, battery_level, request_value
                
        Returns:
            List of Q-values corresponding to each input
        """
        if not batch_inputs:
            return []
        
        # å‡†å¤‡æ‰¹é‡æ•°æ®
        batch_size = len(batch_inputs)
        
        # æ”¶é›†æ‰€æœ‰è¾“å…¥æ•°æ®
        vehicle_ids = []
        target_ids = []
        vehicle_locations = []
        target_locations = []
        current_times = []
        other_vehicles_list = []
        num_requests_list = []
        battery_levels = []
        request_values = []
        pickupdistances = []
        pickzoneids = []
        for input_data in batch_inputs:
            vehicle_ids.append(input_data['vehicle_id'])
            target_ids.append(input_data['target_id'])
            vehicle_locations.append(input_data['vehicle_location'])
            target_locations.append(input_data['target_location'])
            current_times.append(input_data.get('current_time', 0.0))
            other_vehicles_list.append(input_data.get('other_vehicles', 0))
            num_requests_list.append(input_data.get('num_requests', 0))
            battery_levels.append(input_data.get('battery_level', 1.0))
            request_values.append(input_data.get('request_value', 0.0))
            pickupdistances.append(input_data.get('pickup_dist', 0.0))
            pickzoneids.append(input_data.get('pick_zone', 0))
        
        # æ‰¹é‡å‡†å¤‡ç¥ç»ç½‘ç»œè¾“å…¥
        try:
            batch_network_inputs = []
            for i in range(batch_size):
                action_type = f"assign_{target_ids[i]}"
                # ğŸ†• è®¡ç®—target distanceï¼ˆå¦‚æœæœªæä¾›ï¼‰
                target_distance = pickupdistances[i]
                if target_distance == 0.0:
                    if hasattr(self, 'env') and self.env is not None and hasattr(self.env, '_manhattan_distance_loc'):
                        manhattan_dist = self.env._manhattan_distance_loc(vehicle_locations[i], target_locations[i])
                    elif hasattr(self, 'grid_size'):
                        # Fallback: æ‰‹åŠ¨è®¡ç®—
                        grid_size = self.grid_size
                        v_x = vehicle_locations[i] % grid_size
                        v_y = vehicle_locations[i] // grid_size
                        t_x = target_locations[i] % grid_size
                        t_y = target_locations[i] // grid_size
                        manhattan_dist = abs(v_x - t_x) + abs(v_y - t_y)
                    else:
                        manhattan_dist = 0
                    target_distance = manhattan_dist
                
                # ğŸ†• è·å–target zone_idï¼ˆå¦‚æœæœªæä¾›ï¼‰
                target_zoneid = pickzoneids[i]
                if target_zoneid == 0 and hasattr(self, 'env') and self.env is not None:
                    if hasattr(self.env, 'get_zone_id'):
                        target_zoneid = self.env.get_zone_id(target_locations[i])
                
                network_input = self._prepare_network_input_with_battery(
                    vehicle_locations[i], target_locations[i], current_times[i],
                    other_vehicles_list[i], num_requests_list[i], action_type,
                    battery_levels[i], request_values[i],
                    target_distance, target_zoneid  # ğŸ†• ä¼ å…¥targetä¿¡æ¯
                )
                batch_network_inputs.append(network_input)
            
            # æ‰¹é‡è½¬æ¢ä¸ºå¼ é‡
            batch_tensors = self._batch_prepare_tensors(batch_network_inputs)
            
            # ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œæ‰¹é‡å‰å‘ä¼ æ’­
            with torch.no_grad():
                batch_q_values = self.network(
                    path_locations=batch_tensors['path_locations'],
                    path_delays=batch_tensors['path_delays'],
                    current_time=batch_tensors['current_time'],
                    other_agents=batch_tensors['other_agents'],
                    num_requests=batch_tensors['num_requests'],
                    battery_level=batch_tensors['battery_level'],
                    request_value=batch_tensors['request_value'],
                    target_distance=batch_tensors['target_distance'],  # ğŸ†•
                    target_zoneid=batch_tensors['target_zoneid'],      # ğŸ†•
                    action_type=None,  # è®©ç½‘ç»œè‡ªåŠ¨æ¨æ–­
                    vehicle_id=None,   # æ‰¹é‡å¤„ç†æ—¶ä¸ä½¿ç”¨vehicle_id embedding
                    vehicle_type=None  # æ‰¹é‡å¤„ç†æ—¶ä¸ä½¿ç”¨vehicle_type embedding
                )
            
            # è½¬æ¢ä¸ºPythonåˆ—è¡¨
            q_values = batch_q_values.cpu().numpy().flatten().tolist()
            
            return q_values
            
        except Exception as e:
            print(f"Batch Q-value calculation failed: {e}")
            # å›é€€åˆ°å•ç‹¬è®¡ç®—
            return [self.get_assignment_q_value(**input_data) for input_data in batch_inputs]
    
    def _batch_prepare_tensors(self, batch_network_inputs):
        """
        å°†æ‰¹é‡ç½‘ç»œè¾“å…¥è½¬æ¢ä¸ºé€‚åˆæ‰¹é‡å¤„ç†çš„å¼ é‡
        """
        batch_size = len(batch_network_inputs)
        
        # åˆå§‹åŒ–æ‰¹é‡å¼ é‡
        batch_tensors = {}
        
        # è·å–ç¬¬ä¸€ä¸ªè¾“å…¥çš„ç»´åº¦ä¿¡æ¯
        first_input = batch_network_inputs[0]
        if len(first_input) >= 9:  # ğŸ†• åŒ…å«battery, request_value, target_distance, target_zoneid
            path_locations_list = []
            path_delays_list = []
            current_time_list = []
            other_agents_list = []
            num_requests_list = []
            battery_level_list = []
            request_value_list = []
            target_distance_list = []  # ğŸ†•
            target_zoneid_list = []    # ğŸ†•
            
            for network_input in batch_network_inputs:
                path_locations, path_delays, current_time, other_agents, num_requests, battery_level, request_value, target_distance, target_zoneid = network_input
                path_locations_list.append(path_locations.squeeze(0))
                path_delays_list.append(path_delays.squeeze(0))
                current_time_list.append(current_time.squeeze(0))
                other_agents_list.append(other_agents.squeeze(0))
                num_requests_list.append(num_requests.squeeze(0))
                battery_level_list.append(battery_level.squeeze(0))
                request_value_list.append(request_value.squeeze(0))
                target_distance_list.append(target_distance.squeeze(0))  # ğŸ†•
                target_zoneid_list.append(target_zoneid.squeeze(0))      # ğŸ†•
            
            # å †å ä¸ºæ‰¹é‡å¼ é‡
            batch_tensors['path_locations'] = torch.stack(path_locations_list)
            batch_tensors['path_delays'] = torch.stack(path_delays_list)
            batch_tensors['current_time'] = torch.stack(current_time_list)
            batch_tensors['other_agents'] = torch.stack(other_agents_list)
            batch_tensors['num_requests'] = torch.stack(num_requests_list)
            batch_tensors['battery_level'] = torch.stack(battery_level_list)
            batch_tensors['request_value'] = torch.stack(request_value_list)
            batch_tensors['target_distance'] = torch.stack(target_distance_list)  # ğŸ†•
            batch_tensors['target_zoneid'] = torch.stack(target_zoneid_list)      # ğŸ†•
        elif len(first_input) >= 7:  # å‘åå…¼å®¹ï¼šåªæœ‰batteryå’Œrequest_value
            path_locations_list = []
            path_delays_list = []
            current_time_list = []
            other_agents_list = []
            num_requests_list = []
            battery_level_list = []
            request_value_list = []
            
            for network_input in batch_network_inputs:
                path_locations, path_delays, current_time, other_agents, num_requests, battery_level, request_value = network_input
                path_locations_list.append(path_locations.squeeze(0))
                path_delays_list.append(path_delays.squeeze(0))
                current_time_list.append(current_time.squeeze(0))
                other_agents_list.append(other_agents.squeeze(0))
                num_requests_list.append(num_requests.squeeze(0))
                battery_level_list.append(battery_level.squeeze(0))
                request_value_list.append(request_value.squeeze(0))
            
            # å †å ä¸ºæ‰¹é‡å¼ é‡
            batch_tensors['path_locations'] = torch.stack(path_locations_list)
            batch_tensors['path_delays'] = torch.stack(path_delays_list)
            batch_tensors['current_time'] = torch.stack(current_time_list)
            batch_tensors['other_agents'] = torch.stack(other_agents_list)
            batch_tensors['num_requests'] = torch.stack(num_requests_list)
            batch_tensors['battery_level'] = torch.stack(battery_level_list)
            batch_tensors['request_value'] = torch.stack(request_value_list)
            # åˆ›å»ºé»˜è®¤çš„targetä¿¡æ¯
            batch_tensors['target_distance'] = torch.zeros(batch_size, 1).to(self.device)
            batch_tensors['target_zoneid'] = torch.zeros(batch_size, 1, dtype=torch.long).to(self.device)
        else:
            # å‘åå…¼å®¹å¤„ç†
            raise ValueError("Insufficient input dimensions for batch processing")
        
        return batch_tensors
        
    def _calculate_context_adjustment(self, vehicle_id: int, vehicle_location: int, 
                                    target_location: int, battery_level: float,
                                    request_value: float, other_vehicles: int, 
                                    num_requests: int, current_time: float) -> float:
        """
        è®¡ç®—åŸºäºä¸Šä¸‹æ–‡çš„Qå€¼è°ƒæ•´å› å­
        è€ƒè™‘è½¦è¾†ç±»å‹ã€ç”µæ± çŠ¶æ€ã€ç«äº‰ç¯å¢ƒã€è¯·æ±‚ä»·å€¼ç­‰å› ç´ 
        """
        adjustment = 0.0
        
        # 1. ç”µæ± ç”µé‡å¯¹åˆ†é…çš„å½±å“
        if battery_level < 0.3:  # ä½ç”µé‡æ—¶
            # è®¡ç®—åˆ°å……ç”µç«™çš„è·ç¦»å½±å“
            grid_size = int(math.sqrt(max(vehicle_location, target_location)) + 1)
            distance_to_target = self._calculate_manhattan_distance(vehicle_location, target_location, grid_size)
            # è·ç¦»è¶Šè¿œï¼Œç”µé‡è¶Šä½ï¼ŒQå€¼è°ƒæ•´è¶Šè´Ÿ
            battery_penalty = -0.2 * (0.3 - battery_level) * (distance_to_target / 10.0)
            adjustment += battery_penalty
            
        # 2. è¯·æ±‚ä»·å€¼å¯¹åˆ†é…çš„å½±å“
        if request_value > 0:
            # é«˜ä»·å€¼è¯·æ±‚è·å¾—å¥–åŠ±
            value_bonus = min(0.1 * (request_value / 50.0), 0.5)  # æœ€å¤§å¥–åŠ±0.5
            adjustment += value_bonus
            
        # 3. ç«äº‰ç¯å¢ƒçš„å½±å“
        if other_vehicles > 0 and num_requests > 0:
            competition_ratio = other_vehicles / max(num_requests, 1)
            if competition_ratio > 1.0:  # è½¦è¾†å¤šäºè¯·æ±‚
                # ç«äº‰æ¿€çƒˆæ—¶ï¼Œè·ç¦»è¿‘çš„åˆ†é…è·å¾—æ›´å¤šå¥–åŠ±
                grid_size = int(math.sqrt(max(vehicle_location, target_location)) + 1)
                distance = self._calculate_manhattan_distance(vehicle_location, target_location, grid_size)
                distance_bonus = max(0, 0.2 - 0.02 * distance)  # è·ç¦»è¶Šè¿‘å¥–åŠ±è¶Šé«˜
                adjustment += distance_bonus
                
        # 4. æ—¶é—´å› ç´ çš„å½±å“ï¼ˆç´§æ€¥è¯·æ±‚ï¼‰
        # å‡è®¾current_timeå¯ä»¥åæ˜ è¯·æ±‚çš„ç´§æ€¥ç¨‹åº¦
        if current_time > 0:
            time_factor = min(current_time / 100.0, 1.0)  # æ—¶é—´æ ‡å‡†åŒ–
            urgency_bonus = 0.1 * time_factor  # æ—¶é—´è¶Šé•¿è¶Šç´§æ€¥
            adjustment += urgency_bonus
            
        # 5. è½¦è¾†ç±»å‹çš„å½±å“
        vehicle_type_id = 1 if vehicle_id % 2 == 0 else 2  # ç®€åŒ–çš„è½¦è¾†ç±»å‹åˆ¤æ–­
        if vehicle_type_id == 2:  # AEVç±»å‹è½¦è¾†
            # AEVåœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½æœ‰ä¼˜åŠ¿
            aev_bonus = 0.05 if battery_level > 0.7 else -0.05
            adjustment += aev_bonus
            
        return adjustment
    def get_idle_q_value(self, vehicle_id: int, vehicle_location: int, target_location: int,
                        battery_level: float, current_time: float = 0.0, 
                        other_vehicles: int = 0, num_requests: int = 0) -> float:
        """
        Get Q-value for idle action with random movement
        Idle action involves moving to a random nearby location, not staying in place
        """
        import random
        
        # Convert location index to coordinates for random target generation
        current_x = vehicle_location % self.grid_size
        current_y = vehicle_location // self.grid_size
        

        
        # Use the generated random target location for Q-value calculation
        return self.get_q_value(vehicle_id, "idle", vehicle_location, target_location, 
                               current_time, other_vehicles, num_requests, battery_level)

    def get_waiting_q_value(self, vehicle_id: int, vehicle_location: int, 
                        battery_level: float, current_time: float = 0.0, 
                        other_vehicles: int = 0, num_requests: int = 0) -> float:
        """
        Get Q-value for waiting action (staying in place)
        Unlike idle action, waiting means the vehicle stays at the current location
        """
        # For waiting action, target location equals current location (no movement)
        return self.get_q_value(vehicle_id, "idle", vehicle_location, vehicle_location, vehicle_location,
                               current_time, other_vehicles, num_requests, battery_level)



    def get_charging_q_value(self, vehicle_id: int, station_id: int,
                           vehicle_location: int, station_location: int,
                           current_time: float = 0.0, other_vehicles: int = 0,
                           num_requests: int = 0, battery_level: float = 1.0) -> float:
        """
        Get Q-value for vehicle charging decision using neural network
        ç°åœ¨æ”¯æŒbattery_levelå‚æ•°
        """
        return self.get_q_value(vehicle_id, f"charge_{station_id}",
                               vehicle_location, station_location, current_time,
                               other_vehicles, num_requests, battery_level)
    
    def _calculate_rejection_risk_penalty(self, vehicle_id: int, distance: float) -> float:
        """
        åŸºäºç¥ç»ç½‘ç»œä»å†å²æ‹’ç»ç»éªŒå­¦ä¹ çš„æ‹’ç»æ¦‚ç‡è®¡ç®—æƒ©ç½š
        ä½¿ç”¨è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œé¢„æµ‹æ‹’ç»æ¦‚ç‡ï¼Œè€Œä¸æ˜¯å›ºå®šçš„æ•°å­¦å…¬å¼
        """
        # è·å–è½¦è¾†ä¿¡æ¯
        if hasattr(self, 'env') and self.env is not None:
            vehicle = self.env.vehicles.get(vehicle_id)
            if vehicle is None:
                return 0.0
            
            battery_level = vehicle.get('battery', 1.0)
            vehicle_type = vehicle.get('type', 1)
            current_time = self.env.current_time if hasattr(self.env, 'current_time') else 0
            num_requests = len(self.env.active_requests) if hasattr(self.env, 'active_requests') else 0
        else:
            # å›é€€åˆ°é»˜è®¤å€¼
            battery_level = 1.0
            vehicle_type = 1 if vehicle_id % 2 == 0 else 2
            current_time = 0
            num_requests = 0
        
        # åªä¸ºEVè½¦è¾†è®¡ç®—æ‹’ç»é£é™©ï¼ŒAEVä¸æ‹’ç»
        if vehicle_type != 1:  # ä¸æ˜¯EV
            return 0.0
        
        # å‡†å¤‡ç¥ç»ç½‘ç»œè¾“å…¥ç‰¹å¾
        features = torch.tensor([
            distance ,  # å½’ä¸€åŒ–è·ç¦»ï¼ˆå‡è®¾æœ€å¤§è·ç¦»ä¸º20ï¼‰
            battery_level,    # ç”µæ± ç”µé‡
            current_time / 300.0,  # å½’ä¸€åŒ–æ—¶é—´
            num_requests / 50.0,   # å½’ä¸€åŒ–è¯·æ±‚æ•°é‡
            1.0  # EVæ ‡è¯†ï¼ˆ1=EV, 0=AEVï¼‰
        ], dtype=torch.float32).unsqueeze(0).to(self.device)
        
        # ä½¿ç”¨ç¥ç»ç½‘ç»œé¢„æµ‹æ‹’ç»æ¦‚ç‡
        self.rejection_predictor.eval()
        with torch.no_grad():
            rejection_prob = self.rejection_predictor(features).item()
        
        # åŸºäºé¢„æµ‹çš„æ‹’ç»æ¦‚ç‡è®¡ç®—æƒ©ç½š
        rejection_penalty = rejection_prob * 3.0  # å¯è°ƒèŠ‚çš„æ‹’ç»æƒ©ç½šå› å­
        
        # å¦‚æœæ‹’ç»æ•°æ®ä¸è¶³ï¼Œå›é€€åˆ°ç®€å•çš„è·ç¦»å…¬å¼
        if len(self.rejection_buffer) < 50:
            distance_factor = 0.1
            fallback_prob = min(0.9, 1 - math.exp(-distance * distance_factor))
            rejection_penalty = fallback_prob * 2.0
        
        return rejection_penalty
    
    def _calculate_manhattan_distance(self, location1: int, location2: int, grid_size: int = None) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªä½ç½®ä¹‹é—´çš„æ›¼å“ˆé¡¿è·ç¦»
        
        Args:
            location1: ä½ç½®1ï¼ˆç½‘æ ¼ç´¢å¼•ï¼‰
            location2: ä½ç½®2ï¼ˆç½‘æ ¼ç´¢å¼•ï¼‰
            grid_size: ç½‘æ ¼å¤§å°ï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨self.grid_size
            
        Returns:
            æ›¼å“ˆé¡¿è·ç¦»
        """
        if grid_size is None:
            grid_size = self.grid_size
            
        # å°†ä½ç½®ç´¢å¼•è½¬æ¢ä¸ºåæ ‡
        x1, y1 = location1 % grid_size, location1 // grid_size
        x2, y2 = location2 % grid_size, location2 // grid_size
        
        # è®¡ç®—æ›¼å“ˆé¡¿è·ç¦»
        distance = abs(x1 - x2) + abs(y1 - y2)
        
        return float(distance)
    
    def store_experience(self, vehicle_id: int, action_type: str, vehicle_location: int,
                        target_location: int, current_time: float, reward: float,
                        next_vehicle_location: int, next_target_location: int, battery_level: float = 1.0, 
                        next_battery_level: float = 1.0, other_vehicles: int = 0, 
                        num_requests: int = 0, request_value: float = 0.0,
                        next_action_type: str = None, next_request_value: float = 0.0,
                        dur_time: float = 1.0, is_system_done: bool = False):
        """
        Store experience for training - ç°åœ¨æ”¯æŒvehicle_idã€batteryã€request_valueã€æŒç»­æ—¶é—´å’Œç³»ç»Ÿç»“æŸçŠ¶æ€ä¿¡æ¯
        
        Args:
            vehicle_id: è½¦è¾†ID
            action_type: åŠ¨ä½œç±»å‹
            vehicle_location: è½¦è¾†å½“å‰ä½ç½®
            target_location: ç›®æ ‡ä½ç½®
            current_time: å½“å‰æ—¶é—´
            reward: è·å¾—çš„å¥–åŠ±
            next_vehicle_location: ä¸‹ä¸€çŠ¶æ€çš„è½¦è¾†ä½ç½®
            battery_level: å½“å‰ç”µæ± ç”µé‡ (é»˜è®¤1.0ä¸ºå‘åå…¼å®¹)
            next_battery_level: ä¸‹ä¸€çŠ¶æ€çš„ç”µæ± ç”µé‡ (é»˜è®¤1.0ä¸ºå‘åå…¼å®¹)
            other_vehicles: é™„è¿‘å…¶ä»–è½¦è¾†æ•°é‡
            num_requests: å½“å‰è¯·æ±‚æ•°é‡
            request_value: è¯·æ±‚ä»·å€¼ (åªå¯¹assignåŠ¨ä½œæœ‰æ•ˆï¼Œé»˜è®¤0.0)
            next_action_type: ä¸‹ä¸€ä¸ªåŠ¨ä½œç±»å‹ (è½¦è¾†å®Œæˆå½“å‰åŠ¨ä½œåæ ¹æ®ILPåˆ†é…çš„åŠ¨ä½œæ ‡ç­¾)
            dur_time: åŠ¨ä½œæŒç»­æ—¶é—´ (é»˜è®¤1.0ä¸ºå‘åå…¼å®¹)
            is_system_done: æ•´ä¸ªç³»ç»Ÿæ˜¯å¦ç»“æŸ (é»˜è®¤Falseä¸ºå‘åå…¼å®¹)
        """
        # ä»vehicle_idæ¨æ–­è½¦è¾†ç±»å‹ï¼ˆç®€åŒ–å¤„ç†ï¼‰
        vehicle_type = 1 if vehicle_id % 2 == 0 else 2  # 1=EV, 2=AEV
        
        # ğŸ”§ ç¡®ä¿locationæ˜¯intç±»å‹ï¼Œå¦‚æœæ˜¯tupleåˆ™è½¬æ¢ä¸ºlocation ID
        def ensure_location_id(loc):
            """ç¡®ä¿locationæ˜¯int IDè€Œä¸æ˜¯tupleåæ ‡"""
            if loc is None:
                return 0  # é»˜è®¤å€¼
            if isinstance(loc, tuple):
                # å¦‚æœæ˜¯(x, y)åæ ‡ï¼Œè½¬æ¢ä¸ºlocation ID
                x, y = loc
                return int(y * self.env.grid_size + x)
            return int(loc)
        
        # ç¡®ä¿ target_location å’Œ next_target_location ä¸ä¸º None
        if target_location is None:
            target_location = 0
        if next_target_location is None:
            next_target_location = 0
        
        vehicle_location = ensure_location_id(vehicle_location)
        target_location = ensure_location_id(target_location)
        next_vehicle_location = ensure_location_id(next_vehicle_location)
        next_target_location = ensure_location_id(next_target_location)
        reject_dist = self.env._manhattan_distance_loc(vehicle_location, target_location) if hasattr(self.env, '_manhattan_distance_loc') else 0
        #if action_type.startswith('assign') and reward<-10:
            #print(f"âŒğŸš« EV REJECTION - Distance: {reject_dist} | Vehicle: {vehicle_id} | Reward: {reward:.2f}")
        #if action_type.startswith('assign') and reward>10 and self.env.vehicles[vehicle_id]['type']==1:
            #print(f"âœ… EV ASSIGNED - Distance: {reject_dist} | Vehicle: {vehicle_id} | Reward: {reward:.2f} | Battery: {battery_level:.2f}")
        experience = {
            'vehicle_id': vehicle_id,
            'vehicle_type': vehicle_type,  # æ·»åŠ è½¦è¾†ç±»å‹
            'action_type': action_type,
            'vehicle_location': vehicle_location,
            'target_location': target_location,
            'battery_level': battery_level,  # æ·»åŠ å½“å‰ç”µæ± ç”µé‡
            'current_time': current_time,
            'target_distance': self.env._manhattan_distance_loc(vehicle_location, target_location),
            'pickup_dist': self.env._manhattan_distance_loc(vehicle_location, target_location),  # æ·»åŠ pickup_distç”¨äºç»Ÿè®¡
            'target_zoneid': self.env.get_zone_id(target_location),
            'reward': reward,
            'next_vehicle_location': next_vehicle_location,
            'next_battery_level': next_battery_level,  # æ·»åŠ ä¸‹ä¸€çŠ¶æ€ç”µæ± ç”µé‡
            'next_action_type': next_action_type if next_action_type is not None else action_type,  # æ·»åŠ ä¸‹ä¸€åŠ¨ä½œç±»å‹ï¼Œé»˜è®¤ä¸ºå½“å‰åŠ¨ä½œç±»å‹
            'other_vehicles': other_vehicles,
            'num_requests': num_requests,
            'request_value': request_value,  # æ·»åŠ è¯·æ±‚ä»·å€¼ä¿¡æ¯
            'next_request_value': next_request_value,  # ä¸‹ä¸€çŠ¶æ€è¯·æ±‚ä»·å€¼
            'next_target_distance': self.env._manhattan_distance_loc(next_vehicle_location, next_target_location),
            'next_target_zoneid': self.env.get_zone_id(next_target_location),
            'is_idle': action_type == 'idle',  # è‡ªåŠ¨æ ‡è®°idleçŠ¶æ€
            'dur_time': dur_time,  # æ·»åŠ åŠ¨ä½œæŒç»­æ—¶é—´
            'is_system_done': is_system_done  # æ·»åŠ ç³»ç»Ÿç»“æŸçŠ¶æ€
        }
        self.experience_buffer.append(experience)
    
    def store_idle_experience(self, vehicle_id: int, vehicle_location: int, 
                            battery_level: float, current_time: float, reward: float,
                            next_vehicle_location: int, next_battery_level: float,
                            other_vehicles: int = 0, num_requests: int = 0, request_value: float = 0.0):
        """
        Store idle experience for training - ä¸“é—¨ä¸ºidleåŠ¨ä½œå­˜å‚¨ç»éªŒ
        
        Args:
            vehicle_id: è½¦è¾†ID
            vehicle_location: è½¦è¾†å½“å‰ä½ç½®
            battery_level: å½“å‰ç”µæ± ç”µé‡
            current_time: å½“å‰æ—¶é—´
            reward: è·å¾—çš„å¥–åŠ±
            next_vehicle_location: ä¸‹ä¸€çŠ¶æ€çš„è½¦è¾†ä½ç½®
            next_battery_level: ä¸‹ä¸€çŠ¶æ€çš„ç”µæ± ç”µé‡
            other_vehicles: é™„è¿‘å…¶ä»–è½¦è¾†æ•°é‡
            num_requests: å½“å‰è¯·æ±‚æ•°é‡
            request_value: è¯·æ±‚ä»·å€¼ (idleæ—¶ä¸º0.0)
        """
        experience = {
            'vehicle_id': vehicle_id,
            'action_type': 'idle',
            'vehicle_location': vehicle_location,
            'target_location': vehicle_location,  # idleæ—¶ç›®æ ‡ä½ç½®å°±æ˜¯å½“å‰ä½ç½®
            'battery_level': battery_level,
            'current_time': current_time,
            'reward': reward,
            'next_vehicle_location': next_vehicle_location,
            'next_battery_level': next_battery_level,
            'other_vehicles': other_vehicles,
            'num_requests': num_requests,
            'request_value': request_value,  # æ·»åŠ è¯·æ±‚ä»·å€¼ä¿¡æ¯ï¼ˆidleæ—¶ä¸º0ï¼‰
            'is_idle': True  # æ ‡è®°è¿™æ˜¯ä¸€ä¸ªidleç»éªŒ
        }
        self.experience_buffer.append(experience)
    
    def analyze_experience_data(self):
        """åˆ†æç»éªŒç¼“å†²åŒºä¸­çš„å¥–åŠ±åˆ†å¸ƒå’ŒåŠ¨ä½œç±»å‹ç»Ÿè®¡"""
        if len(self.experience_buffer) < 100:
            return None
            
        experiences = list(self.experience_buffer)
        
        # å¥–åŠ±åˆ†æ
        rewards = [exp['reward'] for exp in experiences]
        positive_rewards = [r for r in rewards if r > 0]
        negative_rewards = [r for r in rewards if r < 0]
        neutral_rewards = [r for r in rewards if r == 0]
        
        reward_stats = {
            'total_count': len(rewards),
            'positive_count': len(positive_rewards),
            'negative_count': len(negative_rewards),
            'neutral_count': len(neutral_rewards),
            'positive_ratio': len(positive_rewards) / len(rewards),
            'negative_ratio': len(negative_rewards) / len(rewards),
            'neutral_ratio': len(neutral_rewards) / len(rewards),
            'mean_reward': np.mean(rewards),
            'mean_positive': np.mean(positive_rewards) if positive_rewards else 0,
            'mean_negative': np.mean(negative_rewards) if negative_rewards else 0,
            'std_reward': np.std(rewards),
            'max_reward': np.max(rewards),
            'min_reward': np.min(rewards)
        }
        
        # åŠ¨ä½œç±»å‹åˆ†æ
        action_types = [exp['action_type'] for exp in experiences]
        assign_actions = [exp for exp in experiences if exp['action_type'].startswith('assign')]
        charge_actions = [exp for exp in experiences if exp['action_type'].startswith('charge')]
        idle_actions = [exp for exp in experiences if exp['action_type'] == 'idle']
        
        action_stats = {
            'assign_count': len(assign_actions),
            'charge_count': len(charge_actions), 
            'idle_count': len(idle_actions),
            'assign_ratio': len(assign_actions) / len(experiences),
            'assign_mean_reward': np.mean([exp['reward'] for exp in assign_actions]) if assign_actions else 0,
            'charge_mean_reward': np.mean([exp['reward'] for exp in charge_actions]) if charge_actions else 0,
            'idle_mean_reward': np.mean([exp['reward'] for exp in idle_actions]) if idle_actions else 0,
            'assign_positive_ratio': len([exp for exp in assign_actions if exp['reward'] > 0]) / len(assign_actions) if assign_actions else 0,
            'charge_positive_ratio': len([exp for exp in charge_actions if exp['reward'] > 0]) / len(charge_actions) if charge_actions else 0,
            'idle_positive_ratio': len([exp for exp in idle_actions if exp['reward'] > 0]) / len(idle_actions) if idle_actions else 0
        }
        
        return {
            'reward_stats': reward_stats,
            'action_stats': action_stats
        }
    
    def store_rejection_experience(self, vehicle_id: int, request_id: int, vehicle_location: int,
                                 pickup_location: int, current_time: float, distance: float,
                                 rejection_reason: str = "distance"):
        #print("Storing rejection experience...")
        """
        å­˜å‚¨EVæ‹’ç»è®¢å•çš„è´Ÿé¢ç»éªŒï¼Œç”¨äºè®­ç»ƒé¿å…åˆ†é…ç»™EVè¿œè·ç¦»æˆ–å®¹æ˜“è¢«æ‹’ç»çš„è®¢å•
        
        Args:
            vehicle_id: æ‹’ç»è®¢å•çš„EVè½¦è¾†ID
            request_id: è¢«æ‹’ç»çš„è¯·æ±‚ID
            vehicle_location: è½¦è¾†ä½ç½®
            pickup_location: æ¥å®¢ä½ç½®
            current_time: å½“å‰æ—¶é—´
            distance: è·ç¦»ï¼ˆä¸»è¦çš„æ‹’ç»å› ç´ ï¼‰
            rejection_reason: æ‹’ç»åŸå› 
        """
        # è®¡ç®—è´Ÿé¢å¥–åŠ±ï¼Œè·ç¦»è¶Šè¿œæƒ©ç½šè¶Šå¤§
        distance_penalty = -1.0 - (distance * 0.2)  # åŸºç¡€æƒ©ç½š-1.0ï¼ŒåŠ ä¸Šè·ç¦»æƒ©ç½š
        
        # å­˜å‚¨æ‹’ç»ç»éªŒ
        rejection_experience = {
            'vehicle_id': vehicle_id,
            'vehicle_type': 1,  # EV
            'action_type': f'assign_{request_id}',
            'vehicle_location': vehicle_location,
            'target_location': pickup_location,
            'battery_level': 1.0,  # å‡è®¾é»˜è®¤ç”µé‡
            'current_time': current_time,
            'reward': distance_penalty,  # è´Ÿé¢å¥–åŠ±
            'next_vehicle_location': vehicle_location,  # æ‹’ç»åè½¦è¾†ä½ç½®ä¸å˜
            'next_battery_level': 1.0,
            'next_action_type': 'idle',  # æ‹’ç»åå˜ä¸ºidleçŠ¶æ€
            'other_vehicles': 0,
            'num_requests': 1,
            'request_value': 0.0,
            'next_request_value': 0.0,
            'is_idle': False,
            'is_rejection': True,  # æ ‡è®°ä¸ºæ‹’ç»ç»éªŒ
            'rejection_reason': rejection_reason,
            'rejection_distance': distance
        }
        

        vehicle = self.env.vehicles.get(vehicle_id)
        if vehicle is not None:
            battery_level = vehicle.get('battery', 1.0)
            num_requests = len(self.env.active_requests) if hasattr(self.env, 'active_requests') else 0
            
            rejection_data = {
                'distance': distance,
                'battery_level': battery_level,
                'current_time': current_time,
                'num_requests': num_requests,
                'vehicle_type': 1,  # EV
                'was_rejected': True  # å®é™…æ‹’ç»äº†
            }
            self.rejection_buffer.append(rejection_data)
    
    def store_acceptance_experience(self, vehicle_id: int, request_id: int, vehicle_location: int,
                                  pickup_location: int, current_time: float, distance: float):
        """
        å­˜å‚¨EVæ¥å—è®¢å•çš„æ­£é¢ç»éªŒï¼Œç”¨äºè®­ç»ƒæ‹’ç»æ¦‚ç‡é¢„æµ‹å™¨
        è¿™ä¸æ‹’ç»ç»éªŒå½¢æˆå¯¹æ¯”ï¼Œå¸®åŠ©ç½‘ç»œå­¦ä¹ æ¥å—/æ‹’ç»çš„è¾¹ç•Œ
        """
        if hasattr(self, 'env') and self.env is not None:
            vehicle = self.env.vehicles.get(vehicle_id)
            if vehicle is not None and vehicle.get('type') == 1:  # åªå­˜å‚¨EVçš„æ¥å—æ•°æ®
                battery_level = vehicle.get('battery', 1.0)
                num_requests = len(self.env.active_requests) if hasattr(self.env, 'active_requests') else 0
                
                acceptance_data = {
                    'distance': distance,
                    'battery_level': battery_level,
                    'current_time': current_time,
                    'num_requests': num_requests,
                    'vehicle_type': 1,  # EV
                    'was_rejected': False  # å®é™…æ¥å—äº†
                }
                
                self.rejection_buffer.append(acceptance_data)
                
                if self.debug_mode:
                    print(f"Stored acceptance experience: EV {vehicle_id} accepted request {request_id} "
                          f"(distance={distance:.2f})")
        

    
    def train_rejection_predictor(self, batch_size=64, num_epochs=10):
        """
        è®­ç»ƒæ‹’ç»æ¦‚ç‡é¢„æµ‹ç¥ç»ç½‘ç»œ
        ä½¿ç”¨å­˜å‚¨çš„æ‹’ç»å’Œæ¥å—æ•°æ®è¿›è¡Œç›‘ç£å­¦ä¹ 
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            num_epochs: è®­ç»ƒè½®æ•°
        """
        print("Training rejection predictor...")
        print(f"Rejection buffer size: {len(self.rejection_buffer)}")
        if len(self.rejection_buffer) < batch_size:
            return None  # æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®­ç»ƒ
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        data = list(self.rejection_buffer)
        random.shuffle(data)
        
        # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
        features = []
        labels = []
        
        for sample in data:
            # å½’ä¸€åŒ–ç‰¹å¾ - ä¸predict_rejection_probabilityä¿æŒä¸€è‡´
            feature = [
                sample['distance'] ,  # å‡è®¾æœ€å¤§è·ç¦»ä¸º20
                sample['battery_level'],    # å·²ç»æ˜¯0-1
                sample['current_time'] / 300.0,  # å‡è®¾æœ€å¤§æ—¶é—´ä¸º300
                sample['num_requests'] / 100.0,   # å½’ä¸€åŒ–è¯·æ±‚æ•°é‡
                sample['vehicle_type'] / 2.0  # 1æˆ–2 -> 0.5æˆ–1.0
            ]
            features.append(feature)
            labels.append(1.0 if sample['was_rejected'] else 0.0)
        
        # è½¬æ¢ä¸ºå¼ é‡
        X = torch.tensor(features, dtype=torch.float32).to(self.device)
        y = torch.tensor(labels, dtype=torch.float32).to(self.device)
        
        # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        dataset = TensorDataset(X, y)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        # å¤šè½®è®­ç»ƒ
        total_loss = 0.0
        total_batches = 0
        
        for epoch in range(num_epochs):
            epoch_loss = 0.0
            num_batches = 0
            
            for batch_X, batch_y in dataloader:
                self.rejection_optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                predictions = self.rejection_predictor(batch_X).squeeze(-1)  # åªç§»é™¤æœ€åä¸€ä¸ªç»´åº¦
                # ç¡®ä¿ç»´åº¦åŒ¹é…
                if predictions.dim() == 0:  # å¦‚æœæ˜¯æ ‡é‡ï¼Œæ·»åŠ ä¸€ä¸ªç»´åº¦
                    predictions = predictions.unsqueeze(0)
                if batch_y.dim() == 0:  # å¦‚æœbatch_yæ˜¯æ ‡é‡ï¼Œæ·»åŠ ä¸€ä¸ªç»´åº¦
                    batch_y = batch_y.unsqueeze(0)
                
                loss = self.rejection_criterion(predictions, batch_y)
                
                # åå‘ä¼ æ’­
                loss.backward()
                self.rejection_optimizer.step()
                
                epoch_loss += loss.item()
                num_batches += 1
            
            avg_epoch_loss = epoch_loss / num_batches if num_batches > 0 else 0
            total_loss += epoch_loss
            total_batches += num_batches
            
            if self.debug_mode and (epoch == 0 or (epoch + 1) % 5 == 0 or epoch == num_epochs - 1):
                print(f"  Epoch {epoch+1}/{num_epochs}: avg_loss={avg_epoch_loss:.4f}")
        
        avg_loss = total_loss / total_batches if total_batches > 0 else 0
        
        print(f"Rejection predictor training complete: {len(data)} samples, {num_epochs} epochs, final avg_loss={avg_loss:.4f}")
        
        return avg_loss
    
    def predict_rejection_probability(self, vehicle_id, request_id, vehicle_location, pickup_location, current_time):
        """
        ä½¿ç”¨è®­ç»ƒå¥½çš„rejection_predictoré¢„æµ‹EVæ‹’ç»æŸä¸ªè¯·æ±‚çš„æ¦‚ç‡
        
        Args:
            vehicle_id: è½¦è¾†ID
            request_id: è¯·æ±‚ID
            vehicle_location: è½¦è¾†å½“å‰ä½ç½®
            pickup_location: æ¥å®¢ä½ç½®
            current_time: å½“å‰æ—¶é—´
            
        Returns:
            float: æ‹’ç»æ¦‚ç‡ (0-1ä¹‹é—´)
        """
        if not hasattr(self, 'rejection_predictor') or self.rejection_predictor is None:
            # å¦‚æœæ²¡æœ‰è®­ç»ƒrejection_predictorï¼Œè¿”å›é»˜è®¤å€¼
            return 0.0
        
        # è®¡ç®—è·ç¦»
        if hasattr(self, 'env') and self.env is not None:
            grid_size = self.env.grid_size
        else:
            grid_size = int(math.sqrt(max(vehicle_location, pickup_location) + 1))
        
        vehicle_x = vehicle_location % grid_size
        vehicle_y = vehicle_location // grid_size
        pickup_x = pickup_location % grid_size
        pickup_y = pickup_location // grid_size
        distance = abs(vehicle_x - pickup_x) + abs(vehicle_y - pickup_y)
        
        # è·å–è½¦è¾†ä¿¡æ¯
        if hasattr(self, 'env') and self.env is not None and vehicle_id in self.env.vehicles:
            vehicle = self.env.vehicles[vehicle_id]
            battery_level = vehicle.get('battery', 0.8)
            vehicle_type = vehicle.get('type', 1)
        else:
            battery_level = 0.8
            vehicle_type = 1
        
        # è·å–å½“å‰è¯·æ±‚æ•°
        if hasattr(self, 'env') and self.env is not None:
            num_requests = len(self.env.active_requests)
        else:
            num_requests = 10
        
        # å‡†å¤‡è¾“å…¥ç‰¹å¾ - ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„å½’ä¸€åŒ–
        features = torch.tensor([
            [distance , battery_level, current_time / 300.0, num_requests / 100.0, vehicle_type / 2.0]
        ], dtype=torch.float32).to(self.device)
        
        # é¢„æµ‹
        self.rejection_predictor.eval()
        with torch.no_grad():
            rejection_prob = self.rejection_predictor(features).item()
        
        return rejection_prob
    
    def get_rejection_statistics(self):
        """è·å–æ‹’ç»ç»éªŒçš„ç»Ÿè®¡ä¿¡æ¯"""
        rejection_experiences = [exp for exp in self.experience_buffer if exp.get('is_rejection', False)]
        
        if not rejection_experiences:
            return None
            
        distances = [exp['rejection_distance'] for exp in rejection_experiences]
        rewards = [exp['reward'] for exp in rejection_experiences]
        
        return {
            'total_rejections': len(rejection_experiences),
            'avg_rejection_distance': np.mean(distances),
            'max_rejection_distance': np.max(distances),
            'min_rejection_distance': np.min(distances),
            'avg_rejection_penalty': np.mean(rewards),
            'rejection_ratio': len(rejection_experiences) / len(self.experience_buffer) if self.experience_buffer else 0
        }
    
    def _advanced_sample(self, batch_size: int, method: str = "balanced"):
        """
        ç®€åŒ–çš„é‡‡æ ·ç­–ç•¥ï¼šåªä¿ç•™balancedå’Œimportanceé‡‡æ ·
        """
        experiences = list(self.experience_buffer)
        
        if method == "importance":
            return self._importance_sampling(experiences, batch_size)
        elif method == "action_balanced":
            return self._action_balanced_sample(batch_size)
        else:
            return self._balanced_sample(batch_size)
    
    def _importance_sampling(self, experiences, batch_size: int):
        """
        é‡è¦æ€§é‡‡æ ·ï¼šæ ¹æ®ç»éªŒçš„é‡è¦æ€§æƒé‡è¿›è¡Œé‡‡æ ·
        é‡è¦æ€§åŸºäºï¼šTDè¯¯å·®ã€å¥–åŠ±ç¨€æœ‰æ€§ã€åŠ¨ä½œç±»å‹ç¨€æœ‰æ€§
        """
        if len(experiences) == 0:
            return []
        
        # è®¡ç®—æ¯ä¸ªç»éªŒçš„é‡è¦æ€§æƒé‡
        weights = []
        action_counts = {'idle': 0, 'assign': 0, 'charge': 0}
        reward_values = [exp['reward'] for exp in experiences]
        
        # ç»Ÿè®¡åŠ¨ä½œç±»å‹åˆ†å¸ƒ
        for exp in experiences:
            action_type = exp['action_type']
            if action_type == 'idle':
                action_counts['idle'] += 1
            elif action_type.startswith('assign'):
                action_counts['assign'] += 1
            elif action_type.startswith('charge'):
                action_counts['charge'] += 1
        
        total_experiences = len(experiences)
        reward_std = np.std(reward_values) if len(reward_values) > 1 else 1.0
        
        for i, exp in enumerate(experiences):
            # 1. åŠ¨ä½œç¨€æœ‰æ€§æƒé‡
            action_type = exp['action_type']
            if action_type == 'idle':
                action_rarity = total_experiences / max(1, action_counts['idle'])
            elif action_type.startswith('assign'):
                action_rarity = total_experiences / max(1, action_counts['assign'])
            elif action_type.startswith('charge'):
                action_rarity = total_experiences / max(1, action_counts['charge'])
            else:
                action_rarity = 1.0
            
            # 2. å¥–åŠ±ç¨€æœ‰æ€§æƒé‡
            reward = exp['reward']
            reward_rarity = abs(reward) / (reward_std + 1e-8)
            
            # 3. æ—¶é—´æƒé‡ï¼ˆæœ€è¿‘çš„ç»éªŒæ›´é‡è¦ï¼‰
            time_weight = 0.5 + 0.5 * (i / max(1, len(experiences) - 1))
            
            # 4. å¦‚æœæ˜¯é«˜ä»·å€¼assignåŠ¨ä½œï¼Œç»™äºˆé¢å¤–æƒé‡
            if action_type.startswith('assign') and reward > 10:
                assign_bonus = 2.0
            else:
                assign_bonus = 1.0
            
            # ç»„åˆæƒé‡
            total_weight = action_rarity * reward_rarity * time_weight * assign_bonus
            weights.append(max(0.1, total_weight))  # æœ€å°æƒé‡é˜²æ­¢0æƒé‡
        
        # å½’ä¸€åŒ–æƒé‡
        weights = np.array(weights)
        weights = weights / np.sum(weights)
        
        # æ ¹æ®æƒé‡é‡‡æ ·
        indices = np.random.choice(len(experiences), size=min(batch_size, len(experiences)), 
                                 replace=False, p=weights)
        
        sampled_experiences = [experiences[i] for i in indices]
        
        # è°ƒè¯•ä¿¡æ¯ - åªåœ¨æ¯100æ­¥è¾“å‡ºä¸€æ¬¡
        if hasattr(self, 'training_step') and self.training_step % 100 == 0:
            action_types = [exp['action_type'] for exp in sampled_experiences]
            assign_count = sum(1 for a in action_types if a.startswith('assign'))
            idle_count = sum(1 for a in action_types if a == 'idle')
            charge_count = sum(1 for a in action_types if a.startswith('charge'))
            
            print(f"ğŸ“Š Importance sampling: Assign={assign_count}, Idle={idle_count}, Charge={charge_count}")
        
        return sampled_experiences
    
    def _thompson_sampling(self, experiences, batch_size: int):
        """
        Thompsoné‡‡æ ·ï¼šåŸºäºè´å¶æ–¯ä¼˜åŒ–çš„æ¢ç´¢-åˆ©ç”¨å¹³è¡¡
        ä¸ºæ¯ç§åŠ¨ä½œç±»å‹ç»´æŠ¤ä¸€ä¸ªBetaåˆ†å¸ƒ
        """
        if len(experiences) == 0:
            return []
        
        # ä¸ºæ¯ç§åŠ¨ä½œç±»å‹ç»´æŠ¤æˆåŠŸ/å¤±è´¥è®¡æ•°
        action_stats = {
            'idle': {'success': 1, 'failure': 1},      # å…ˆéªŒå‚æ•°
            'assign': {'success': 1, 'failure': 1},
            'charge': {'success': 1, 'failure': 1}
        }
        
        # æ›´æ–°ç»Ÿè®¡æ•°æ®
        for exp in experiences:
            action_type = exp['action_type']
            reward = exp['reward']
            
            if action_type == 'idle':
                key = 'idle'
            elif action_type.startswith('assign'):
                key = 'assign'
            elif action_type.startswith('charge'):
                key = 'charge'
            else:
                continue
            
            # å®šä¹‰æˆåŠŸçš„æ ‡å‡†
            if reward > 0:
                action_stats[key]['success'] += 1
            else:
                action_stats[key]['failure'] += 1
        
        # ä»Betaåˆ†å¸ƒé‡‡æ ·è·å¾—æ¯ç§åŠ¨ä½œçš„æœŸæœ›å›æŠ¥
        action_expectations = {}
        for action_type, stats in action_stats.items():
            # Betaåˆ†å¸ƒé‡‡æ ·
            alpha = stats['success']
            beta = stats['failure']
            expectation = np.random.beta(alpha, beta)
            action_expectations[action_type] = expectation
        
        print(f"ğŸ² Thompson sampling expectations: {action_expectations}")
        
        # åŸºäºæœŸæœ›å›æŠ¥åˆ†é…é‡‡æ ·æ¦‚ç‡
        total_expectation = sum(action_expectations.values())
        if total_expectation > 0:
            sampling_probs = {k: v/total_expectation for k, v in action_expectations.items()}
        else:
            sampling_probs = {k: 1.0/3 for k in action_expectations.keys()}
        
        # åˆ†åˆ«ä»æ¯ç§åŠ¨ä½œç±»å‹ä¸­é‡‡æ ·
        sampled_experiences = []
        for action_type, prob in sampling_probs.items():
            target_count = int(batch_size * prob)
            
            # æ‰¾åˆ°è¯¥åŠ¨ä½œç±»å‹çš„æ‰€æœ‰ç»éªŒ
            if action_type == 'idle':
                type_experiences = [exp for exp in experiences if exp['action_type'] == 'idle']
            elif action_type == 'assign':
                type_experiences = [exp for exp in experiences if exp['action_type'].startswith('assign')]
            elif action_type == 'charge':
                type_experiences = [exp for exp in experiences if exp['action_type'].startswith('charge')]
            
            if type_experiences and target_count > 0:
                actual_count = min(target_count, len(type_experiences))
                sampled = random.sample(type_experiences, actual_count)
                sampled_experiences.extend(sampled)
        
        # å¦‚æœé‡‡æ ·ä¸è¶³ï¼Œéšæœºè¡¥å……
        remaining = batch_size - len(sampled_experiences)
        if remaining > 0:
            remaining_experiences = [exp for exp in experiences if exp not in sampled_experiences]
            if remaining_experiences:
                additional = random.sample(remaining_experiences, min(remaining, len(remaining_experiences)))
                sampled_experiences.extend(additional)
        
        # è°ƒè¯•ä¿¡æ¯ - åªåœ¨æ¯100æ­¥è¾“å‡ºä¸€æ¬¡
        if hasattr(self, 'training_step') and self.training_step % 10000 == 0:
            action_types = [exp['action_type'] for exp in sampled_experiences]
            assign_count = sum(1 for a in action_types if a.startswith('assign'))
            idle_count = sum(1 for a in action_types if a == 'idle')
            charge_count = sum(1 for a in action_types if a.startswith('charge'))
            
            print(f"ğŸ“Š Thompson sampling: Assign={assign_count}, Idle={idle_count}, Charge={charge_count}")
        
        return sampled_experiences
    
    def _prioritized_sampling(self, experiences, batch_size: int):
        """
        ä¼˜å…ˆç»éªŒå›æ”¾ï¼šåŸºäºTDè¯¯å·®çš„ä¼˜å…ˆçº§é‡‡æ ·
        ä¼˜å…ˆçº§ = |TDè¯¯å·®| + åŠ¨ä½œä»·å€¼ + æ¢ç´¢å¥–åŠ±
        """
        if len(experiences) == 0:
            return []
        
        priorities = []
        
        for exp in experiences:
            # 1. åŸºäºå¥–åŠ±çš„åŸºç¡€ä¼˜å…ˆçº§
            reward = exp['reward']
            base_priority = abs(reward) + 1e-6  # é¿å…0ä¼˜å…ˆçº§
            
            # 2. åŠ¨ä½œç±»å‹å¥–åŠ±
            action_type = exp['action_type']
            if action_type.startswith('assign'):
                action_bonus = 2.0  # assignåŠ¨ä½œæ›´é‡è¦
            elif action_type.startswith('charge'):
                action_bonus = 1.5  # chargeåŠ¨ä½œä¸­ç­‰é‡è¦
            else:  # idle
                action_bonus = 1.0
            
            # 3. ç¨€æœ‰åŠ¨ä½œå¥–åŠ±
            rarity_bonus = 1.0
            if action_type.startswith('assign') and reward > 10:
                rarity_bonus = 3.0  # é«˜ä»·å€¼assignåŠ¨ä½œ
            elif action_type.startswith('charge') and reward > 0:
                rarity_bonus = 2.0  # æœ‰æ­£å›æŠ¥çš„chargeåŠ¨ä½œ
            
            # ç»„åˆä¼˜å…ˆçº§
            priority = base_priority * action_bonus * rarity_bonus
            priorities.append(priority)
        
        # è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        priorities = np.array(priorities)
        
        # ä½¿ç”¨alphaå‚æ•°æ§åˆ¶ä¼˜å…ˆçº§å¼ºåº¦
        alpha = 0.6  # 0è¡¨ç¤ºå‡åŒ€é‡‡æ ·ï¼Œ1è¡¨ç¤ºçº¯ä¼˜å…ˆçº§é‡‡æ ·
        priorities = priorities ** alpha
        
        # å½’ä¸€åŒ–
        probabilities = priorities / np.sum(priorities)
        
        # é‡‡æ ·
        indices = np.random.choice(len(experiences), size=min(batch_size, len(experiences)), 
                                 replace=False, p=probabilities)
        
        sampled_experiences = [experiences[i] for i in indices]
        
        # è°ƒè¯•ä¿¡æ¯ - åªåœ¨æ¯100æ­¥è¾“å‡ºä¸€æ¬¡
        if hasattr(self, 'training_step') and self.training_step % 100 == 0:
            action_types = [exp['action_type'] for exp in sampled_experiences]
            assign_count = sum(1 for a in action_types if a.startswith('assign'))
            idle_count = sum(1 for a in action_types if a == 'idle')
            charge_count = sum(1 for a in action_types if a.startswith('charge'))
            
            avg_priority = np.mean([priorities[i] for i in indices])
            print(f"ğŸ“Š Prioritized sampling: Assign={assign_count}, Idle={idle_count}, Charge={charge_count}, Avg Priority={avg_priority:.3f}")
        
        return sampled_experiences
        """
        å¹³è¡¡é‡‡æ ·ç­–ç•¥ï¼šç¡®ä¿æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬çš„æ¯”ä¾‹å‡è¡¡
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            å‡è¡¡é‡‡æ ·çš„ç»éªŒåˆ—è¡¨
        """
    def _balanced_sample(self, batch_size: int):
        experiences = list(self.experience_buffer)
        
        # æ ¹æ®å¥–åŠ±å°†ç»éªŒåˆ†ä¸ºæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬
        positive_samples = []  # æ­£å¥–åŠ±æ ·æœ¬
        negative_samples = []  # è´Ÿå¥–åŠ±æ ·æœ¬
        neutral_samples = []   # æ¥è¿‘é›¶çš„å¥–åŠ±æ ·æœ¬
        reward_threshold = 0
        reward_threshold_positive = 1.0   # æ­£æ ·æœ¬é˜ˆå€¼ - åªæœ‰æ˜æ˜¾çš„æ­£å¥–åŠ±
        reward_threshold_negative = -0.1  # è´Ÿæ ·æœ¬é˜ˆå€¼ - åŒ…å«å¤§éƒ¨åˆ†è´Ÿå¥–åŠ±
        
        for exp in experiences:
            reward = exp['reward']
            if reward > reward_threshold_positive:
                positive_samples.append(exp)
            elif reward < reward_threshold_negative:
                negative_samples.append(exp)
            else:
                neutral_samples.append(exp)
        
        # è®¡ç®—é‡‡æ ·æ¯”ä¾‹
        total_positive = len(positive_samples)
        total_negative = len(negative_samples)
        total_neutral = len(neutral_samples)
        
        if total_positive == 0 and total_negative == 0:
            # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„æ­£è´Ÿæ ·æœ¬ï¼Œä½¿ç”¨éšæœºé‡‡æ ·
            return random.sample(experiences, min(batch_size, len(experiences)))
        
        # è®¡ç®—æœŸæœ›çš„é‡‡æ ·æ•°é‡ - ä¼˜å…ˆä¿è¯æ­£è´Ÿæ ·æœ¬å‡è¡¡
        if total_positive > 0 and total_negative > 0:
            # æœ‰æ­£è´Ÿæ ·æœ¬æ—¶ï¼Œé‡‡ç”¨å¹³è¡¡ç­–ç•¥
            positive_count = min(batch_size // 3, total_positive)  # 1/3 æ­£æ ·æœ¬
            negative_count = min(batch_size // 3, total_negative)  # 1/3 è´Ÿæ ·æœ¬
            neutral_count = min(batch_size - positive_count - negative_count, total_neutral)  # å‰©ä½™ä¸ºä¸­æ€§æ ·æœ¬
        elif total_positive > 0:
            # åªæœ‰æ­£æ ·æœ¬æ—¶
            positive_count = min(batch_size // 2, total_positive)
            negative_count = 0
            neutral_count = min(batch_size - positive_count, total_neutral)
        else:
            # åªæœ‰è´Ÿæ ·æœ¬æ—¶
            positive_count = 0
            negative_count = min(batch_size // 2, total_negative)
            neutral_count = min(batch_size - negative_count, total_neutral)
        
        # æ‰§è¡Œé‡‡æ ·
        sampled_batch = []
        
        if positive_count > 0:
            sampled_batch.extend(random.sample(positive_samples, positive_count))
        
        if negative_count > 0:
            sampled_batch.extend(random.sample(negative_samples, negative_count))
        
        if neutral_count > 0:
            sampled_batch.extend(random.sample(neutral_samples, neutral_count))
        
        # å¦‚æœé‡‡æ ·æ•°é‡ä¸è¶³ï¼Œä»æ‰€æœ‰æ ·æœ¬ä¸­è¡¥å……
        remaining_needed = batch_size - len(sampled_batch)
        if remaining_needed > 0:
            remaining_experiences = [exp for exp in experiences if exp not in sampled_batch]
            if remaining_experiences:
                additional_samples = random.sample(
                    remaining_experiences, 
                    min(remaining_needed, len(remaining_experiences))
                )
                sampled_batch.extend(additional_samples)
        
        # æ‰“å°é‡‡æ ·ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ¯100æ­¥æ‰“å°ä¸€æ¬¡ï¼‰
        if hasattr(self, 'training_step') and self.training_step % 100 == 0:
            pos_in_batch = sum(1 for exp in sampled_batch if exp['reward'] > reward_threshold)
            neg_in_batch = sum(1 for exp in sampled_batch if exp['reward'] < reward_threshold)
            neu_in_batch = len(sampled_batch) - pos_in_batch - neg_in_batch
            
            print(f"  ğŸ“Š Balanced sampling: Pos={pos_in_batch}, Neg={neg_in_batch}, Neutral={neu_in_batch}")
            print(f"     Buffer stats: Pos={total_positive}, Neg={total_negative}, Neutral={total_neutral}")
        
        return sampled_batch


    def _assign_evbalanced_sample(self, batch_size: int):
        experiences = list(self.experience_buffer)






    def _action_balanced_sample(self, batch_size: int, ifEV = False):
        """
        åŸºäºåŠ¨ä½œç±»å‹çš„å¹³è¡¡é‡‡æ ·ï¼Œè§£å†³åŠ¨ä½œåˆ†å¸ƒä¸å¹³è¡¡é—®é¢˜
        ç¡®ä¿assignã€idleã€chargeåŠ¨ä½œåœ¨è®­ç»ƒbatchä¸­æœ‰åˆç†çš„æ¯”ä¾‹
        """
        experiences = list(self.experience_buffer)
        
        # æŒ‰åŠ¨ä½œç±»å‹åˆ†ç±»
        assign_samples = [exp for exp in experiences if exp['action_type'].startswith('assign')]
        idle_samples = [exp for exp in experiences if exp['action_type'] == 'idle']
        charge_samples = [exp for exp in experiences if exp['action_type'].startswith('charge')]
        
        total_assign = len(assign_samples)
        total_idle = len(idle_samples)
        total_charge = len(charge_samples)
        
        if self.training_step % 100 == 0:  # å‡å°‘æ‰“å°é¢‘ç‡
            print(f"  ğŸ” Action distribution in buffer: Assign={total_assign}, Idle={total_idle}, Charge={total_charge}")
        
        # å¦‚æœæŸç±»åŠ¨ä½œå¤ªå°‘ï¼Œç›´æ¥è¿”å›éšæœºé‡‡æ ·
        if total_assign == 0 and total_idle == 0:
            return random.sample(experiences, min(batch_size, len(experiences)))
        
        # è®¡ç®—é‡‡æ ·æ¯”ä¾‹ - å¼ºåˆ¶å¹³è¡¡
        if total_assign > 0 and total_idle > 0:
            if ifEV:
                assign_ratio = 0.8
                idle_ratio = 0.1
                charge_ratio = 0.1
            else:
                assign_ratio = 0.6
                idle_ratio = 0.2
                charge_ratio = 0.2
            
            assign_count = min(int(batch_size * assign_ratio), total_assign)
            idle_count = min(int(batch_size * idle_ratio), total_idle)
            charge_count = min(int(batch_size * charge_ratio), total_charge)
            
            # è°ƒæ•´ä»¥ç¡®ä¿æ€»æ•°ä¸è¶…è¿‡batch_size
            total_selected = assign_count + idle_count + charge_count
            if total_selected > batch_size:
                # æŒ‰æ¯”ä¾‹ç¼©å‡
                scale = batch_size / total_selected
                assign_count = int(assign_count * scale)
                idle_count = int(idle_count * scale)
                charge_count = batch_size - assign_count - idle_count
                charge_count = max(0, min(charge_count, total_charge))
        
        elif total_assign > 0:
            # åªæœ‰assignæ—¶ï¼Œä¼˜å…ˆé‡‡æ ·assign
            assign_count = min(int(batch_size * 0.7), total_assign)
            idle_count = 0
            charge_count = min(batch_size - assign_count, total_charge)
        else:
            # åªæœ‰idleæ—¶ï¼Œé‡‡æ ·idle
            assign_count = 0
            idle_count = min(int(batch_size * 0.5), total_idle)
            charge_count = min(batch_size - idle_count, total_charge)
        
        # æ‰§è¡Œé‡‡æ ·
        sampled_batch = []
        
        if assign_count > 0:
            sampled_batch.extend(random.sample(assign_samples, assign_count))
        
        if idle_count > 0:
            sampled_batch.extend(random.sample(idle_samples, idle_count))
            
        if charge_count > 0:
            sampled_batch.extend(random.sample(charge_samples, charge_count))
        
        # å¦‚æœé‡‡æ ·æ•°é‡ä¸è¶³ï¼Œä»å‰©ä½™ç»éªŒä¸­è¡¥å……
        remaining_needed = batch_size - len(sampled_batch)
        if remaining_needed > 0:
            remaining_experiences = [exp for exp in experiences if exp not in sampled_batch]
            if remaining_experiences:
                additional_samples = random.sample(
                    remaining_experiences, 
                    min(remaining_needed, len(remaining_experiences))
                )
                sampled_batch.extend(additional_samples)
        
        # æ‰“å°é‡‡æ ·ç»“æœç»Ÿè®¡
        if self.training_step % 100 == 0:  # æ¯100æ­¥æ‰“å°è¯¦ç»†ä¿¡æ¯
            final_assign = sum(1 for exp in sampled_batch if exp['action_type'].startswith('assign'))
            final_idle = sum(1 for exp in sampled_batch if exp['action_type'] == 'idle')
            final_charge = sum(1 for exp in sampled_batch if exp['action_type'].startswith('charge'))
            
            print(f"  ğŸ“Š Action-balanced batch: Assign={final_assign}, Idle={final_idle}, Charge={final_charge}")
            print(f"     Target ratios achieved: Assign={final_assign/len(sampled_batch):.1%}, Idle={final_idle/len(sampled_batch):.1%}")
        
        return sampled_batch

    def train_step(self, batch_size: int = 64, tau: float = 0.005,ifEV=False):  # è½¯æ›´æ–°ç³»æ•°ï¼Œæ¨è0.001~0.01ï¼Œå¯è°ƒ
        """Perform one training step using stored experiences with proper DQN algorithm"""
        if len(self.experience_buffer) < batch_size   :  # Wait for more experiences
            return 0.0
        

        batch = self._action_balanced_sample(batch_size,ifEV)

        
        # å¦‚æœå¹³è¡¡é‡‡æ ·å¤±è´¥ï¼Œå›é€€åˆ°éšæœºé‡‡æ ·
        if not batch or len(batch) < batch_size // 2:
            batch = random.sample(list(self.experience_buffer), min(batch_size, len(self.experience_buffer)))

        # ç»Ÿè®¡æ‰¹æ¬¡ä¸­æ‹’ç»å’Œæ¥å—è®¢å•çš„distanceï¼ˆAEVç‰ˆæœ¬ï¼‰
        reject_distances = []
        accept_distances = []
        reject_details = []  # å­˜å‚¨æ‹’ç»è®¢å•çš„è¯¦ç»†ä¿¡æ¯
        accept_details = []  # å­˜å‚¨æ¥å—è®¢å•çš„è¯¦ç»†ä¿¡æ¯
        
        for exp in batch:
            if 'pickup_dist' in exp:
                dist = exp['pickup_dist']
                reward = exp['reward']
                action_type = exp.get('action_type', 'unknown')
                
                if reward < 0:  # æ‹’ç»è®¢å•ï¼ˆè´Ÿå¥–åŠ±ï¼‰
                    reject_distances.append(dist)
                    reject_details.append({
                        'dist': dist,
                        'reward': reward,
                        'action': action_type,
                        'vehicle_id': exp.get('vehicle_id', -1)
                    })
                else:  # æ¥å—è®¢å•ï¼ˆæ­£å¥–åŠ±ï¼‰
                    accept_distances.append(dist)
                    accept_details.append({
                        'dist': dist,
                        'reward': reward,
                        'action': action_type,
                        'vehicle_id': exp.get('vehicle_id', -1)
                    })
        
        # æ¯100ä¸ªtraining stepæ‰“å°ä¸€æ¬¡ç»Ÿè®¡ä¿¡æ¯
        # if self.training_step % 100 == 0 and (reject_distances or accept_distances):
        #     print(f"\nğŸ“Š AEV Train Step {self.training_step} - Distance Analysis:")
        #     if reject_distances:
        #         print(f"   âŒ REJECT: n={len(reject_distances)}, "
        #               f"mean={sum(reject_distances)/len(reject_distances):.2f}, "
        #               f"min={min(reject_distances):.1f}, max={max(reject_distances):.1f}")
        #         # æ‰“å°å‰5ä¸ªæ‹’ç»è®¢å•çš„è¯¦ç»†ä¿¡æ¯
        #         print(f"      Sample reject orders (first 5):")
        #         for detail in reject_details[:5]:
        #             print(f"        pickup_dist={detail['dist']:.1f}, reward={detail['reward']:.2f}, "
        #                   f"action={detail['action']}, vehicle={detail['vehicle_id']}")
            
        #     if accept_distances:
        #         print(f"   âœ… ACCEPT: n={len(accept_distances)}, "
        #               f"mean={sum(accept_distances)/len(accept_distances):.2f}, "
        #               f"min={min(accept_distances):.1f}, max={max(accept_distances):.1f}")
        #         # æ‰“å°å‰5ä¸ªæ¥å—è®¢å•çš„è¯¦ç»†ä¿¡æ¯
        #         print(f"      Sample accept orders (first 5):")
        #         for detail in accept_details[:5]:
        #             print(f"        pickup_dist={detail['dist']:.1f}, reward={detail['reward']:.2f}, "
        #                   f"action={detail['action']}, vehicle={detail['vehicle_id']}")
            
        #     if reject_distances and accept_distances:
        #         reject_mean = sum(reject_distances)/len(reject_distances)
        #         accept_mean = sum(accept_distances)/len(accept_distances)
        #         print(f"   ğŸ” Difference: reject_mean - accept_mean = {reject_mean - accept_mean:.2f}")

        # Separate current states and next states for batch processing
        current_states = []
        next_states = []
        rewards = []
        


        for exp in batch:
            # Current state - ä½¿ç”¨æ”¯æŒbatteryå’Œrequest_valueçš„è¾“å…¥å‡†å¤‡æ–¹æ³•
            current_battery = exp.get('battery_level', 0.5)  # å‘åå…¼å®¹
            current_request_value = exp.get('request_value', 0.0)  # æå–è¯·æ±‚ä»·å€¼
            current_target_distance = exp.get('target_distance', 0)  # å½“å‰ç›®æ ‡è·ç¦»
            current_target_zoneid = exp.get('target_zoneid', 0)  # å½“å‰ç›®æ ‡åŒºåŸŸID
            current_inputs = self._prepare_network_input_with_battery(
                exp['vehicle_location'], exp['target_location'], exp['current_time'], 
                exp['other_vehicles'], exp['num_requests'], exp['action_type'], 
                current_battery, current_request_value, current_target_distance, current_target_zoneid
            )
            
            # å¤„ç†è¿”å›çš„è¾“å…¥ï¼ˆç°åœ¨åŒ…å«batteryã€request_valueã€target_distanceã€target_zoneidï¼‰
            if len(current_inputs) == 9:  # ğŸ†• å®Œæ•´è¾“å…¥ï¼šåŒ…å«æ‰€æœ‰ç‰¹å¾
                current_path_locations, current_path_delays, current_time_tensor, current_others_tensor, current_requests_tensor, current_battery_tensor, current_value_tensor, current_distance_tensor, current_zoneid_tensor = current_inputs
            elif len(current_inputs) == 7:  # å‘åå…¼å®¹ï¼šåŒ…å«batteryå’Œrequest_value
                current_path_locations, current_path_delays, current_time_tensor, current_others_tensor, current_requests_tensor, current_battery_tensor, current_value_tensor = current_inputs
                current_distance_tensor = torch.tensor([[0.0]], dtype=torch.float32).to(self.device)
                current_zoneid_tensor = torch.tensor([[0]], dtype=torch.long).to(self.device)
            elif len(current_inputs) == 6:  # åŒ…å«batteryä½†æ²¡æœ‰request_value
                current_path_locations, current_path_delays, current_time_tensor, current_others_tensor, current_requests_tensor, current_battery_tensor = current_inputs
                current_value_tensor = torch.tensor([[0.0]], dtype=torch.float32).to(self.device)
                current_distance_tensor = torch.tensor([[0.0]], dtype=torch.float32).to(self.device)
                current_zoneid_tensor = torch.tensor([[0]], dtype=torch.long).to(self.device)
            else:  # ä¸åŒ…å«batteryå’Œrequest_valueï¼ˆå‘åå…¼å®¹ï¼‰
                current_path_locations, current_path_delays, current_time_tensor, current_others_tensor, current_requests_tensor = current_inputs
                current_battery_tensor = torch.tensor([[1.0]], dtype=torch.float32).to(self.device)
                current_value_tensor = torch.tensor([[0.0]], dtype=torch.float32).to(self.device)
                current_distance_tensor = torch.tensor([[0.0]], dtype=torch.float32).to(self.device)
                current_zoneid_tensor = torch.tensor([[0]], dtype=torch.long).to(self.device)
            
            current_states.append({
                'path_locations': current_path_locations.squeeze(0),
                'path_delays': current_path_delays.squeeze(0),
                'current_time': current_time_tensor.squeeze(0),
                'other_agents': current_others_tensor.squeeze(0),
                'num_requests': current_requests_tensor.squeeze(0),
                'battery_level': current_battery_tensor.squeeze(0),  # æ·»åŠ batteryä¿¡æ¯
                'request_value': current_value_tensor.squeeze(0),  # æ·»åŠ request_valueä¿¡æ¯
                'target_distance': current_distance_tensor.squeeze(0),  # ğŸ†• æ·»åŠ target_distance
                'target_zoneid': current_zoneid_tensor.squeeze(0),  # ğŸ†• æ·»åŠ target_zoneid
                'action_type': exp['action_type'],  # æ·»åŠ action_typeä¿¡æ¯
                'vehicle_id': exp['vehicle_id'],    # æ·»åŠ vehicle_idä¿¡æ¯
                'vehicle_type': exp.get('vehicle_type', 1)  # æ·»åŠ vehicle_typeä¿¡æ¯ï¼ˆå‘åå…¼å®¹ï¼‰
            })
            
            # Next state (for target calculation) - ä½¿ç”¨æ”¯æŒbatteryå’Œrequest_valueçš„è¾“å…¥å‡†å¤‡æ–¹æ³•
            next_battery = exp.get('next_battery_level', 1.0)  # å‘åå…¼å®¹
            next_request_value = exp.get('next_request_value', 0.0)  # ä¸‹ä¸€çŠ¶æ€è¯·æ±‚ä»·å€¼
            next_action_type = exp.get('next_action_type', exp['action_type'])  # è·å–ä¸‹ä¸€ä¸ªåŠ¨ä½œç±»å‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å½“å‰åŠ¨ä½œç±»å‹ä½œä¸ºå¤‡ç”¨
            next_target_distance = exp.get('next_target_distance', 0)  # ä¸‹ä¸€çŠ¶æ€ç›®æ ‡è·ç¦»
            next_target_zoneid = exp.get('next_target_zoneid', 0)  # ä¸‹ä¸€çŠ¶æ€ç›®æ ‡åŒºåŸŸID
            next_inputs = self._prepare_network_input_with_battery(
                exp['next_vehicle_location'], exp['target_location'], 
                exp['current_time'] + 1, exp['other_vehicles'], exp['num_requests'], 
                next_action_type, next_battery, next_request_value, next_target_distance, next_target_zoneid
            )
            
            
            # å¤„ç†next stateçš„è¿”å›å€¼ï¼ˆç°åœ¨åŒ…å«batteryã€request_valueã€target_distanceã€target_zoneidï¼‰
            if len(next_inputs) == 9:  # ğŸ†• å®Œæ•´è¾“å…¥ï¼šåŒ…å«æ‰€æœ‰ç‰¹å¾
                next_path_locations, next_path_delays, next_time_tensor, next_others_tensor, next_requests_tensor, next_battery_tensor, next_value_tensor, next_distance_tensor, next_zoneid_tensor = next_inputs
            elif len(next_inputs) == 7:  # å‘åå…¼å®¹ï¼šåŒ…å«batteryå’Œrequest_value
                next_path_locations, next_path_delays, next_time_tensor, next_others_tensor, next_requests_tensor, next_battery_tensor, next_value_tensor = next_inputs
                next_distance_tensor = torch.tensor([[0.0]], dtype=torch.float32).to(self.device)
                next_zoneid_tensor = torch.tensor([[0]], dtype=torch.long).to(self.device)
            elif len(next_inputs) == 6:  # åŒ…å«batteryä½†æ²¡æœ‰request_value
                next_path_locations, next_path_delays, next_time_tensor, next_others_tensor, next_requests_tensor, next_battery_tensor = next_inputs
                next_value_tensor = torch.tensor([[0.0]], dtype=torch.float32).to(self.device)
                next_distance_tensor = torch.tensor([[0.0]], dtype=torch.float32).to(self.device)
                next_zoneid_tensor = torch.tensor([[0]], dtype=torch.long).to(self.device)
            else:  # ä¸åŒ…å«batteryå’Œrequest_valueï¼ˆå‘åå…¼å®¹ï¼‰
                next_path_locations, next_path_delays, next_time_tensor, next_others_tensor, next_requests_tensor = next_inputs
                next_battery_tensor = torch.tensor([[1.0]], dtype=torch.float32).to(self.device)
                next_value_tensor = torch.tensor([[0.0]], dtype=torch.float32).to(self.device)
                next_distance_tensor = torch.tensor([[0.0]], dtype=torch.float32).to(self.device)
                next_zoneid_tensor = torch.tensor([[0]], dtype=torch.long).to(self.device)
            
            next_states.append({
                'path_locations': next_path_locations.squeeze(0),
                'path_delays': next_path_delays.squeeze(0),
                'current_time': next_time_tensor.squeeze(0),
                'other_agents': next_others_tensor.squeeze(0),
                'num_requests': next_requests_tensor.squeeze(0),
                'battery_level': next_battery_tensor.squeeze(0),  # æ·»åŠ batteryä¿¡æ¯
                'request_value': next_value_tensor.squeeze(0),  # æ·»åŠ request_valueä¿¡æ¯
                'target_distance': next_distance_tensor.squeeze(0),  # ğŸ†• æ·»åŠ target_distance
                'target_zoneid': next_zoneid_tensor.squeeze(0),  # ğŸ†• æ·»åŠ target_zoneid
                'action_type': next_action_type,  # ä½¿ç”¨ä¸‹ä¸€ä¸ªåŠ¨ä½œç±»å‹è€Œä¸æ˜¯å½“å‰åŠ¨ä½œç±»å‹
                'vehicle_id': exp['vehicle_id'],    # æ·»åŠ vehicle_idä¿¡æ¯
                'vehicle_type': exp.get('vehicle_type', 1)  # æ·»åŠ vehicle_typeä¿¡æ¯ï¼ˆå‘åå…¼å®¹ï¼‰
            })
            
            rewards.append(exp['reward'])
        
        # Stack batch inputs for current states
        current_batch_path_locations = torch.stack([state['path_locations'] for state in current_states])
        current_batch_path_delays = torch.stack([state['path_delays'] for state in current_states])
        current_batch_current_time = torch.stack([state['current_time'] for state in current_states])
        current_batch_other_agents = torch.stack([state['other_agents'] for state in current_states])
        current_batch_num_requests = torch.stack([state['num_requests'] for state in current_states])
        current_batch_battery_levels = torch.stack([state['battery_level'] for state in current_states])  # æ·»åŠ batteryæ‰¹å¤„ç†
        current_batch_request_values = torch.stack([state['request_value'] for state in current_states])  # æ·»åŠ request_valueæ‰¹å¤„ç†
        current_batch_target_distances = torch.stack([state['target_distance'] for state in current_states])  # ğŸ†• æ·»åŠ target_distanceæ‰¹å¤„ç†
        current_batch_target_zoneids = torch.stack([state['target_zoneid'] for state in current_states])  # ğŸ†• æ·»åŠ target_zoneidæ‰¹å¤„ç†
        
        # Convert action_type strings to tensors for current states
        current_action_types = []
        current_vehicle_ids = []
        current_vehicle_types = []
        for state in current_states:
            action_type_str = state['action_type']
            if action_type_str == 'idle':
                action_type_id = 1
            elif action_type_str.startswith('assign'):
                action_type_id = 2
            elif action_type_str.startswith('charge'):
                action_type_id = 3
            else:
                action_type_id = 2  # é»˜è®¤ä¸ºassign
            current_action_types.append(action_type_id)
            current_vehicle_ids.append(state['vehicle_id'] + 1)  # +1å› ä¸º0æ˜¯padding
            current_vehicle_types.append(state['vehicle_type'])
        
        current_batch_action_types = torch.tensor(current_action_types, dtype=torch.long).to(self.device)
        current_batch_vehicle_ids = torch.tensor(current_vehicle_ids, dtype=torch.long).to(self.device)
        current_batch_vehicle_types = torch.tensor(current_vehicle_types, dtype=torch.long).to(self.device)
        
        # Stack batch inputs for next states
        next_batch_path_locations = torch.stack([state['path_locations'] for state in next_states])
        next_batch_path_delays = torch.stack([state['path_delays'] for state in next_states])
        next_batch_current_time = torch.stack([state['current_time'] for state in next_states])
        next_batch_other_agents = torch.stack([state['other_agents'] for state in next_states])
        next_batch_num_requests = torch.stack([state['num_requests'] for state in next_states])
        next_batch_battery_levels = torch.stack([state['battery_level'] for state in next_states])  # æ·»åŠ next statesçš„batteryæ‰¹å¤„ç†
        next_batch_request_values = torch.stack([state['request_value'] for state in next_states])  # æ·»åŠ next statesçš„request_valueæ‰¹å¤„ç†
        next_batch_target_distances = torch.stack([state['target_distance'] for state in next_states])  # ğŸ†• æ·»åŠ next statesçš„target_distanceæ‰¹å¤„ç†
        next_batch_target_zoneids = torch.stack([state['target_zoneid'] for state in next_states])  # ğŸ†• æ·»åŠ next statesçš„target_zoneidæ‰¹å¤„ç†
        # Convert action_type strings to tensors for next states

        next_vehicle_ids = []
        next_vehicle_types = []
        next_action_types = []
        for state in next_states:
            action_type_str = state['action_type']
            next_vehicle_ids.append(state['vehicle_id'] )  # +1å› ä¸º0æ˜¯padding
            next_vehicle_types.append(state['vehicle_type'])
            if action_type_str == 'idle':
                action_type_id = 1
            elif action_type_str.startswith('assign'):
                action_type_id = 2
            elif action_type_str.startswith('charge'):
                action_type_id = 3
            else:
                action_type_id = 2  # é»˜è®¤ä¸ºassign
            next_action_types.append(action_type_id)


        next_batch_vehicle_ids = torch.tensor(next_vehicle_ids, dtype=torch.long).to(self.device)
        next_batch_vehicle_types = torch.tensor(next_vehicle_types, dtype=torch.long).to(self.device)
        next_batch_action_types = torch.tensor(next_action_types, dtype=torch.long).to(self.device)
        # Current Q-values (with gradients) - ç°åœ¨åŒ…å«æ‰€æœ‰ç‰¹å¾ä¿¡æ¯
        self.network.train()
        current_q_values = self.network(
            path_locations=current_batch_path_locations,
            path_delays=current_batch_path_delays,
            current_time=current_batch_current_time,
            other_agents=current_batch_other_agents,
            num_requests=current_batch_num_requests,
            battery_level=current_batch_battery_levels,
            request_value=current_batch_request_values,
            target_distance=current_batch_target_distances,  # ğŸ†•
            target_zoneid=current_batch_target_zoneids,      # ğŸ†•
            action_type=current_batch_action_types.unsqueeze(1),
            vehicle_id=current_batch_vehicle_ids.unsqueeze(1),
            vehicle_type=current_batch_vehicle_types.unsqueeze(1)
        )
        
        # Next Q-values using target network (without gradients) - ç°åœ¨åŒ…å«æ‰€æœ‰ç‰¹å¾ä¿¡æ¯
        with torch.no_grad():
            self.target_network.eval()
            next_q_values = self.target_network(
                path_locations=next_batch_path_locations,
                path_delays=next_batch_path_delays,
                current_time=next_batch_current_time,
                other_agents=next_batch_other_agents,
                num_requests=next_batch_num_requests,
                battery_level=next_batch_battery_levels,
                request_value=next_batch_request_values,
                target_distance=next_batch_target_distances,  # ğŸ†•
                target_zoneid=next_batch_target_zoneids,      # ğŸ†•
                action_type=next_batch_action_types.unsqueeze(1),
                vehicle_id=next_batch_vehicle_ids.unsqueeze(1),
                vehicle_type=next_batch_vehicle_types.unsqueeze(1)
            )
        
        # Calculate TD targets without normalization
        gamma = 0.90  # é™ä½æŠ˜æ‰£å› å­ï¼Œå‡å°‘æœªæ¥idleæƒ©ç½šå¯¹å½“å‰å†³ç­–çš„å½±å“
        
        # å¥–åŠ±ç¼©æ”¾ï¼šå°†å¥–åŠ±å€¼ç¼©æ”¾åˆ°æ›´åˆç†çš„èŒƒå›´ï¼Œé¿å…æ•°å€¼è¿‡å¤§å¯¼è‡´Qå€¼ä¸ç¨³å®š
        reward_scale = 1  # å°†å¥–åŠ±ç¼©æ”¾10å€ï¼Œä½¿å¾—10-50çš„è®¢å•ä»·å€¼å˜æˆ1-5
        rewards_tensor = torch.tensor([r * reward_scale for r in rewards], dtype=torch.float32).to(self.device).unsqueeze(1)
        
        # Extract duration times and system done flags from batch
        dur_times = [exp.get('dur_time', 1.0) for exp in batch]
        is_system_done_flags = [exp.get('is_system_done', False) for exp in batch]
        
        dur_times_tensor = torch.tensor(dur_times, dtype=torch.float32).to(self.device).unsqueeze(1)
        is_done_tensor = torch.tensor([1.0 if not done else 0.0 for done in is_system_done_flags], 
                                     dtype=torch.float32).to(self.device).unsqueeze(1)
        
        # Calculate target Q-values with duration-adjusted discount and terminal state handling
        with torch.no_grad():
            target_q_values = rewards_tensor + (gamma ** dur_times_tensor) * next_q_values * is_done_tensor
            
            # æ·»åŠ æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
            if torch.isnan(target_q_values).any() or torch.isinf(target_q_values).any():
                print(f"WARNING: Invalid target Q-values detected!")
                print(f"  Rewards range: [{rewards_tensor.min():.3f}, {rewards_tensor.max():.3f}]")
                print(f"  Next Q-values range: [{next_q_values.min():.3f}, {next_q_values.max():.3f}]")
                return 0.0
            
        # æ·»åŠ æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(current_q_values).any() or torch.isinf(current_q_values).any():
            print(f"WARNING: Invalid current Q-values detected!")
            return 0.0
            
        # Compute loss with raw values
        loss = self.loss_fn(current_q_values, target_q_values)
        loss_value = loss.item()  # Define loss_value immediately after loss computation
        
        # æ¯100ä¸ªtraining stepæˆ–losså¼‚å¸¸æ—¶ï¼Œæ‰“å°è¯¦ç»†çš„lossåˆ†æï¼ˆAEVç‰ˆæœ¬ï¼‰
        # if self.training_step % 200 == 0 or loss_value > 100.0:
        #     with torch.no_grad():
        #         td_errors = (current_q_values - target_q_values).abs()
        #         print(f"\nğŸ“ˆ AEV Loss Analysis at step {self.training_step}:")
        #         print(f"   Loss: {loss_value:.4f}")
        #         print(f"   Current Q: mean={current_q_values.mean().item():.2f}, "
        #               f"std={current_q_values.std().item():.2f}, "
        #               f"range=[{current_q_values.min().item():.2f}, {current_q_values.max().item():.2f}]")
        #         print(f"   Target Q:  mean={target_q_values.mean().item():.2f}, "
        #               f"std={target_q_values.std().item():.2f}, "
        #               f"range=[{target_q_values.min().item():.2f}, {target_q_values.max().item():.2f}]")
        #         print(f"   Rewards:   mean={rewards_tensor.mean().item():.2f}, "
        #               f"range=[{rewards_tensor.min().item():.2f}, {rewards_tensor.max().item():.2f}]")
        #         print(f"   TD Error:  mean={td_errors.mean().item():.2f}, max={td_errors.max().item():.2f}")
                
        #         # ç»Ÿè®¡æ‰¹æ¬¡ä¸­å„ç±»åŠ¨ä½œçš„æ•°é‡å’Œå¥–åŠ±
        #         action_stats = {}
        #         for exp in batch:
        #             action = exp['action_type']
        #             if action not in action_stats:
        #                 action_stats[action] = {'count': 0, 'rewards': []}
        #             action_stats[action]['count'] += 1
        #             action_stats[action]['rewards'].append(exp['reward'])
                
        #         print(f"   Batch composition:")
        #         for action, stats in action_stats.items():
        #             pick_updist = [exp['pickup_dist'] for exp in batch if exp['action_type'] == action and 'pickup_dist' in exp]
        #             avg_reward = sum(stats['rewards']) / len(stats['rewards'])
        #             if pick_updist:
        #                 avg_dist = sum(pick_updist) / len(pick_updist)
        #                 print(f"      {action}: n={stats['count']}, avg_reward={avg_reward:.2f}, "
        #                       f"avg_pickup_dist={avg_dist:.2f} (min={min(pick_updist):.1f}, max={max(pick_updist):.1f})")
        #             else:
        #                 print(f"      {action}: n={stats['count']}, avg_reward={avg_reward:.2f}, pickup_dist=N/A")
        
        # æ£€æŸ¥æŸå¤±æ˜¯å¦å¼‚å¸¸
        if torch.isnan(loss) or torch.isinf(loss):
            print(f"WARNING: Invalid loss detected: {loss_value}")
            return 0.0
        
        # Additional safety check
        if not loss.requires_grad:
            print("WARNING: Loss does not require gradients!")
            return 0.0
        
        # Backpropagation with gradient monitoring
        self.optimizer.zero_grad()
        loss.backward()
        
        # Monitor gradients for debugging
        total_grad_norm = 0.0
        for name, param in self.network.named_parameters():
            if param.grad is not None:
                param_norm = param.grad.data.norm(2)
                total_grad_norm += param_norm.item() ** 2
        total_grad_norm = total_grad_norm ** (1. / 2)
        
        # ä¿®å¤æ¢¯åº¦è£å‰ªï¼šä»0.5å¢åŠ åˆ°10.0ï¼Œé¿å…è¿‡åº¦è£å‰ª
        # åŸæ¥çš„0.5å¤ªå°ï¼Œå¯¼è‡´æ¢¯åº¦è¢«ä¸¥é‡è£å‰ªï¼Œå­¦ä¹ èƒ½åŠ›å—é™
        # torch.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm=10.0)
        self.optimizer.step()
        
        # Update learning rate scheduler based on loss
        self.scheduler.step(loss_value)
        
        # Update target network periodically (key DQN component)
        
        if self.training_step % self.target_update_frequency == 0:
            for target_param, param in zip(self.target_network.parameters(), self.network.parameters()):
                target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)
            print(f"ğŸ”„ Target network soft-updated at step {self.training_step} with tau={tau}")
        
        # Record training metrics
        self.training_losses.append(loss_value)
        
        # Record Q-values statistics
        with torch.no_grad():
            q_mean = current_q_values.mean().item()
            q_std = current_q_values.std().item()
            q_max = current_q_values.max().item()
            q_min = current_q_values.min().item()
            self.q_values_history.append({
                'mean': q_mean, 'std': q_std, 'max': q_max, 'min': q_min
            })
        
        self.training_step += 1
        
        # æ¯10æ­¥è®­ç»ƒä¸€æ¬¡æ‹’ç»é¢„æµ‹å™¨
        # if self.training_step % 10 == 0:
        #     rejection_loss = self.train_rejection_predictor()
        #     if rejection_loss is not None:
        #         print(f"  Rejection predictor loss: {rejection_loss:.4f}")
        
        # Print training progress occasionally  
        if self.training_step % 100 == 0:
            current_lr = self.optimizer.param_groups[0]['lr']
            
            print(f"Training step {self.training_step}: Loss={loss_value:.4f}, Q_mean={q_mean:.4f}, Q_std={q_std:.4f}, Q_range=[{q_min:.4f}, {q_max:.4f}], LR={current_lr:.6f}")
            print(f"  Gradient norm: {total_grad_norm:.4f}, No normalization - using raw Q-values")
        
        return loss_value

    def train_step_supervised(self, env, num_samples: int = 64):
        """
        Supervised training: minimize MSE between optimizer-induced option values (labels)
        and network predictions for the same actions, aligning with src_2's "planner-in-the-loop" idea.

        This does NOT use optimizer to assign vehicles in the environment step; it only calls the
        optimizer here to get a one-shot assignment to define labels. If optimizer is unavailable,
        it falls back to a heuristic assignment.
        """
        import torch
        import random
        from collections import deque

        # Sanity: environment must provide vehicles, requests, and evaluators
        if env is None or not hasattr(env, 'vehicles'):
            return 0.0

        # 1) Build a pool of candidate idle vehicles similar to simulate_motion
        vehicles_to_rebalance = []
        for vehicle_id, vehicle in env.vehicles.items():
            if ((vehicle.get('assigned_request') is None and
                 vehicle.get('passenger_onboard') is None and
                 vehicle.get('charging_station') is None and
                 vehicle.get('idle_target') is None) or
                (vehicle.get('battery', 1.0) <= getattr(env, 'rebalance_battery_threshold', 0.5)) or
                vehicle.get('is_stationary', False)):
                vehicles_to_rebalance.append(vehicle_id)

        if not vehicles_to_rebalance:
            return 0.0

        # 2) Get an assignment mapping using optimizer (preferred) or heuristic fallback
        assignments = {}
        try:
            if not hasattr(env, 'gurobi_optimizer'):
                from src.GurobiOptimizer import GurobiOptimizer
                env.gurobi_optimizer = GurobiOptimizer(env)
            assignments = env.gurobi_optimizer.optimize_vehicle_rebalancing_reject(vehicles_to_rebalance)
        except Exception:
            # Heuristic fallback without modifying optimizer implementation
            try:
                available_requests = list(getattr(env, 'active_requests', {}).values())
                charging_stations = [st for st in getattr(env, 'charging_manager').stations.values() if st.available_slots > 0] if hasattr(env, 'charging_manager') else []
                assignments = env.gurobi_optimizer._heuristic_assignment_with_reject(vehicles_to_rebalance, available_requests, charging_stations)  # type: ignore
            except Exception:
                return 0.0

        if not assignments:
            return 0.0

        # 3) Build supervised mini-batch from assignments (action -> label)
        #    Label = env.evaluate_service_option / evaluate_charging_option (option completion value)
        #    Match the network inputs for those actions
        inputs_list = []
        labels_list = []

        # Helper: pack a single sample
        def _append_sample(vehicle_id: int, action_type: str, veh_loc: int, tgt_loc: int,
                           current_time: float, battery: float, request_value: float, label: float):
            # Raw counts for normalization inside helper
            other_vehicles_raw = max(0, sum(1 for v in env.vehicles.values() if v.get('assigned_request') is None and v.get('passenger_onboard') is None and v.get('charging_station') is None) - 1)
            num_requests_raw = len(getattr(env, 'active_requests', {}))

            # Prepare full set of inputs with battery/request value; tensors are already normalized as in other code paths
            path_locations_b, path_delays_b, time_b, others_b, requests_b, battery_b, value_b = self._prepare_network_input_with_battery(
                veh_loc, tgt_loc, current_time, other_vehicles_raw, num_requests_raw, action_type, battery, request_value
            )
            # Package tensors in expected dict form
            sample = {
                'path_locations': path_locations_b,
                'path_delays': path_delays_b,
                'current_time': time_b,
                'other_agents': others_b,
                'num_requests': requests_b,
                'battery_level': battery_b,
                'request_value': value_b,
                'action_type': torch.tensor([[1 if action_type=='idle' else (2 if action_type.startswith('assign') else 3)]], dtype=torch.long, device=self.device),
                'vehicle_id': torch.tensor([[vehicle_id + 1]], dtype=torch.long, device=self.device),
                'vehicle_type': torch.tensor([[1 if vehicle_id % 2 == 0 else 2]], dtype=torch.long, device=self.device)
            }
            inputs_list.append(sample)
            labels_list.append(label)

        # Fill samples from assignments
        for vehicle_id, target in assignments.items():
            vehicle = env.vehicles.get(vehicle_id)
            if not vehicle:
                continue
            veh_loc = vehicle.get('location', 0)
            battery = vehicle.get('battery', 1.0)

            # Service assignment
            if target and hasattr(target, 'pickup') and hasattr(target, 'dropoff'):
                try:
                    label_val = env.evaluate_service_option(vehicle_id, target)
                except Exception:
                    label_val = 0.0
                # Use pickup as target_location for the assign action
                tgt_loc = getattr(target, 'pickup', veh_loc)
                req_val = getattr(target, 'final_value', getattr(target, 'value', 0.0))
                _append_sample(vehicle_id, f"assign_{getattr(target, 'request_id', 0)}", veh_loc, tgt_loc, env.current_time, battery, req_val, label_val)
            # Charging assignment
            elif target and hasattr(target, 'id') and hasattr(target, 'location'):
                try:
                    label_val = env.evaluate_charging_option(vehicle_id, target)
                except Exception:
                    label_val = 0.0
                tgt_loc = getattr(target, 'location', veh_loc)
                _append_sample(vehicle_id, f"charge_{getattr(target, 'id', 0)}", veh_loc, tgt_loc, env.current_time, battery, 0.0, label_val)
            else:
                # Idle/no-op: optionally skip, to focus on meaningful supervised labels
                continue

        if not inputs_list:
            return 0.0
        
        # Downsample to num_samples if necessary
        if len(inputs_list) > num_samples:
            idxs = random.sample(range(len(inputs_list)), num_samples)
            inputs_list = [inputs_list[i] for i in idxs]
            labels_list = [labels_list[i] for i in idxs]

        # 4) Batch the tensors and train with MSE
        def _stack(key):
            return torch.cat([s[key] for s in inputs_list], dim=0)

        self.network.train()
        preds = self.network(
            path_locations=_stack('path_locations'),
            path_delays=_stack('path_delays'),
            current_time=_stack('current_time'),
            other_agents=_stack('other_agents'),
            num_requests=_stack('num_requests'),
            battery_level=_stack('battery_level'),
            request_value=_stack('request_value'),
            action_type=_stack('action_type'),
            vehicle_id=_stack('vehicle_id'),
            vehicle_type=_stack('vehicle_type')
        )

        labels_tensor = torch.tensor(labels_list, dtype=torch.float32, device=self.device).unsqueeze(1)
        loss = self.loss_fn(preds, labels_tensor)

        # Optimize
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm=10.0)
        self.optimizer.step()

        loss_value = float(loss.item())
        self.training_losses.append(loss_value)
        self.training_step += 1

        # Track Q stats
        with torch.no_grad():
            self.q_values_history.append({
                'mean': preds.mean().item(),
                'std': preds.std().item(),
                'max': preds.max().item(),
                'min': preds.min().item()
            })

        return loss_value
    
    def get_value(self, experiences: List[Experience]) -> List[List[Tuple[Action, float]]]:
        """Compatibility method for Experience-based interface"""
        # This is a simplified implementation for compatibility
        return []
    
    def update(self, *args, **kwargs):
        """Update the neural network"""
        if len(self.experience_buffer) > 100:
            loss = self.train_step()
            self.add_to_logs('training_loss', loss, self.training_step)
            self.training_step += 1
    
    def remember(self, experience: Experience):
        """ç®€åŒ–çš„ç»éªŒå­˜å‚¨ï¼Œä¾èµ–Environmentè¿›è¡Œç­›é€‰"""
        try:
            # ä»Experienceä¸­æå–ç›¸å…³ä¿¡æ¯
            for agent_id, actions_info in experience.action_to_take_all_agents.items():
                if not actions_info:
                    continue
                
                action, reward = actions_info[0] if len(actions_info) > 0 else (None, 0.0)
                if action is None:
                    continue
                
                # è·å–å½“å‰çŠ¶æ€ä¿¡æ¯
                current_state = experience.current_states.get(agent_id)
                next_state = experience.next_states.get(agent_id) if hasattr(experience, 'next_states') else None
                
                if current_state is None:
                    continue
                
                # åˆ›å»ºç®€åŒ–çš„ç»éªŒè®°å½•
                enhanced_experience = {
                    'vehicle_id': agent_id,
                    'vehicle_location': getattr(current_state, 'location', 0),
                    'target_location': 0,  # å°†ä»actionä¸­æå–
                    'current_time': experience.current_time,
                    'reward': reward,
                    'next_vehicle_location': getattr(next_state, 'location', 0) if next_state else 0,
                    'other_vehicles': len(experience.current_states) - 1,
                    'num_requests': len(getattr(experience, 'active_requests', [])),
                    'battery_level': getattr(current_state, 'battery', 1.0),
                    'next_battery_level': getattr(next_state, 'battery', 1.0) if next_state else 1.0,
                    'request_value': 0.0,
                    'action_type': 'idle',  # é»˜è®¤å€¼
                }
                
                # æ ¹æ®actionç±»å‹æ›´æ–°ç›¸å…³ä¿¡æ¯
                if hasattr(action, 'requests') and action.requests:
                    # Service action
                    request = list(action.requests)[0]
                    enhanced_experience['target_location'] = getattr(request, 'pickup', 0)
                    enhanced_experience['request_value'] = getattr(request, 'final_value', 0.0)
                    enhanced_experience['action_type'] = 'assign'
                elif hasattr(action, 'charging_station_id'):
                    # Charging action
                    enhanced_experience['action_type'] = 'charge'
                    enhanced_experience['target_location'] = getattr(action, 'charging_station_id', 0)
                else:
                    # Idle action
                    enhanced_experience['action_type'] = 'idle'
                    if hasattr(action, 'target_coords'):
                        target_x, target_y = action.target_coords
                        grid_size = int(math.sqrt(enhanced_experience['vehicle_location']) + 1)
                        enhanced_experience['target_location'] = target_y * grid_size + target_x
                
                # ç›´æ¥å­˜å‚¨ï¼Œä¾èµ–Environmentè¿›è¡Œé¢„ç­›é€‰
                self.experience_buffer.append(enhanced_experience)
                    
                # å®šæœŸè¿›è¡Œè®­ç»ƒ
                if len(self.experience_buffer) % 50 == 0:
                    self.train_step(batch_size=32)
                        
        except Exception as e:
            print(f"Warning: Error in simplified remember method: {e}")
            pass
    
    def _evaluate_assignment_quality(self, action, reward: float) -> float:
        """
        é‡æ–°å®šä¹‰åˆ†é…è´¨é‡è¯„ä¼°ï¼šä¸“æ³¨äºè®¢å•å®Œæˆèƒ½åŠ›
        æˆåŠŸçš„assignment = èƒ½å¤Ÿå®Œæˆæ•´ä¸ªæœåŠ¡æµç¨‹çš„åˆ†é…
        """
        # 1. æœ€é«˜ä¼˜å…ˆçº§ï¼šå®é™…å®Œæˆäº†è®¢å•ï¼ˆè·å¾—äº†final_valueå¥–åŠ±ï¼‰
        if reward >= 15:  # å®Œæˆè®¢å•çš„å…¸å‹å¥–åŠ±èŒƒå›´
            return 1.0  # å®Œç¾è´¨é‡ - è¿™æ˜¯æˆ‘ä»¬æœ€æƒ³å­¦ä¹ çš„ç»éªŒ
        
        # 2. é«˜ä¼˜å…ˆçº§ï¼šéƒ¨åˆ†å®Œæˆä½†æœ‰æ­£å‘è¿›å±•
        elif reward >= 5:  # å¯èƒ½å®Œæˆäº†pickupä½†è¿˜æœªdropoff
            return 0.8  # é«˜è´¨é‡ - å±•ç¤ºäº†å®Œæˆèƒ½åŠ›
        
        # 3. ä¸­ç­‰ä¼˜å…ˆçº§ï¼šæˆåŠŸåˆ†é…ä½†è¿˜åœ¨æ‰§è¡Œä¸­
        elif reward > 0:  # æˆåŠŸåˆ†é…ï¼Œæ­£åœ¨æ‰§è¡Œ
            return 0.6  # ä¸­ç­‰è´¨é‡ - æœ‰æ½œåŠ›å®Œæˆ
        
        # 4. ä½ä¼˜å…ˆçº§ï¼šåˆ†é…è¢«æ‹’ç»æˆ–å¤±è´¥
        elif reward == 0:  # åˆ†é…å¤±è´¥æˆ–è¢«æ‹’ç»
            return 0.2  # ä½è´¨é‡ - å¯ä»¥å­¦ä¹ ä¸ºä»€ä¹ˆå¤±è´¥
        
        # 5. è´Ÿé¢æ¡ˆä¾‹ï¼šç”µæ± è€—å°½ã€æ— æ³•å®Œæˆç­‰
        else:  # è´Ÿå¥–åŠ± - ç”µæ± è€—å°½ã€ä¹˜å®¢æ»ç•™ç­‰
            return 0.0  # é›¶è´¨é‡ - é¿å…å­¦ä¹ è¿™ç±»ç»éªŒ
    
    def _analyze_competitive_context(self, experience: Experience) -> float:
        """åˆ†æç«äº‰ç¯å¢ƒä¸Šä¸‹æ–‡"""
        num_vehicles = len(experience.current_states) if hasattr(experience, 'current_states') else 1
        num_requests = len(getattr(experience, 'active_requests', []))
        
        if num_requests == 0:
            return 0.0  # æ— è¯·æ±‚ç¯å¢ƒ
        
        competition_ratio = num_vehicles / num_requests
        if competition_ratio > 2.0:
            return 1.0  # é«˜ç«äº‰
        elif competition_ratio > 1.0:
            return 0.6  # ä¸­ç­‰ç«äº‰
        else:
            return 0.2  # ä½ç«äº‰

    def _assess_order_completion_potential(self, action, current_state, reward: float) -> float:
        """
        è¯„ä¼°è®¢å•å®Œæˆæ½œåŠ›ï¼šé¢„æµ‹è¿™ä¸ªåˆ†é…å†³ç­–èƒ½å¦æˆåŠŸå®Œæˆè®¢å•
        """
        # åŸºç¡€å®Œæˆæ½œåŠ›è¯„ä¼°
        completion_potential = 0.0
        
        # 1. ç”µæ± å……è¶³åº¦å¯¹å®Œæˆæ½œåŠ›çš„å½±å“
        battery_level = getattr(current_state, 'battery', 1.0)
        if battery_level > 0.5:
            completion_potential += 0.4  # é«˜ç”µé‡ = é«˜å®Œæˆæ½œåŠ›
        elif battery_level > 0.3:
            completion_potential += 0.2  # ä¸­ç­‰ç”µé‡ = ä¸­ç­‰å®Œæˆæ½œåŠ›
        else:
            completion_potential += 0.0  # ä½ç”µé‡ = ä½å®Œæˆæ½œåŠ›
        
        # 2. å¦‚æœæ˜¯assignment actionï¼Œè€ƒè™‘è·ç¦»å› ç´ 
        if hasattr(action, 'requests') and action.requests:
            request = list(action.requests)[0]
            pickup_location = getattr(request, 'pickup', 0)
            current_location = getattr(current_state, 'location', 0)
            
            # ç®€åŒ–çš„è·ç¦»è®¡ç®—ï¼ˆå‡è®¾grid_size=40ï¼‰
            grid_size = 40
            pickup_x, pickup_y = pickup_location % grid_size, pickup_location // grid_size
            current_x, current_y = current_location % grid_size, current_location // grid_size
            distance = abs(pickup_x - current_x) + abs(pickup_y - current_y)
            
            # è·ç¦»è¶Šè¿‘ï¼Œå®Œæˆæ½œåŠ›è¶Šé«˜
            if distance <= 3:
                completion_potential += 0.3  # å¾ˆè¿‘
            elif distance <= 6:
                completion_potential += 0.2  # è¾ƒè¿‘
            elif distance <= 10:
                completion_potential += 0.1  # ä¸­ç­‰è·ç¦»
            # è¿œè·ç¦»ä¸åŠ åˆ†
        
        # 3. å®é™…å¥–åŠ±åé¦ˆçš„å®Œæˆæ½œåŠ›
        if reward >= 15:  # å·²å®Œæˆè®¢å•
            completion_potential = 1.0  # ç¡®å®šå®Œæˆ
        elif reward >= 5:  # éƒ¨åˆ†å®Œæˆ
            completion_potential = max(completion_potential, 0.8)
        elif reward > 0:  # æ­£åœ¨æ‰§è¡Œ
            completion_potential = max(completion_potential, 0.6)
        
        return min(1.0, completion_potential)
    
    def _is_order_completion_valuable_experience(self, experience: dict) -> bool:
        """
        ä¸¥æ ¼æ§åˆ¶experienceå­˜å‚¨ï¼šåªå­˜å‚¨å…³é”®å†³ç­–ç‚¹
        - å®Œæˆè®¢å•çš„experienceï¼ˆæœ€ç»ˆæ”¶ç›Šï¼‰
        - å……ç”µå†³ç­–çš„experience  
        - idleç§»åŠ¨å†³ç­–çš„experience
        - æ’é™¤pickup/dropoffæ‰§è¡Œè¿‡ç¨‹ä¸­çš„experience
        """
        reward = experience['reward']
        action_type = experience['action_type']
        assignment_quality = experience['assignment_quality']
        
        # 1. ã€æœ€é«˜ä¼˜å…ˆçº§ã€‘å®Œæˆè®¢å•çš„experience - è¿™æ˜¯æœ€ç»ˆçš„æˆåŠŸå†³ç­–ç»“æœ
        if reward >= 15 and assignment_quality >= 0.8:
            print(f"âœ“ Storing COMPLETED ORDER experience: reward={reward}, vehicle={experience['vehicle_id']}")
            return True
        
        # 2. ã€å……ç”µå†³ç­–ã€‘- ç”µæ± ç®¡ç†çš„å…³é”®å†³ç­–ç‚¹
        if action_type.startswith('charge'):
            battery_level = experience.get('battery_level', 1.0)
            # åªå­˜å‚¨çœŸæ­£éœ€è¦å……ç”µçš„å†³ç­–ï¼ˆä½ç”µé‡ï¼‰
            if battery_level < 0.5:
                print(f"âœ“ Storing CHARGING decision experience: battery={battery_level}, vehicle={experience['vehicle_id']}")
                return True
            return False
        
        # 3. ã€Idleå†³ç­–ã€‘- ç©ºé—²çŠ¶æ€çš„ç§»åŠ¨å†³ç­–
        if action_type == 'idle':
            # å­˜å‚¨æ‰€æœ‰idleå†³ç­–ï¼Œå› ä¸ºè¿™äº›æ˜¯é‡è¦çš„å®šä½å†³ç­–
            return True
        
        # 4. ã€åˆå§‹assignmentå†³ç­–ã€‘- åªå­˜å‚¨åˆšå¼€å§‹åˆ†é…çš„å†³ç­–ï¼Œä¸å­˜å‚¨æ‰§è¡Œè¿‡ç¨‹
        if action_type.startswith('assign'):
            # åªå­˜å‚¨çœŸæ­£çš„åˆ†é…å†³ç­–æ—¶åˆ»ï¼ˆé«˜è´¨é‡æˆ–è´Ÿé¢æ•™è®­ï¼‰
            if assignment_quality >= 0.6:  # æˆåŠŸçš„åˆ†é…å†³ç­–
                print(f"âœ“ Storing SUCCESSFUL assignment decision: quality={assignment_quality}, reward={reward}")
                return True
            elif assignment_quality == 0.0 and reward <= 0:  # å¤±è´¥çš„åˆ†é…å†³ç­–ï¼ˆå­¦ä¹ æ•™è®­ï¼‰
                print(f"âœ“ Storing FAILED assignment decision for learning: quality={assignment_quality}, reward={reward}")
                return True
            else:
                # æ’é™¤æ‰§è¡Œè¿‡ç¨‹ä¸­çš„ä¸­é—´çŠ¶æ€ï¼ˆpickupè¿›è¡Œä¸­ã€dropoffè¿›è¡Œä¸­ç­‰ï¼‰
                return False
        
        # 5. å…¶ä»–æƒ…å†µï¼šä¸å­˜å‚¨
        return False
    
    def plot_training_metrics(self, save_path: str = None):
        """Plot training losses and Q-values over time"""
        import matplotlib.pyplot as plt
        
        if not self.training_losses:
            print("No training data to plot")
            return
        
        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 10))
        
        # Plot training loss
        ax1.plot(self.training_losses, label='Training Loss', color='red', alpha=0.7)
        ax1.set_title('Neural Network Training Loss')
        ax1.set_xlabel('Training Steps')
        ax1.set_ylabel('MSE Loss')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Plot Q-values mean
        if self.q_values_history:
            q_means = [q['mean'] for q in self.q_values_history]
            q_stds = [q['std'] for q in self.q_values_history]
            
            ax2.plot(q_means, label='Q-value Mean', color='blue', alpha=0.7)
            ax2.fill_between(range(len(q_means)), 
                           [m - s for m, s in zip(q_means, q_stds)],
                           [m + s for m, s in zip(q_means, q_stds)],
                           alpha=0.2, color='blue', label='Â±1 Std')
            ax2.set_title('Q-Values Statistics')
            ax2.set_xlabel('Training Steps')
            ax2.set_ylabel('Q-Value')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            
            # Plot Q-values standard deviation
            ax3.plot(q_stds, label='Q-value Std Dev', color='green', alpha=0.7)
            ax3.set_title('Q-Values Standard Deviation')
            ax3.set_xlabel('Training Steps')
            ax3.set_ylabel('Standard Deviation')
            ax3.legend()
            ax3.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"Training metrics plot saved to: {save_path}")
        else:
            plt.savefig('results/training_metrics.png', dpi=300, bbox_inches='tight')
            print("Training metrics plot saved to: results/training_metrics.png")
        
        plt.show()
        return fig


class PyTorchPathBasedNetwork(nn.Module):
    """PyTorch implementation of path-based neural network with action type and vehicle embedding"""
    
    def __init__(self, 
                 num_locations: int,
                 num_vehicles: int,
                 max_capacity: int,
                 embedding_dim: int = 100,
                 lstm_hidden: int = 200,
                 dense_hidden: int = 300,
                 pretrained_embeddings: Optional[torch.Tensor] = None):
        super(PyTorchPathBasedNetwork, self).__init__()
        
        self.num_locations = num_locations
        self.num_vehicles = num_vehicles
        self.max_capacity = max_capacity
        self.embedding_dim = embedding_dim
        
        # Location embedding layer
        self.location_embedding = nn.Embedding(
            num_embeddings=num_locations + 1,
            embedding_dim=embedding_dim,
            padding_idx=0
        )
        
        # Vehicle ID embedding layer
        self.vehicle_embedding = nn.Embedding(
            num_embeddings=num_vehicles + 1,  # +1 for padding/unknown vehicles
            embedding_dim=embedding_dim // 4,  # è¾ƒå°çš„ç»´åº¦ï¼Œä¸“æ³¨äºè½¦è¾†ç‰¹å¾
            padding_idx=0
        )
        
        # Vehicle type embedding layer (EV vs AEV)
        # 0: unknown, 1: EV, 2: AEV
        self.vehicle_type_embedding = nn.Embedding(
            num_embeddings=3,
            embedding_dim=embedding_dim // 4,
            padding_idx=0
        )
        
        # ğŸ†• Target location embedding layer (for pickup/dropoff/charge station locations)
        # å°†target locationç¼–ç ä¸ºembeddingï¼Œä¸path locationå…±äº«ç›¸åŒçš„embeddingç©ºé—´
        self.target_location_embedding = nn.Embedding(
            num_embeddings=num_locations + 1,
            embedding_dim=embedding_dim // 4,
            padding_idx=0
        )
        
        # ğŸ†• Zone ID embedding layer (for geographical zones)
        # å‡è®¾æœ€å¤šæœ‰100ä¸ªzone (0 for padding, 1-100 for actual zones)
        self.zone_embedding = nn.Embedding(
            num_embeddings=101,
            embedding_dim=embedding_dim // 8,
            padding_idx=0
        )

        # Action type embedding layer
        # 0: padding, 1: idle, 2: assign, 3: charge
        self.action_type_embedding = nn.Embedding(
            num_embeddings=4,
            embedding_dim=embedding_dim // 2,
            padding_idx=0
        )
        
        # Initialize with pretrained embeddings if available
        if pretrained_embeddings is not None:
            self.location_embedding.weight.data.copy_(pretrained_embeddings)
            self.location_embedding.weight.requires_grad = False
        
        # LSTM for path processing
        self.path_lstm = nn.LSTM(
            input_size=embedding_dim + 1,  # embedding + delay
            hidden_size=lstm_hidden,
            batch_first=True
        )
        
        # Time embedding
        self.time_embedding = nn.Sequential(
            nn.Linear(1, embedding_dim),
            nn.ELU()
        )
        
        # Context embedding for action-specific features
        self.context_embedding = nn.Sequential(
            nn.Linear(2, embedding_dim // 2),  # battery + request_value
            nn.ELU(),
            
        )
        
        # Vehicle-specific feature processing
        self.vehicle_feature_embedding = nn.Sequential(
            nn.Linear(embedding_dim // 4 + embedding_dim // 4, embedding_dim // 2),  # vehicle_id + vehicle_type
            nn.ELU(),
            
        )
        
        # ğŸ†• Target feature processing (target_location + distance + zone)
        # è¾“å…¥: target_location_embed (dim//4) + distance (1) + zone_embed (dim//8)
        self.target_feature_embedding = nn.Sequential(
            nn.Linear(embedding_dim // 4 + 1 + embedding_dim // 8, embedding_dim // 2),
            nn.ELU(),
            
        )
        
        # State embedding layers - åŒ…å«æ‰€æœ‰ç‰¹å¾
        state_input_dim = (lstm_hidden + embedding_dim + 2 +      # path + time + other_agents + num_requests
                          embedding_dim // 2 +                    # action_type_embedding
                          embedding_dim // 2 +                    # context_embedding (battery + request_value)
                          embedding_dim // 2 +                    # vehicle_feature_embedding (vehicle_id + type)
                          embedding_dim // 2)                     # ğŸ†• target_feature_embedding (target + distance + zone)
        
        self.state_embedding = nn.Sequential(
            nn.Linear(state_input_dim, dense_hidden),
            nn.ELU(),
            
            nn.Linear(dense_hidden, dense_hidden),
            nn.ELU(),
            
            nn.Linear(dense_hidden, 1)
        )
        
        # Initialize weights
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """Initialize network weights"""
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.LSTM):
            for name, param in module.named_parameters():
                if 'weight' in name:
                    nn.init.xavier_uniform_(param)
                elif 'bias' in name:
                    nn.init.zeros_(param)
    
    def forward(self, 
                path_locations: torch.Tensor,
                path_delays: torch.Tensor,
                current_time: torch.Tensor,
                other_agents: torch.Tensor,
                num_requests: torch.Tensor,
                battery_level: torch.Tensor = None,
                request_value: torch.Tensor = None,
                target_distance: torch.Tensor = None,
                target_zoneid: torch.Tensor = None,
                action_type: torch.Tensor = None,
                vehicle_id: torch.Tensor = None,
                vehicle_type: torch.Tensor = None) -> torch.Tensor:
        """
        Forward pass through the network
        
        Args:
            path_locations: [batch_size, seq_len] - Location IDs in path
            path_delays: [batch_size, seq_len, 1] - Delay information
            current_time: [batch_size, 1] - Current time
            other_agents: [batch_size, 1] - Number of other agents nearby
            num_requests: [batch_size, 1] - Number of current requests
            battery_level: [batch_size, 1] - Battery level (0-1), optional
            request_value: [batch_size, 1] - Request value (0-1), optional
            target_distance: [batch_size, 1] - ğŸ†• Manhattan distance to target (normalized 0-1), optional
            target_zoneid: [batch_size, 1] - ğŸ†• Zone ID of target location (0-100), optional
            action_type: [batch_size, 1] - Action type (1=idle, 2=assign, 3=charge), optional
            vehicle_id: [batch_size, 1] - Vehicle ID (1-num_vehicles), optional
            vehicle_type: [batch_size, 1] - Vehicle type (1=EV, 2=AEV), optional
        """
        batch_size = path_locations.size(0)
        
        # ğŸ†• æå–target location (å‡è®¾æ˜¯pathçš„æœ€åä¸€ä¸ªéé›¶ä½ç½®)
        # å¯¹äºassignåŠ¨ä½œï¼Œtargetæ˜¯pickup locationï¼›å¯¹äºchargeåŠ¨ä½œï¼Œtargetæ˜¯station location
        mask = (path_locations != 0).long()
        seq_lengths = mask.sum(dim=1)  # [batch_size]
        target_location_ids = torch.zeros(batch_size, dtype=torch.long, device=path_locations.device)
        for i in range(batch_size):
            if seq_lengths[i] > 0:
                target_location_ids[i] = path_locations[i, seq_lengths[i]-1]
        
        # ğŸ†• å¤„ç†target distance
        if target_distance is None:
            target_distance = torch.zeros(batch_size, 1).to(path_locations.device)
        
        # ğŸ†• å¤„ç†target zone ID
        if target_zoneid is None:
            target_zoneid = torch.zeros(batch_size, 1, dtype=torch.long).to(path_locations.device)
        else:
            target_zoneid = target_zoneid.long()
        
        # Get location embeddings
        location_embeds = self.location_embedding(path_locations)  # [batch_size, seq_len, embedding_dim]
        
        # Create mask for padding
        mask = (path_locations != 0).float().unsqueeze(-1)  # [batch_size, seq_len, 1]
        
        # Apply mask to delays  
        masked_delays = path_delays * mask
        
        # Combine location embeddings with delays
        path_input = torch.cat([location_embeds, masked_delays], dim=-1)  # [batch_size, seq_len, embedding_dim + 1]
        
        # Process through LSTM
        lstm_out, (hidden, _) = self.path_lstm(path_input)  # [batch_size, seq_len, lstm_hidden]
        
        # Use final hidden state as path representation
        path_representation = hidden[-1]  # [batch_size, lstm_hidden]
        
        # Process time
        time_embed = self.time_embedding(current_time)  # [batch_size, embedding_dim]
        
        # Handle battery level - å¦‚æœæ²¡æœ‰æä¾›battery_levelï¼Œä½¿ç”¨é»˜è®¤å€¼1.0
        if battery_level is None:
            battery_level = torch.ones(current_time.size()).to(current_time.device)
        
        # Handle request value - å¦‚æœæ²¡æœ‰æä¾›request_valueï¼Œä½¿ç”¨é»˜è®¤å€¼0.0
        if request_value is None:
            request_value = torch.zeros(current_time.size()).to(current_time.device)
        
        # Handle vehicle_id - å¦‚æœæ²¡æœ‰æä¾›ï¼Œä½¿ç”¨é»˜è®¤å€¼1
        if vehicle_id is None:
            vehicle_id = torch.ones(current_time.size(), dtype=torch.long).to(current_time.device)
        
        # Handle vehicle_type - å¦‚æœæ²¡æœ‰æä¾›ï¼Œé»˜è®¤ä¸ºEV (1)
        if vehicle_type is None:
            vehicle_type = torch.ones(current_time.size(), dtype=torch.long).to(current_time.device)
        
        # Handle action type - å¦‚æœæ²¡æœ‰æä¾›action_typeï¼Œå°è¯•ä»è·¯å¾„æ¨æ–­
        if action_type is None:
            # ä»è·¯å¾„æ¨¡å¼æ¨æ–­action type
            # idle: è·¯å¾„ä¸­ç¬¬ä¸€ä¸ªä½ç½® == ç¬¬äºŒä¸ªä½ç½®
            # assign/charge: è·¯å¾„ä¸­ç¬¬ä¸€ä¸ªä½ç½® != ç¬¬äºŒä¸ªä½ç½®
            is_idle = (path_locations[:, 0] == path_locations[:, 1])  # ä¿æŒä¸ºå¸ƒå°”å¼ é‡
            action_type = torch.where(is_idle, 
                                    torch.ones(is_idle.size(), dtype=torch.long, device=is_idle.device),  # idle = 1
                                    torch.full(is_idle.size(), 2, dtype=torch.long, device=is_idle.device))  # assign/charge = 2
            action_type = action_type.unsqueeze(1)  # [batch_size, 1]
        
        # Get embeddings
        action_embed = self.action_type_embedding(action_type.squeeze(1))  # [batch_size, embedding_dim//2]
        vehicle_id_embed = self.vehicle_embedding(vehicle_id.squeeze(1))   # [batch_size, embedding_dim//4]
        vehicle_type_embed = self.vehicle_type_embedding(vehicle_type.squeeze(1))  # [batch_size, embedding_dim//4]
        
        # Process context features (battery + request_value)
        context_features = torch.cat([battery_level, request_value], dim=1)  # [batch_size, 2]
        context_embed = self.context_embedding(context_features)  # [batch_size, embedding_dim//2]
        
        # Process vehicle features (vehicle_id + vehicle_type)
        vehicle_features = torch.cat([vehicle_id_embed, vehicle_type_embed], dim=1)  # [batch_size, embedding_dim//2]
        vehicle_embed = self.vehicle_feature_embedding(vehicle_features)  # [batch_size, embedding_dim//2]
        
        # ğŸ†• Process target features (target_location + distance + zone)
        target_loc_embed = self.target_location_embedding(target_location_ids)  # [batch_size, embedding_dim//4]
        target_zone_embed = self.zone_embedding(target_zoneid.squeeze(1))  # [batch_size, embedding_dim//8]
        target_features = torch.cat([target_loc_embed, target_distance, target_zone_embed], dim=1)  # [batch_size, dim//4 + 1 + dim//8]
        target_embed = self.target_feature_embedding(target_features)  # [batch_size, embedding_dim//2]
        
        # Combine all features
        combined_features = torch.cat([
            path_representation,     # [batch_size, lstm_hidden]
            time_embed,             # [batch_size, embedding_dim]
            other_agents,           # [batch_size, 1]
            num_requests,           # [batch_size, 1]
            action_embed,           # [batch_size, embedding_dim//2]
            context_embed,          # [batch_size, embedding_dim//2]
            vehicle_embed,          # [batch_size, embedding_dim//2]
            target_embed            # ğŸ†• [batch_size, embedding_dim//2]
        ], dim=1)  # [batch_size, total_features]
        
        # Get final value prediction
        value = self.state_embedding(combined_features)  # [batch_size, 1]
        
        return value


class PyTorchNeuralNetworkBased(PyTorchValueFunction):
    """PyTorch implementation of neural network-based value function"""
    
    def __init__(self,
                 envt: Environment,
                 num_locations: int,
                 max_capacity: int,
                 load_model_path: str = '',
                 log_dir: str = 'logs/nn_based',
                 gamma: float = 0.99,
                 learning_rate: float = 1e-3,
                 batch_size_fit: int = 32,
                 batch_size_predict: int = 1024,
                 target_update_tau: float = 0.001,
                 device: str = 'cpu'):
        
        super().__init__(log_dir, device)
        
        # Environment and hyperparameters
        self.envt = envt
        self.num_locations = num_locations
        self.max_capacity = max_capacity
        self.gamma = gamma
        self.batch_size_fit = batch_size_fit
        self.batch_size_predict = batch_size_predict
        self.target_update_tau = target_update_tau
        
        # Initialize networks
        self.value_network = self._init_network().to(self.device)
        self.target_network = self._init_network().to(self.device)
        
        # Load pretrained model if specified
        if load_model_path and PathlibPath(load_model_path).exists():
            self.load_model(load_model_path)
        
        # Copy weights to target network
        self.target_network.load_state_dict(self.value_network.state_dict())
        
        # Optimizer
        self.optimizer = optim.Adam(self.value_network.parameters(), lr=learning_rate)
        
        # Replay buffer
        buffer_size = max(int(1e6 / getattr(envt, 'NUM_AGENTS', 10)), 10000)
        self.replay_buffer = PyTorchReplayBuffer(buffer_size, str(self.device))
        
    def _init_network(self) -> PyTorchPathBasedNetwork:
        """Initialize the neural network"""
        # Try to load pretrained embeddings
        pretrained_embeddings = None
        if hasattr(self.envt, 'DATA_DIR'):
            embedding_path = PathlibPath(self.envt.DATA_DIR) / 'embedding_weights.pkl'
            if embedding_path.exists():
                try:
                    with open(embedding_path, 'rb') as f:
                        weights = pickle.load(f)
                    pretrained_embeddings = torch.FloatTensor(weights[0])
                except Exception as e:
                    logging.warning(f"Failed to load pretrained embeddings: {e}")
        
        return PyTorchPathBasedNetwork(
            num_locations=self.num_locations,
            max_capacity=self.max_capacity,
            pretrained_embeddings=pretrained_embeddings
        )
    
    def _format_input_batch(self, experiences: List[Experience]) -> Dict[str, torch.Tensor]:
        """Format experiences into network input tensors"""
        batch_size = len(experiences)
        max_seq_len = self.max_capacity * 2 + 1
        
        # Initialize tensors
        path_locations = torch.zeros(batch_size, max_seq_len, dtype=torch.long)
        path_delays = torch.zeros(batch_size, max_seq_len, 1, dtype=torch.float32)
        current_times = torch.zeros(batch_size, 1, dtype=torch.float32)
        other_agents = torch.zeros(batch_size, 1, dtype=torch.float32)
        num_requests = torch.zeros(batch_size, 1, dtype=torch.float32)
        
        for i, experience in enumerate(experiences):
            # Extract time information
            if hasattr(experience, 'time'):
                normalized_time = self._normalize_time(experience.time)
                current_times[i, 0] = normalized_time
            
            # Extract request count
            if hasattr(experience, 'num_requests'):
                normalized_requests = experience.num_requests / getattr(self.envt, 'NUM_AGENTS', 10)
                num_requests[i, 0] = normalized_requests
            
            # Process agents (simplified - take first agent for demo)
            if hasattr(experience, 'agents') and len(experience.agents) > 0:
                agent = experience.agents[0]
                
                # Extract path information
                if hasattr(agent, 'path') and hasattr(agent.path, 'request_order'):
                    self._extract_path_features(agent, path_locations[i], path_delays[i])
                
                # Count other agents (simplified)
                other_agents[i, 0] = len(experience.agents) / getattr(self.envt, 'NUM_AGENTS', 10)
        
        return {
            'path_locations': path_locations.to(self.device),
            'path_delays': path_delays.to(self.device),
            'current_time': current_times.to(self.device),
            'other_agents': other_agents.to(self.device),
            'num_requests': num_requests.to(self.device)
        }
    
    def _normalize_time(self, time: float) -> float:
        """Normalize time to [0, 1] range"""
        start_time = getattr(self.envt, 'START_EPOCH', 0)
        end_time = getattr(self.envt, 'STOP_EPOCH', 86400)
        return (time - start_time) / (end_time - start_time) if end_time > start_time else 0.0
    
    def _extract_path_features(self, agent: LearningAgent, 
                              path_locations: torch.Tensor, 
                              path_delays: torch.Tensor):
        """Extract path features from agent"""
        # Add current location
        if hasattr(agent, 'position') and hasattr(agent.position, 'next_location'):
            path_locations[0] = agent.position.next_location + 1
            path_delays[0, 0] = 1.0
        
        # Add path nodes (simplified extraction)
        if hasattr(agent, 'path') and hasattr(agent.path, 'request_order'):
            for idx, node in enumerate(agent.path.request_order):
                if idx >= len(path_locations) - 1:
                    break
                
                # Extract location and delay information
                try:
                    location, deadline = agent.path.get_info(node)
                    visit_time = getattr(node, 'expected_visit_time', 0)
                    
                    path_locations[idx + 1] = location + 1
                    # Normalize delay
                    max_delay = getattr(Request, 'MAX_DROPOFF_DELAY', 3600)
                    normalized_delay = (deadline - visit_time) / max_delay
                    path_delays[idx + 1, 0] = normalized_delay
                except Exception:
                    # Handle cases where path info extraction fails
                    break
    
    def get_value(self, experiences: List[Experience], 
                 use_target: bool = False) -> List[List[Tuple[Action, float]]]:
        """Get value estimates for experiences"""
        if not experiences:
            return []
        
        # Format input batch
        inputs = self._format_input_batch(experiences)
        
        # Get network predictions
        network = self.target_network if use_target else self.value_network
        network.eval()
        
        with torch.no_grad():
            values = network(**inputs)  # [batch_size, 1]
            values = values.cpu().numpy().flatten()
        
        # Format output to match expected interface
        scored_actions_all_agents = []
        value_idx = 0
        
        for experience in experiences:
            if hasattr(experience, 'feasible_actions_all_agents'):
                for feasible_actions in experience.feasible_actions_all_agents:
                    scored_actions = []
                    for action in feasible_actions:
                        # Get immediate reward
                        immediate_reward = self._get_immediate_reward(action)
                        
                        # Add discounted future value
                        future_value = values[value_idx] if value_idx < len(values) else 0.0
                        total_value = immediate_reward + self.gamma * future_value
                        
                        scored_actions.append((action, total_value))
                    
                    scored_actions_all_agents.append(scored_actions)
                    value_idx += 1
        
        return scored_actions_all_agents
    
    def _get_immediate_reward(self, action: Action) -> float:
        """Get immediate reward for an action"""
        if hasattr(self.envt, 'get_reward'):
            return self.envt.get_reward(action)
        elif hasattr(action, 'requests'):
            return sum([getattr(req, 'value', 0) for req in action.requests])
        else:
            return 0.0
    
    def remember(self, experience: Experience):
        """Store experience in replay buffer"""
        self.replay_buffer.add(experience)
    
    def update(self, central_agent: CentralAgent, num_samples: int = 3):
        """Update value function using sampled experiences"""
        # Check if enough experiences for training
        min_samples = max(self.batch_size_fit, 100)
        if len(self.replay_buffer) < min_samples:
            return
        
        # Sample experiences
        experiences, weights, indices = self.replay_buffer.sample(num_samples)
        if not experiences:
            return
        
        # Prepare training data
        self.value_network.train()
        
        # Get current value predictions
        inputs = self._format_input_batch(experiences)
        current_values = self.value_network(**inputs)
        
        # Get target values using target network
        target_values = self._compute_target_values(experiences, central_agent)
        
        # Compute loss with importance sampling
        weights_tensor = torch.FloatTensor(weights).to(self.device).unsqueeze(1)
        td_errors = F.mse_loss(current_values, target_values, reduction='none')
        weighted_loss = (td_errors * weights_tensor).mean()
        
        # Backward pass
        self.optimizer.zero_grad()
        weighted_loss.backward()
        # ä¿®å¤æ¢¯åº¦è£å‰ªï¼šä»1.0å¢åŠ åˆ°10.0ï¼Œé¿å…è¿‡åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.value_network.parameters(), max_norm=10.0)
        self.optimizer.step()
        
        # Update target network
        self._soft_update_target()
        
        # Update replay buffer priorities
        priorities = td_errors.detach().cpu().numpy().flatten() + 1e-6
        self.replay_buffer.update_priorities(indices, priorities.tolist())
        
        # Log training statistics
        self.add_to_logs('loss', weighted_loss.item(), self.training_step)
        self.add_to_logs('mean_value', current_values.mean().item(), self.training_step)
        self.training_step += 1
    
    def _compute_target_values(self, experiences: List[Experience], 
                              central_agent: CentralAgent) -> torch.Tensor:
        """Compute target values for training"""
        target_values = []
        
        for experience in experiences:
            # Get next state values using target network
            next_state_values = self.get_value([experience], use_target=True)
            
            # Use central agent to get optimal actions and their values
            if next_state_values and hasattr(central_agent, 'choose_actions'):
                try:
                    optimal_actions = central_agent.choose_actions(
                        next_state_values, 
                        is_training=False
                    )
                    target_value = sum([score for _, score in optimal_actions]) / len(optimal_actions)
                except Exception:
                    target_value = 0.0
            else:
                target_value = 0.0
            
            target_values.append(target_value)
        
        return torch.FloatTensor(target_values).unsqueeze(1).to(self.device)
    
    def _soft_update_target(self):
        """Soft update of target network"""
        for target_param, param in zip(self.target_network.parameters(), 
                                     self.value_network.parameters()):
            target_param.data.copy_(
                self.target_update_tau * param.data + 
                (1 - self.target_update_tau) * target_param.data
            )
    
    def save_model(self, filepath: str):
        """Save model state"""
        torch.save({
            'value_network': self.value_network.state_dict(),
            'target_network': self.target_network.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'training_step': self.training_step
        }, filepath)
    
    def load_model(self, filepath: str):
        """Load model state"""
        checkpoint = torch.load(filepath, map_location=self.device, weights_only=False)
        self.value_network.load_state_dict(checkpoint['value_network'])
        self.target_network.load_state_dict(checkpoint['target_network'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        self.training_step = checkpoint.get('training_step', 0)


# Compatibility aliases for existing code
ValueFunction = PyTorchValueFunction
RewardPlusDelay = PyTorchRewardPlusDelay
ImmediateReward = lambda: PyTorchRewardPlusDelay(delay_coefficient=0)
NeuralNetworkBased = PyTorchNeuralNetworkBased
PathBasedNN = PyTorchNeuralNetworkBased


def main():
    """Test the PyTorch value function implementation"""
    logging.basicConfig(level=logging.INFO)
    
    # Create dummy environment for testing
    class DummyEnvironment:
        NUM_AGENTS = 5
        NUM_LOCATIONS = 100
        MAX_CAPACITY = 4
        START_EPOCH = 0
        STOP_EPOCH = 86400
        DATA_DIR = 'data/'
        
        def get_reward(self, action):
            return random.uniform(0, 10)
    
    env = DummyEnvironment()
    
    # Test PyTorch value function
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    value_function = PyTorchNeuralNetworkBased(
        envt=env,
        num_locations=env.NUM_LOCATIONS,
        max_capacity=env.MAX_CAPACITY,
        device=device
    )
    
    print("PyTorch Value Function initialized successfully!")
    print(f"Network parameters: {sum(p.numel() for p in value_function.value_network.parameters())}")
    print(f"Device: {device}")


# =============================================================================
# DQN Implementation for Benchmark Comparison
# =============================================================================

class DQNActionNetwork(nn.Module):
    """
    Deep Q-Network for action selection in vehicle dispatch
    Provides a benchmark for comparison with ILP-ADP approach
    """
    def __init__(self, state_dim=64, action_dim=32, hidden_dim=128, device='cpu'):
        super(DQNActionNetwork, self).__init__()
        self.device = device
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # Feature encoders for different input modalities
        self.vehicle_encoder = nn.Sequential(
            nn.Linear(8, hidden_dim//2),  # vehicle_id, type, location, battery, etc.
            nn.ReLU(),
            nn.Linear(hidden_dim//2, hidden_dim//4)
        )
        
        self.request_encoder = nn.Sequential(
            nn.Linear(6, hidden_dim//2),  # pickup, dropoff, time, value, etc.
            nn.ReLU(),
            nn.Linear(hidden_dim//2, hidden_dim//4)
        )
        
        self.global_encoder = nn.Sequential(
            nn.Linear(4, hidden_dim//4),  # num_vehicles, num_requests, time, etc.
            nn.ReLU(),
            nn.Linear(hidden_dim//4, hidden_dim//8)
        )
        
        # Main DQN network (Dueling architecture)
        total_feature_dim = hidden_dim//4 + hidden_dim//4 + hidden_dim//8
        
        self.feature_layer = nn.Sequential(
            nn.Linear(total_feature_dim, hidden_dim),
            nn.ReLU(),
            
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        # Dueling DQN: separate value and advantage streams
        self.value_stream = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.ReLU(),
            nn.Linear(hidden_dim//2, 1)
        )
        
        self.advantage_stream = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.ReLU(),
            nn.Linear(hidden_dim//2, action_dim)
        )
        
        self.to(device)
    
    def forward(self, vehicle_features, request_features, global_features):
        """
        Forward pass through the DQN
        
        Args:
            vehicle_features: Tensor of vehicle state features
            request_features: Tensor of request features
            global_features: Tensor of global environment features
        
        Returns:
            Q-values for all possible actions
        """
        # Encode different feature types
        vehicle_encoded = self.vehicle_encoder(vehicle_features)
        request_encoded = self.request_encoder(request_features)
        global_encoded = self.global_encoder(global_features)
        
        # Concatenate all features
        combined_features = torch.cat([vehicle_encoded, request_encoded, global_encoded], dim=-1)
        
        # Main feature processing
        features = self.feature_layer(combined_features)
        
        # Dueling streams
        value = self.value_stream(features)
        advantage = self.advantage_stream(features)
        
        # Combine value and advantage (dueling DQN formula)
        q_values = value + advantage - advantage.mean(dim=-1, keepdim=True)
        
        return q_values
    
    def get_action(self, vehicle_features, request_features, global_features, epsilon=0.0):
        """
        Select action using epsilon-greedy policy
        
        Args:
            vehicle_features: Vehicle state features
            request_features: Request features
            global_features: Global environment features
            epsilon: Exploration probability
        
        Returns:
            Selected action index and Q-values
        """
        if random.random() < epsilon:
            # Random action for exploration
            action = random.randint(0, self.action_dim - 1)
            with torch.no_grad():
                q_values = self.forward(vehicle_features, request_features, global_features)
            return action, q_values
        else:
            # Greedy action selection
            with torch.no_grad():
                q_values = self.forward(vehicle_features, request_features, global_features)
                action = q_values.argmax(dim=-1).item()
            return action, q_values


class DQNExperienceReplay:
    """Experience replay buffer for DQN training"""
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        """Add experience to buffer"""
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        """Sample random batch from buffer"""
        batch = random.sample(self.buffer, batch_size)
        return zip(*batch)
    
    def __len__(self):
        return len(self.buffer)


class DQNAgent:
    """
    DQN Agent for vehicle dispatch decision making
    Serves as benchmark comparison for ILP-ADP approach
    """
    def __init__(self, state_dim=64, action_dim=32, lr=1e-4, gamma=0.99, 
                 epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=1000,
                 target_update=100, device='cpu'):
        self.device = device
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon_start = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        self.target_update = target_update
        self.steps_done = 0
        
        # Networks
        self.policy_net = DQNActionNetwork(state_dim, action_dim, device=device)
        self.target_net = DQNActionNetwork(state_dim, action_dim, device=device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()
        
        # Optimizer and loss
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)
        self.criterion = nn.MSELoss()
        
        # Experience replay
        self.memory = DQNExperienceReplay(capacity=10000)
        
        # Training statistics
        self.training_stats = {
            'episode_rewards': [],
            'episode_lengths': [],
            'avg_q_values': [],
            'losses': []
        }
    
    def get_epsilon(self):
        """Calculate current epsilon for exploration"""
        epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \
                 math.exp(-1. * self.steps_done / self.epsilon_decay)
        return epsilon
    
    def select_action(self, vehicle_features, request_features, global_features, training=True, force_idle_constraint=False):
        """
        Select action for given state
        
        Args:
            vehicle_features: Vehicle state tensor
            request_features: Request features tensor  
            global_features: Global environment tensor
            training: Whether in training mode (affects exploration)
            force_idle_constraint: Whether to prioritize idle actions due to constraint
        
        Returns:
            action: Selected action index
            q_values: Q-values for all actions
        """
        epsilon = self.get_epsilon() if training else 0.0
        self.steps_done += 1
        
        # å¦‚æœæœ‰idleçº¦æŸï¼Œä¿®æ”¹åŠ¨ä½œé€‰æ‹©ç­–ç•¥
        if force_idle_constraint:
            # è·å–æ‰€æœ‰Qå€¼
            action, q_values = self.policy_net.get_action(vehicle_features, request_features, 
                                            global_features, epsilon=0.0)  # ä¸ä½¿ç”¨éšæœºæ¢ç´¢
            
            # è¯†åˆ«idle/waitåŠ¨ä½œçš„ç´¢å¼•ï¼ˆé€šå¸¸åœ¨åŠ¨ä½œç©ºé—´çš„æœ«å°¾ï¼‰
            # å‡è®¾åŠ¨ä½œç©ºé—´ï¼š[assign_0, assign_1, ..., assign_9, rebalance_0, ..., rebalance_9, charge_0, ..., charge_4, wait_0, wait_1, idle]
            idle_action_indices = list(range(28, 32))  # waitå’ŒidleåŠ¨ä½œçš„ç´¢å¼•èŒƒå›´
            
            # è®¡ç®—idleåŠ¨ä½œçš„å¹³å‡Qå€¼
            idle_q_values = [q_values[i].item() if i < len(q_values) else -1000 for i in idle_action_indices]
            best_idle_idx = idle_action_indices[idle_q_values.index(max(idle_q_values))]
            
            # ä½¿ç”¨é«˜æ¦‚ç‡é€‰æ‹©idleåŠ¨ä½œï¼Œä½†ä»ä¿ç•™ä¸€å®šçš„æ¢ç´¢æ€§
            if training and epsilon > 0:
                # 80%æ¦‚ç‡é€‰æ‹©æœ€ä½³idleåŠ¨ä½œï¼Œ20%æ¦‚ç‡æ­£å¸¸é€‰æ‹©
                if torch.rand(1).item() < 0.8:
                    return best_idle_idx, q_values
            else:
                # éè®­ç»ƒæ¨¡å¼ä¸‹ç›´æ¥é€‰æ‹©æœ€ä½³idleåŠ¨ä½œ
                return best_idle_idx, q_values
        
        # æ­£å¸¸çš„åŠ¨ä½œé€‰æ‹©
        return self.policy_net.get_action(vehicle_features, request_features, 
                                        global_features, epsilon)
    
    def store_transition(self, state, action, reward, next_state, done):
        """Store experience in replay buffer"""
        self.memory.push(state, action, reward, next_state, done)
    
    def train_step(self, batch_size=32):
        """
        Perform one training step
        
        Args:
            batch_size: Size of training batch
            
        Returns:
            loss: Training loss value
        """
        if len(self.memory) < batch_size:
            return None
        
        # Sample batch from memory
        states, actions, rewards, next_states, dones = self.memory.sample(batch_size)
        
        # Convert to tensors
        batch_states = {
            'vehicle': torch.stack([s['vehicle'] for s in states]).to(self.device),
            'request': torch.stack([s['request'] for s in states]).to(self.device),
            'global': torch.stack([s['global'] for s in states]).to(self.device)
        }
        
        batch_next_states = {
            'vehicle': torch.stack([s['vehicle'] for s in next_states]).to(self.device),
            'request': torch.stack([s['request'] for s in next_states]).to(self.device),
            'global': torch.stack([s['global'] for s in next_states]).to(self.device)
        }
        
        actions = torch.tensor(actions, dtype=torch.long).to(self.device)
        rewards = torch.tensor(rewards, dtype=torch.float).to(self.device)
        dones = torch.tensor(dones, dtype=torch.bool).to(self.device)
        
        # Current Q-values
        current_q_values = self.policy_net(
            batch_states['vehicle'], 
            batch_states['request'], 
            batch_states['global']
        ).gather(1, actions.unsqueeze(1))
        
        # Next Q-values from target network
        with torch.no_grad():
            next_q_values = self.target_net(
                batch_next_states['vehicle'],
                batch_next_states['request'], 
                batch_next_states['global']
            ).max(1)[0]
            target_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        # Compute loss
        loss = self.criterion(current_q_values.squeeze(), target_q_values)
        
        # Optimize
        self.optimizer.zero_grad()
        loss.backward()
        
        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), max_norm=1.0)
        
        self.optimizer.step()
        
        # Update target network
        if self.steps_done % self.target_update == 0:
            self.target_net.load_state_dict(self.policy_net.state_dict())
        
        # Store training stats
        self.training_stats['losses'].append(loss.item())
        avg_q = current_q_values.mean().item()
        self.training_stats['avg_q_values'].append(avg_q)
        
        return loss.item()
    
    def save_model(self, filepath):
        """Save trained model"""
        torch.save({
            'policy_net_state_dict': self.policy_net.state_dict(),
            'target_net_state_dict': self.target_net.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'training_stats': self.training_stats,
            'steps_done': self.steps_done
        }, filepath)
    
    def load_model(self, filepath):
        """Load trained model"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])
        self.target_net.load_state_dict(checkpoint['target_net_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.training_stats = checkpoint['training_stats']
        self.steps_done = checkpoint['steps_done']


def create_dqn_state_features(environment, vehicle_id, current_time=0.0):
    """
    Convert environment state to DQN input features
    
    Args:
        environment: Environment instance
        vehicle_id: ID of the vehicle
        current_time: Current simulation time
    
    Returns:
        dict: State features for DQN input
    """
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # Get vehicle information
    vehicle = environment.vehicles.get(vehicle_id, {})
    vehicle_location = vehicle.get('location', 0)
    vehicle_type = vehicle.get('type', 1)
    battery_level = vehicle.get('battery', 1.0)
    is_idle = vehicle.get('idle', True)
    
    # Vehicle features: [id, type, location, battery, is_idle, x_coord, y_coord, capacity]
    vehicle_features = torch.tensor([
        vehicle_id / 100.0,  # Normalized vehicle ID
        vehicle_type / 2.0,  # Normalized vehicle type (1 or 2)
        vehicle_location / float(environment.NUM_LOCATIONS if hasattr(environment, 'NUM_LOCATIONS') else 50),
        battery_level,
        1.0 if is_idle else 0.0,
        (vehicle_location % 10) / 10.0,  # X coordinate (assuming grid layout)
        (vehicle_location // 10) / 10.0,  # Y coordinate
        environment.MAX_CAPACITY / 10.0 if hasattr(environment, 'MAX_CAPACITY') else 1.0
    ], dtype=torch.float32).to(device)
    
    # Request features (using active requests if available)
    active_requests = getattr(environment, 'active_requests', None)
    pickup_loc = 0
    dropoff_loc = 0
    request_value = 0.0
    request_time = current_time
    distance = 0
    urgency = 0.0

    # Convert to a list if it's a dict; environment.active_requests is often a dict {id: Request}
    if active_requests:
        if isinstance(active_requests, dict):
            req_list = list(active_requests.values())
        else:
            req_list = list(active_requests)

        if len(req_list) > 0:
            # Use the first available request as a simple representative feature
            req = req_list[0]
            pickup_loc = getattr(req, 'pickup', 0)
            dropoff_loc = getattr(req, 'dropoff', 0)
            # Prefer final_value if available, otherwise fallback to value
            request_value = getattr(req, 'final_value', getattr(req, 'value', 1.0))
            # We don't track a per-request creation time here; default to current_time
            request_time = current_time
            distance = abs(int(pickup_loc) - int(vehicle_location)) if isinstance(vehicle_location, (int, float)) else 0
            # Simple urgency proxy (no timestamp available): constant mid-level urgency
            urgency = 0.5
    
    # Request features: [pickup, dropoff, value, time, distance, urgency]
    request_features = torch.tensor([
        pickup_loc / float(environment.NUM_LOCATIONS if hasattr(environment, 'NUM_LOCATIONS') else 50),
        dropoff_loc / float(environment.NUM_LOCATIONS if hasattr(environment, 'NUM_LOCATIONS') else 50),
        request_value / 100.0,  # Normalized request value
        (current_time % 1440) / 1440.0,  # Normalized time (assuming daily cycle)
        distance / float(environment.NUM_LOCATIONS if hasattr(environment, 'NUM_LOCATIONS') else 50),
        urgency
    ], dtype=torch.float32).to(device)
    
    # Global features: [num_vehicles, num_requests, current_time, avg_battery]
    num_vehicles = len(environment.vehicles) if hasattr(environment, 'vehicles') else 1
    num_requests = len(active_requests)
    avg_battery = sum(v.get('battery', 1.0) for v in environment.vehicles.values()) / num_vehicles if hasattr(environment, 'vehicles') and environment.vehicles else 1.0
    
    global_features = torch.tensor([
        num_vehicles / 100.0,  # Normalized number of vehicles
        num_requests / 50.0,   # Normalized number of requests
        (current_time % 1440) / 1440.0,  # Normalized time
        avg_battery
    ], dtype=torch.float32).to(device)
    
    return {
        'vehicle': vehicle_features,
        'request': request_features,
        'global': global_features
    }


if __name__ == "__main__":
    main()
