import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import logging
import networkx as nx
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Optional, Tuple, List, Dict, Any
from torch_geometric.nn import GCNConv, GATConv, SAGEConv, global_mean_pool, global_add_pool
from torch_geometric.data import Data, Batch
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from matplotlib.patches import FancyBboxPatch, Circle, Arrow
import matplotlib.patches as patches

# ä» gnn_models å¯¼å…¥ç°æœ‰çš„ç¼–ç å™¨å’ŒæŸå¤±å‡½æ•°ç±»
from .gnn_models import GCNEncoder, GATEncoder, GraphSAGEEncoder, ContrastiveLoss, FairnessLoss




class VariationalDeepGraphClustering(nn.Module):

    
    def __init__(self, input_dim: int, embedding_dim: int, num_clusters: int,
                 encoder_type: str = 'gcn', hidden_dims: List[int] = [64, 32],
                 dropout: float = 0.1, temperature: float = 0.1,
                 fairness_beta: float = 1.0, use_frank_wolfe: bool = False,
                 initial_lambda: float = 0.5, learning_rate: float = 0.001,
                 kl_weight: float = 0.1, use_batch_norm: bool = True):
        """
        åˆå§‹åŒ–å˜åˆ†æ·±åº¦å›¾èšç±»æ¨¡å‹ã€‚
        
        Args:
            input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
            embedding_dim: åµŒå…¥ç»´åº¦ï¼ˆæ½œåœ¨ç©ºé—´ç»´åº¦ï¼‰
            num_clusters: èšç±»æ•°é‡
            encoder_type: ç¼–ç å™¨ç±»å‹ ('gcn', 'gat', 'sage')
            hidden_dims: éšè—å±‚ç»´åº¦åˆ—è¡¨
            dropout: Dropoutç‡
            temperature: å¯¹æ¯”å­¦ä¹ æ¸©åº¦å‚æ•°
            fairness_beta: å…¬å¹³æ€§æŸå¤±betaå‚æ•°
            use_frank_wolfe: æ˜¯å¦ä½¿ç”¨Frank-Wolfeç®—æ³•
            initial_lambda: æŸå¤±ç»„åˆåˆå§‹lambdaå€¼
            learning_rate: å­¦ä¹ ç‡
            kl_weight: KLæ•£åº¦æŸå¤±æƒé‡
            use_batch_norm: æ˜¯å¦ä½¿ç”¨æ‰¹æ ‡å‡†åŒ–
        """
        super(VariationalDeepGraphClustering, self).__init__()
        self.input_dim = input_dim
        self.num_clusters = num_clusters
        self.embedding_dim = embedding_dim
        self.kl_weight = kl_weight
        self.use_frank_wolfe = use_frank_wolfe
        self.learning_rate = learning_rate
        self.use_batch_norm = use_batch_norm
        self.encoder_type = encoder_type
        self.dropout = dropout
        self.hidden_dims = hidden_dims
        # Lambdaå‚æ•°ç”¨äºæŸå¤±ç»„åˆ
        if use_frank_wolfe:
            self.lambda_param = nn.Parameter(torch.tensor(initial_lambda, requires_grad=False))
        else:
            self.register_buffer('lambda_param', torch.tensor(initial_lambda))
        
        # ç¼–ç å™¨é€‰æ‹©
        if encoder_type == 'gcn':
            self.encoder = GCNEncoder(input_dim, hidden_dims, hidden_dims[-1], dropout)
        elif encoder_type == 'gat':
            heads = [4] * len(hidden_dims) + [1]
            self.encoder = GATEncoder(input_dim, hidden_dims, hidden_dims[-1], heads, dropout)
        elif encoder_type == 'sage':
            self.encoder = GraphSAGEEncoder(input_dim, hidden_dims, hidden_dims[-1], dropout)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç¼–ç å™¨ç±»å‹: {encoder_type}")
        
        # å˜åˆ†å±‚ï¼šä»ç¼–ç å™¨è¾“å‡ºæ˜ å°„åˆ°å‡å€¼å’Œå¯¹æ•°æ–¹å·®
        self.fc_mu = nn.Linear(hidden_dims[-1], embedding_dim)  # å‡å€¼å±‚
        self.fc_logvar = nn.Linear(hidden_dims[-1], embedding_dim)  # å¯¹æ•°æ–¹å·®å±‚
        
        # æ‰¹æ ‡å‡†åŒ–ï¼ˆå¯é€‰ï¼‰
        if use_batch_norm:
            self.bn_mu = nn.BatchNorm1d(embedding_dim)
            self.bn_logvar = nn.BatchNorm1d(embedding_dim)
        
        # è§£ç å™¨ï¼šä»æ½œåœ¨ç©ºé—´é‡æ„åŸå§‹ç‰¹å¾
        decoder_layers = []
        decoder_dims = [embedding_dim] + hidden_dims[::-1] + [input_dim]
        
        for i in range(len(decoder_dims) - 1):
            decoder_layers.append(nn.Linear(decoder_dims[i], decoder_dims[i + 1]))
            if i < len(decoder_dims) - 2:  # ä¸åœ¨æœ€åä¸€å±‚æ·»åŠ æ¿€æ´»å‡½æ•°
                decoder_layers.append(nn.ReLU())
                decoder_layers.append(nn.Dropout(dropout))
                if use_batch_norm:
                    decoder_layers.append(nn.BatchNorm1d(decoder_dims[i + 1]))
        
        self.decoder = nn.Sequential(*decoder_layers)
        
        # èšç±»å±‚
        self.cluster_centers = nn.Parameter(torch.randn(num_clusters, embedding_dim))
        
        # æŸå¤±å‡½æ•°
        self.contrastive_loss = ContrastiveLoss(temperature)
        self.reconstruction_loss = nn.MSELoss()
        self.fairness_loss = FairnessLoss(fairness_beta)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)
        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.5)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def encode(self, x: torch.Tensor, edge_index: torch.Tensor, 
               edge_weight: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ç¼–ç å™¨ï¼šå°†è¾“å…¥ç‰¹å¾æ˜ å°„åˆ°æ½œåœ¨ç©ºé—´çš„å‡å€¼å’Œæ–¹å·®
        
        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [N, input_dim]
            edge_index: è¾¹ç´¢å¼• [2, num_edges]
            edge_weight: è¾¹æƒé‡ [num_edges] (å¯é€‰)
            
        Returns:
            mu: æ½œåœ¨è¡¨ç¤ºçš„å‡å€¼ [N, embedding_dim]
            logvar: æ½œåœ¨è¡¨ç¤ºçš„å¯¹æ•°æ–¹å·® [N, embedding_dim]
        """
        # é€šè¿‡å›¾ç¥ç»ç½‘ç»œç¼–ç å™¨æå–ç‰¹å¾
        h = self.encoder(x, edge_index, edge_weight)
        
        # æ˜ å°„åˆ°å‡å€¼å’Œå¯¹æ•°æ–¹å·®
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        
        # æ‰¹æ ‡å‡†åŒ–ï¼ˆå¯é€‰ï¼‰
        if self.use_batch_norm:
            mu = self.bn_mu(mu)
            logvar = self.bn_logvar(logvar)
        
        return mu, logvar
    
    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:
        """
        é‡å‚æ•°åŒ–æŠ€å·§ï¼šä»æ ‡å‡†æ­£æ€åˆ†å¸ƒé‡‡æ ·æ½œåœ¨è¡¨ç¤º
        
        Args:
            mu: å‡å€¼ [N, embedding_dim]
            logvar: å¯¹æ•°æ–¹å·® [N, embedding_dim]
            
        Returns:
            z: é‡‡æ ·çš„æ½œåœ¨è¡¨ç¤º [N, embedding_dim]
        """
        if self.training:
            # è®­ç»ƒæ—¶ä½¿ç”¨é‡å‚æ•°åŒ–æŠ€å·§
            std = torch.exp(0.5 * logvar)
            eps = torch.randn_like(std)
            z = mu + eps * std
        else:
            # æ¨ç†æ—¶ç›´æ¥ä½¿ç”¨å‡å€¼
            z = mu
        
        return z
    
    def decode(self, z: torch.Tensor) -> torch.Tensor:
        """
        è§£ç å™¨ï¼šä»æ½œåœ¨ç©ºé—´é‡æ„åŸå§‹ç‰¹å¾
        
        Args:
            z: æ½œåœ¨è¡¨ç¤º [N, embedding_dim]
            
        Returns:
            x_recon: é‡æ„çš„ç‰¹å¾ [N, input_dim]
        """
        return self.decoder(z)
    
    def forward(self, data: Data) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            data: PyTorch Geometricæ•°æ®å¯¹è±¡
            
        Returns:
            z: æ½œåœ¨è¡¨ç¤º [N, embedding_dim]
            mu: å‡å€¼ [N, embedding_dim] 
            logvar: å¯¹æ•°æ–¹å·® [N, embedding_dim]
            x_recon: é‡æ„ç‰¹å¾ [N, input_dim]
            cluster_probs: èšç±»æ¦‚ç‡ [N, num_clusters]
        """
        # ç¼–ç 
        mu, logvar = self.encode(data.x, data.edge_index, 
                                getattr(data, 'edge_weight', None))
        
        # é‡å‚æ•°åŒ–é‡‡æ ·
        z = self.reparameterize(mu, logvar)
        
        # è§£ç 
        x_recon = self.decode(z)
        
        # è®¡ç®—èšç±»æ¦‚ç‡
        cluster_probs = self._compute_cluster_probs(z)
        
        return z, mu, logvar, x_recon, cluster_probs
    
    def _compute_cluster_probs(self, z: torch.Tensor, alpha: float = 1.0) -> torch.Tensor:
        """
        è®¡ç®—èšç±»åˆ†é…æ¦‚ç‡ï¼Œä½¿ç”¨t-åˆ†å¸ƒ
        
        Args:
            z: æ½œåœ¨è¡¨ç¤º [N, embedding_dim]
            alpha: t-åˆ†å¸ƒè‡ªç”±åº¦å‚æ•°
            
        Returns:
            cluster_probs: èšç±»æ¦‚ç‡ [N, num_clusters]
        """
        # è®¡ç®—èšç±»ä¸­å¿ƒçš„è·ç¦»
        distances = torch.cdist(z, self.cluster_centers)
        
        # ä½¿ç”¨t-åˆ†å¸ƒè®¡ç®—æ¦‚ç‡
        q = (1.0 + distances**2 / alpha) ** (-(alpha + 1.0) / 2.0)
        q = q / q.sum(dim=1, keepdim=True)
        
        return q
    
    def compute_kl_loss(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—KLæ•£åº¦æŸå¤±ï¼Œçº¦æŸæ½œåœ¨åˆ†å¸ƒæ¥è¿‘æ ‡å‡†æ­£æ€åˆ†å¸ƒ
        
        Args:
            mu: å‡å€¼ [N, embedding_dim]
            logvar: å¯¹æ•°æ–¹å·® [N, embedding_dim]
            
        Returns:
            kl_loss: KLæ•£åº¦æŸå¤±ï¼ˆæ ‡é‡ï¼‰
        """
        # KLæ•£åº¦ï¼šKL(q(z|x) || p(z))ï¼Œå…¶ä¸­p(z) = N(0, I)
        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)
        return kl_loss.mean()
    
    def compute_target_distribution(self, cluster_probs: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—ç›®æ ‡åˆ†å¸ƒç”¨äºè‡ªè®­ç»ƒ
        
        Args:
            cluster_probs: å½“å‰èšç±»æ¦‚ç‡ [N, num_clusters]
            
        Returns:
            target_probs: ç›®æ ‡åˆ†å¸ƒ [N, num_clusters]
        """
        # å¹³æ–¹æ¦‚ç‡å¹¶å½’ä¸€åŒ–ï¼ˆå¢å¼ºç½®ä¿¡åº¦é«˜çš„åˆ†é…ï¼‰
        p = cluster_probs**2 / cluster_probs.sum(dim=0, keepdim=True)
        p = p / p.sum(dim=1, keepdim=True)
        
        return p
    
    def compute_total_loss(self, data: Data, z: torch.Tensor, mu: torch.Tensor, 
                          logvar: torch.Tensor, x_recon: torch.Tensor, 
                          cluster_probs: torch.Tensor,
                          positive_pairs: Optional[torch.Tensor] = None,
                          target_probs: Optional[torch.Tensor] = None,
                          alpha_recon: float = 1.0, alpha_kl: float = 1.0,
                          alpha_cluster: float = 1.0, alpha_contrastive: float = 0.5) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—æ€»æŸå¤±ï¼ŒåŒ…æ‹¬é‡æ„æŸå¤±ã€KLæŸå¤±ã€èšç±»æŸå¤±å’Œå¯¹æ¯”æŸå¤±
        
        Args:
            data: PyTorch Geometricæ•°æ®å¯¹è±¡
            z: æ½œåœ¨è¡¨ç¤º [N, embedding_dim]
            mu: å‡å€¼ [N, embedding_dim]
            logvar: å¯¹æ•°æ–¹å·® [N, embedding_dim] 
            x_recon: é‡æ„ç‰¹å¾ [N, input_dim]
            cluster_probs: èšç±»æ¦‚ç‡ [N, num_clusters]
            positive_pairs: æ­£æ ·æœ¬å¯¹ [num_pairs, 2] (å¯é€‰)
            target_probs: ç›®æ ‡èšç±»åˆ†å¸ƒ [N, num_clusters] (å¯é€‰)
            alpha_recon: é‡æ„æŸå¤±æƒé‡
            alpha_kl: KLæŸå¤±æƒé‡
            alpha_cluster: èšç±»æŸå¤±æƒé‡
            alpha_contrastive: å¯¹æ¯”æŸå¤±æƒé‡
            
        Returns:
            æŸå¤±å­—å…¸
        """
        losses = {}
        
        # 1. é‡æ„æŸå¤±
        recon_loss = self.reconstruction_loss(x_recon, data.x)
        losses['reconstruction'] = recon_loss
        
        # 2. KLæ•£åº¦æŸå¤±
        kl_loss = self.compute_kl_loss(mu, logvar)
        losses['kl_divergence'] = kl_loss
        
        # 3. èšç±»æŸå¤±ï¼ˆå¦‚æœæä¾›ç›®æ ‡åˆ†å¸ƒï¼‰
        if target_probs is not None:
            cluster_loss = F.kl_div(torch.log(cluster_probs + 1e-8), target_probs, reduction='batchmean')
            losses['clustering'] = cluster_loss
        else:
            # ä½¿ç”¨ç†µæŸå¤±é¼“åŠ±ç½®ä¿¡çš„åˆ†é…
            entropy_loss = -torch.sum(cluster_probs * torch.log(cluster_probs + 1e-8), dim=1).mean()
            losses['clustering'] = -entropy_loss  # è´Ÿç†µï¼Œé¼“åŠ±ä½ç†µï¼ˆé«˜ç½®ä¿¡åº¦ï¼‰
        
        # 4. å¯¹æ¯”æŸå¤±ï¼ˆå¦‚æœæä¾›æ­£æ ·æœ¬å¯¹ï¼‰
        if positive_pairs is not None and len(positive_pairs) > 0:
            contrastive_loss = self.contrastive_loss(z, positive_pairs)
            losses['contrastive'] = contrastive_loss
        
        # 5. å…¬å¹³æ€§æŸå¤±ï¼ˆå¯é€‰ï¼‰
        # éœ€è¦ (od_matrix [N,N], cluster_assignments [N] ç¡¬æ ‡ç­¾, charge_num [N])
        if hasattr(data, 'charge_num') and data.charge_num is not None:
            try:
                # æ„å»ºæˆ–å¤ç”¨ OD çŸ©é˜µ
                if hasattr(data, 'od_matrix') and data.od_matrix is not None:
                    od_matrix = data.od_matrix
                else:
                    N = data.num_nodes if hasattr(data, 'num_nodes') else data.x.size(0)
                    od_matrix = torch.zeros((N, N), device=data.x.device, dtype=data.x.dtype)
                    if hasattr(data, 'edge_index') and data.edge_index is not None:
                        ei = data.edge_index
                        # è¯»å–æƒé‡ï¼Œä¼˜å…ˆ edge_attrï¼ˆå¯èƒ½ä¸º [E,1] æˆ– [E]ï¼‰
                        if hasattr(data, 'edge_attr') and data.edge_attr is not None:
                            ew = data.edge_attr
                            if ew.dim() > 1:
                                ew = ew.squeeze(-1)
                        else:
                            # è‹¥æ— æƒé‡ï¼Œåˆ™é»˜è®¤ 1.0
                            ew = torch.ones(ei.size(1), device=od_matrix.device, dtype=od_matrix.dtype)
                        od_matrix[ei[0], ei[1]] = ew
                    # ç¼“å­˜ï¼Œé¿å…æ¯æ­¥é‡å»º
                    data.od_matrix = od_matrix

                # å°†è½¯æ¦‚ç‡è½¬ä¸ºç¡¬æ ‡ç­¾
                cluster_assignments = torch.argmax(cluster_probs, dim=1).long()

                fairness_loss = self.fairness_loss(od_matrix, cluster_assignments, data.charge_num)
                losses['fairness'] = fairness_loss
            except Exception as _fair_e:
                # è‹¥å…¬å¹³æ€§æŸå¤±è®¡ç®—å¤±è´¥ï¼Œè·³è¿‡ä»¥ä¸å½±å“ä¸»è®­ç»ƒ
                pass
        
        # è®¡ç®—æ€»æŸå¤±
        total_loss = (alpha_recon * losses['reconstruction'] + 
                     alpha_kl * self.kl_weight * losses['kl_divergence'] +
                     alpha_cluster * losses['clustering'])
        
        if 'contrastive' in losses:
            total_loss += alpha_contrastive * losses['contrastive']
        
        if 'fairness' in losses:
            total_loss += 0.1 * losses['fairness']  # å›ºå®šæƒé‡
        
        losses['total'] = total_loss
        
        return losses
    
    def pretrain_with_em(self, data: Data, max_iter: int = 100, tol: float = 1e-4,
                        init_method: str = 'kmeans', verbose: bool = True,
                        use_fairness_prior: bool = False, fairness_weight: float = 0.1,
                        charge_num: Optional[torch.Tensor] = None,
                        target_balance_ratio: float = 0.8) -> Dict[str, Any]:
        """
        ä½¿ç”¨EMç®—æ³•é¢„è®­ç»ƒèšç±»ä¸­å¿ƒï¼ˆåŸºäºå˜åˆ†ç¼–ç å™¨çš„å‡å€¼è¡¨ç¤ºï¼‰
        
        Args:
            data: PyTorch Geometricæ•°æ®å¯¹è±¡
            max_iter: EMç®—æ³•æœ€å¤§è¿­ä»£æ¬¡æ•°
            tol: æ”¶æ•›å®¹å¿åº¦
            init_method: åˆå§‹åŒ–æ–¹æ³•
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
            use_fairness_prior: æ˜¯å¦ä½¿ç”¨å…¬å¹³æ€§å…ˆéªŒ
            fairness_weight: å…¬å¹³æ€§æƒé‡
            charge_num: å……ç”µæ¡©æ•°é‡
            target_balance_ratio: ç›®æ ‡å‡è¡¡æ¯”ä¾‹
            
        Returns:
            EMè®­ç»ƒç»“æœå­—å…¸        """
        print("å¼€å§‹å˜åˆ†æ¨¡å‹çš„EMé¢„è®­ç»ƒ...")
        
        # è·å–æ½œåœ¨è¡¨ç¤ºï¼ˆä½¿ç”¨å‡å€¼ï¼Œä¸ä½¿ç”¨é‡‡æ ·ï¼‰
        self.eval()
        with torch.no_grad():
            mu, logvar = self.encode(data.x, data.edge_index, 
                                   getattr(data, 'edge_weight', None))
            embeddings = mu  # ä½¿ç”¨å‡å€¼ä½œä¸ºç¡®å®šæ€§è¡¨ç¤º
        
        # ä½¿ç”¨K-meansè¿›è¡Œç®€åŒ–çš„èšç±»ä¸­å¿ƒåˆå§‹åŒ–
        embeddings_np = embeddings.detach().cpu().numpy()
        N, D = embeddings_np.shape
        K = self.num_clusters
        
        # å¤„ç†æœåŠ¡èƒ½åŠ›æ•°æ®ï¼ˆç§»åŠ¨åˆ°è·å–Nå’ŒKä¹‹åï¼‰
        if use_fairness_prior and charge_num is not None:
            charge_capacities = charge_num.detach().cpu().numpy()
            total_capacity = np.sum(charge_capacities)
            # è®¡ç®—ç†æƒ³çš„æ¯ä¸ªèšç±»çš„æœåŠ¡èƒ½åŠ› (å‡åŒ€åˆ†å¸ƒ)
            target_capacity_per_cluster = total_capacity / K
        else:
            charge_capacities = np.ones(N)  # å¦‚æœæ²¡æœ‰æœåŠ¡èƒ½åŠ›æ•°æ®ï¼Œå‡è®¾æ‰€æœ‰èŠ‚ç‚¹èƒ½åŠ›ç›¸ç­‰
            total_capacity = N
            target_capacity_per_cluster = N / K
        
        # K-meansåˆå§‹åŒ–
        if init_method == 'kmeans':
            from sklearn.cluster import KMeans
            kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)
            kmeans.fit(embeddings_np)
            centers = kmeans.cluster_centers_
            
        elif init_method == 'random':
            indices = np.random.choice(N, K, replace=False)
            centers = embeddings_np[indices].copy()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆå§‹åŒ–æ–¹æ³•: {init_method}")
        
        # ç®€åŒ–çš„EMç®—æ³•
        log_likelihood_history = []
        
        for iter_num in range(max_iter):
            # Eæ­¥ï¼šè®¡ç®—è·ç¦»å’Œè´£ä»»çŸ©é˜µ
            distances = np.linalg.norm(embeddings_np[:, np.newaxis] - centers, axis=2)
            responsibilities = np.exp(-distances)
            responsibilities = responsibilities / responsibilities.sum(axis=1, keepdims=True)
            
            # Mæ­¥ï¼šæ›´æ–°èšç±»ä¸­å¿ƒ
            old_centers = centers.copy()
            for k in range(K):
                if responsibilities[:, k].sum() > 1e-6:
                    centers[k] = np.average(embeddings_np, axis=0, weights=responsibilities[:, k])
            
            # è®¡ç®—ä¼¼ç„¶
            log_likelihood = -np.sum(np.min(distances, axis=1))
            log_likelihood_history.append(log_likelihood)
            
            # æ£€æŸ¥æ”¶æ•›
            center_change = np.linalg.norm(centers - old_centers)
            if center_change < tol:
                if verbose:
                    print(f"EMç®—æ³•åœ¨ç¬¬ {iter_num + 1} è½®æ”¶æ•›")
                break
        
        # æ›´æ–°æ¨¡å‹èšç±»ä¸­å¿ƒ
        device = next(self.parameters()).device
        self.cluster_centers.data = torch.tensor(centers, dtype=torch.float32, device=device)
        
        if verbose:
            print(f"EMé¢„è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆä¼¼ç„¶: {log_likelihood_history[-1]:.4f}")
        
        return {
            'converged': iter_num < max_iter - 1,
            'n_iter': iter_num + 1,
            'log_likelihood': log_likelihood_history,
            'final_centers': torch.tensor(centers, dtype=torch.float32),
            'responsibilities': torch.tensor(responsibilities, dtype=torch.float32)
        }
    
    def complete_pretraining_workflow(self, data: Data,
                                    pretrain_epochs: int = 50,
                                    em_max_iter: int = 100,
                                    em_init_method: str = 'kmeans',
                                    learning_rate: float = 0.001,
                                    verbose: bool = True) -> Dict[str, Any]:
        """
        å®Œæ•´çš„é¢„è®­ç»ƒå·¥ä½œæµç¨‹
        
        Args:
            data: PyTorch Geometricæ•°æ®å¯¹è±¡
            pretrain_epochs: å˜åˆ†è‡ªç¼–ç å™¨é¢„è®­ç»ƒè½®æ•°
            em_max_iter: EMç®—æ³•æœ€å¤§è¿­ä»£æ¬¡æ•°
            em_init_method: EMåˆå§‹åŒ–æ–¹æ³•
            learning_rate: å­¦ä¹ ç‡
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
            
        Returns:
            é¢„è®­ç»ƒç»“æœå­—å…¸
        """
        print("=" * 60)
        print("å¼€å§‹å˜åˆ†æ·±åº¦å›¾èšç±»é¢„è®­ç»ƒå·¥ä½œæµ")
        print("=" * 60)
        
        device = next(self.parameters()).device
        data = data.to(device)
        
        # é˜¶æ®µ1ï¼šå˜åˆ†è‡ªç¼–ç å™¨é¢„è®­ç»ƒ
        print(f"\né˜¶æ®µ1: å˜åˆ†è‡ªç¼–ç å™¨é¢„è®­ç»ƒ ({pretrain_epochs} è½®)")
        print("-" * 40)
        
        self.train()
        reconstruction_losses = []
        kl_losses = []
        
        for epoch in range(pretrain_epochs):
            self.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            z, mu, logvar, x_recon, _ = self.forward(data)
            
            # è®¡ç®—æŸå¤±
            recon_loss = self.reconstruction_loss(x_recon, data.x)
            kl_loss = self.compute_kl_loss(mu, logvar)
            total_loss = recon_loss + self.kl_weight * kl_loss
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            self.optimizer.step()
            
            reconstruction_losses.append(recon_loss.item())
            kl_losses.append(kl_loss.item())
            
            if verbose and (epoch + 1) % 10 == 0:
                print(f"è½®æ¬¡ {epoch + 1}/{pretrain_epochs}, "
                      f"é‡æ„æŸå¤±: {recon_loss.item():.6f}, "
                      f"KLæŸå¤±: {kl_loss.item():.6f}")
        
        print(f"å˜åˆ†è‡ªç¼–ç å™¨é¢„è®­ç»ƒå®Œæˆ")
        
        # é˜¶æ®µ2ï¼šEMç®—æ³•åˆå§‹åŒ–èšç±»ä¸­å¿ƒ
        print(f"\né˜¶æ®µ2: EMç®—æ³•èšç±»ä¸­å¿ƒåˆå§‹åŒ–")
        print("-" * 40)
        
        em_results = self.pretrain_with_em(
            data=data,
            max_iter=em_max_iter,
            tol=0.001,
            init_method=em_init_method,
            verbose=verbose
        )
        
        # æ±‡æ€»ç»“æœ
        results = {
            'reconstruction_losses': reconstruction_losses,
            'kl_losses': kl_losses,
            'em_results': em_results,
            'final_cluster_centers': self.cluster_centers.detach().cpu()
        }
        
        print("\n" + "=" * 60)
        print("å˜åˆ†æ·±åº¦å›¾èšç±»é¢„è®­ç»ƒå·¥ä½œæµå®Œæˆï¼")
        print("=" * 60)
        
        return results
    
    def evaluate_clustering_quality(self, data: Data, true_labels: Optional[np.ndarray] = None) -> Dict[str, float]:
        """
        è¯„ä¼°èšç±»è´¨é‡
        
        Args:
            data: PyTorch Geometricæ•°æ®å¯¹è±¡
            true_labels: çœŸå®æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
        
        self.eval()
        with torch.no_grad():
            mu, logvar = self.encode(data.x, data.edge_index, 
                                   getattr(data, 'edge_weight', None))
            z = mu  # ä½¿ç”¨å‡å€¼è¿›è¡Œè¯„ä¼°
            cluster_probs = self._compute_cluster_probs(z)
        
        z_np = z.cpu().numpy()
        predicted_labels = cluster_probs.argmax(dim=1).cpu().numpy()
        
        metrics = {}
        
        # å†…éƒ¨è¯„ä¼°æŒ‡æ ‡
        if len(np.unique(predicted_labels)) > 1:
            metrics['silhouette_score'] = silhouette_score(z_np, predicted_labels)
            metrics['calinski_harabasz_score'] = calinski_harabasz_score(z_np, predicted_labels)
            metrics['davies_bouldin_score'] = davies_bouldin_score(z_np, predicted_labels)
        
        # å¤–éƒ¨è¯„ä¼°æŒ‡æ ‡
        if true_labels is not None:
            from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score
            metrics['adjusted_rand_score'] = adjusted_rand_score(true_labels, predicted_labels)
            metrics['normalized_mutual_info'] = normalized_mutual_info_score(true_labels, predicted_labels)
        
        return metrics
    
    def generate_zoning_policies(self, n: int, similarity_threshold: float = 0.95) -> torch.Tensor:
        """
        ç”Ÿæˆ n ä¸ªä¸åŒçš„ zoning policies (èšç±»æ¦‚ç‡åˆ†å¸ƒ)
        
        Args:
            n: è¦ç”Ÿæˆçš„ policy æ•°é‡
            similarity_threshold: å»é‡æ—¶çš„ä½™å¼¦ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼è®¤ä¸ºæ˜¯é‡å¤
            
        Returns:
            torch.Tensor: shape [n, num_clusters] çš„ policy çŸ©é˜µ
        """
        self.eval()
        
        policies = []
        max_attempts = n * 10  # æœ€å¤§å°è¯•æ¬¡æ•°ï¼Œé¿å…æ— é™å¾ªç¯
        attempts = 0
        
        with torch.no_grad():
            while len(policies) < n and attempts < max_attempts:
                attempts += 1
                
                # ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒé‡‡æ · latent å‘é‡
                z_sample = torch.randn(1, self.embedding_dim, device=next(self.parameters()).device)
                
                # é€šè¿‡èšç±»æ¦‚ç‡è®¡ç®—å¾—åˆ° policy
                cluster_probs = self._compute_cluster_probs(z_sample)
                policy = cluster_probs.squeeze(0)  # ç§»é™¤ batch ç»´åº¦ï¼Œå¾—åˆ° [num_clusters] çš„æ¦‚ç‡åˆ†å¸ƒ
                
                # æ£€æŸ¥ä¸å·²æœ‰ policies çš„ç›¸ä¼¼åº¦
                is_unique = True
                for existing_policy in policies:
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = F.cosine_similarity(policy.unsqueeze(0), existing_policy.unsqueeze(0), dim=-1)
                    if similarity.item() > similarity_threshold:
                        is_unique = False
                        break
                
                if is_unique:
                    policies.append(policy)
            
            if len(policies) < n:
                print(f"Warning: Only generated {len(policies)} unique policies out of {n} requested after {max_attempts} attempts")
        
        # å°† policies å †å æˆçŸ©é˜µ
        if policies:
            return torch.stack(policies)
        else:
            # å¦‚æœæ²¡æœ‰ç”Ÿæˆä»»ä½• policyï¼Œè¿”å›éšæœºåˆå§‹åŒ–çš„
            print("Warning: No unique policies generated, returning random policies")
            random_policies = torch.randn(n, self.num_clusters, device=next(self.parameters()).device)
            return F.softmax(random_policies, dim=-1)

    def generate_node_policies(self, data: Data, n: int = 1, use_mean: bool = True,
                               noise_scale: float = 1.0) -> torch.Tensor:
        """
        ç”ŸæˆèŠ‚ç‚¹çº§çš„èšç±» policyï¼ˆæ¯ä¸ªèŠ‚ç‚¹ä¸€è¡Œã€æ¯ä¸ªç°‡ä¸€åˆ—ï¼‰ã€‚

        Args:
            data: å›¾æ•°æ®ï¼ˆéœ€åŒ…å« x, edge_index, å¯é€‰ edge_weightï¼‰
            n: ç”Ÿæˆçš„ policy ä»½æ•°ï¼›n=1 è¿”å› [N, K]ï¼›n>1 è¿”å› [n, N, K]
            use_mean: æ˜¯å¦ä½¿ç”¨å‡å€¼ mu ä½œä¸ºæ½œå˜é‡ï¼›è‹¥ä¸º False åˆ™æŒ‰é‡å‚æ•°åŒ–é‡‡æ ·
            noise_scale: é‡‡æ ·æ—¶å¯¹ std çš„ç¼©æ”¾ç³»æ•°

        Returns:
            Tensor: å½“ n==1 æ—¶ shape [N, num_clusters]ï¼›å¦åˆ™ [n, N, num_clusters]

        è¯´æ˜ï¼šèšç±»æ¦‚ç‡æ˜¯ç”±æ½œå˜é‡ z ä¸å¯å­¦ä¹ çš„ cluster_centers é€šè¿‡ t-åˆ†å¸ƒæ ¸è®¡ç®—å¾—åˆ°ï¼Œ
        ä¸è§£ç å™¨é‡æ„æ— ç›´æ¥å…³ç³»ï¼›decoder ä¸»è¦æœåŠ¡äºé‡æ„æŸå¤±ã€‚
        """
        self.eval()
        device = next(self.parameters()).device
        data = data.to(device)

        with torch.no_grad():
            mu, logvar = self.encode(data.x, data.edge_index, getattr(data, 'edge_weight', None))

            if use_mean and n == 1:
                z = mu
                probs = self._compute_cluster_probs(z)
                return probs  # [N, K]

            # å¦åˆ™è¿›è¡Œ n æ¬¡é‡‡æ ·
            std = torch.exp(0.5 * logvar) * noise_scale
            policies = []
            for _ in range(max(1, n)):
                if use_mean:
                    z = mu
                else:
                    eps = torch.randn_like(std)
                    z = mu + eps * std
                probs = self._compute_cluster_probs(z)  # [N, K]
                policies.append(probs.unsqueeze(0))  # [1, N, K]

            return torch.cat(policies, dim=0)  # [n, N, K]


class VariationalClusteringTrainer:
    """
    å˜åˆ†æ·±åº¦å›¾èšç±»è®­ç»ƒå™¨
    """
    
    def __init__(self, model: VariationalDeepGraphClustering, device: str = 'cpu'):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            model: å˜åˆ†æ·±åº¦å›¾èšç±»æ¨¡å‹
            device: è®¡ç®—è®¾å¤‡
        """
        self.model = model.to(device)
        self.device = device
        
    def train_epoch(self, data: Data, target_probs: Optional[torch.Tensor] = None,
                   alpha_recon: float = 1.0, alpha_kl: float = 1.0,
                   alpha_cluster: float = 1.0) -> Dict[str, float]:
        """
        è®­ç»ƒä¸€ä¸ªepoch
        
        Args:
            data: å›¾æ•°æ®
            target_probs: ç›®æ ‡èšç±»åˆ†å¸ƒ
            alpha_recon: é‡æ„æŸå¤±æƒé‡
            alpha_kl: KLæŸå¤±æƒé‡
            alpha_cluster: èšç±»æŸå¤±æƒé‡
            
        Returns:
            æŸå¤±å­—å…¸
        """
        self.model.train()
        data = data.to(self.device)
        
        # å‰å‘ä¼ æ’­
        z, mu, logvar, x_recon, cluster_probs = self.model(data)
        
        # è®¡ç®—æŸå¤±
        losses = self.model.compute_total_loss(
            data, z, mu, logvar, x_recon, cluster_probs,
            target_probs=target_probs,
            alpha_recon=alpha_recon,
            alpha_kl=alpha_kl,
            alpha_cluster=alpha_cluster
        )
        
        # åå‘ä¼ æ’­
        self.model.optimizer.zero_grad()
        losses['total'].backward()
        self.model.optimizer.step()
        
        # è¿”å›æŸå¤±å€¼
        return {k: v.item() if torch.is_tensor(v) else v for k, v in losses.items()}
    
    def train_full_workflow(self, data: Data, epochs: int = 100,
                           pretrain_epochs: int = 50, em_max_iter: int = 100,
                           update_target_freq: int = 10, verbose: bool = True,
                           use_fairness: bool = True, use_frank_wolfe: bool = True,
                           data_type: str = "vgnn", save_model: bool = True) -> Dict[str, Any]:
        """
        å®Œæ•´çš„è®­ç»ƒå·¥ä½œæµç¨‹
        
        Args:
            data: å›¾æ•°æ®
            epochs: ä¸»è®­ç»ƒè½®æ•°
            pretrain_epochs: é¢„è®­ç»ƒè½®æ•°
            em_max_iter: EMç®—æ³•æœ€å¤§è¿­ä»£æ¬¡æ•°
            update_target_freq: æ›´æ–°ç›®æ ‡åˆ†å¸ƒçš„é¢‘ç‡
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            use_fairness: æ˜¯å¦ä½¿ç”¨å…¬å¹³æ€§æŸå¤±
            use_frank_wolfe: æ˜¯å¦ä½¿ç”¨Frank-Wolfeç®—æ³•
            data_type: æ•°æ®ç±»å‹æ ‡è¯†
            save_model: æ˜¯å¦ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
            
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        print("å¼€å§‹å˜åˆ†æ·±åº¦å›¾èšç±»å®Œæ•´è®­ç»ƒæµç¨‹")
        print("=" * 60)
        
        # é˜¶æ®µ1ï¼šé¢„è®­ç»ƒ
        if verbose:
            print("é˜¶æ®µ1: é¢„è®­ç»ƒé˜¶æ®µ")
        
        pretrain_results = self.model.complete_pretraining_workflow(
            data=data,
            pretrain_epochs=pretrain_epochs,
            em_max_iter=em_max_iter,
            verbose=verbose
        )
        
        # é˜¶æ®µ2ï¼šä¸»è®­ç»ƒå¾ªç¯
        if verbose:
            print(f"\né˜¶æ®µ2: ä¸»è®­ç»ƒé˜¶æ®µ ({epochs} è½®)")
            print("-" * 40)
        
        train_losses = []
        target_probs = None
        
        for epoch in range(epochs):
            # å®šæœŸæ›´æ–°ç›®æ ‡åˆ†å¸ƒ
            if epoch % update_target_freq == 0:
                self.model.eval()
                with torch.no_grad():
                    _, _, _, _, cluster_probs = self.model(data.to(self.device))
                    target_probs = self.model.compute_target_distribution(cluster_probs)
            
            # è®­ç»ƒä¸€ä¸ªepoch
            losses = self.train_epoch(data, target_probs)
            train_losses.append(losses)
            
            if verbose and (epoch + 1) % 20 == 0:
                print(f"è½®æ¬¡ {epoch + 1}/{epochs}, "
                      f"æ€»æŸå¤±: {losses['total']:.6f}, "
                      f"é‡æ„: {losses['reconstruction']:.6f}, "
                      f"KL: {losses['kl_divergence']:.6f}, "
                      f"èšç±»: {losses['clustering']:.6f}")
        
        # æœ€ç»ˆè¯„ä¼°
        if verbose:
            print("\né˜¶æ®µ3: æœ€ç»ˆè¯„ä¼°")
            print("-" * 40)
        
        final_metrics = self.model.evaluate_clustering_quality(data.to(self.device))
        
        if verbose:
            print("æœ€ç»ˆèšç±»è´¨é‡:")
            for metric, value in final_metrics.items():
                print(f"  {metric}: {value:.4f}")
        
        results = {
            'pretrain_results': pretrain_results,
            'train_losses': train_losses,
            'final_metrics': final_metrics
        }
        
        # ä¿å­˜æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if save_model:
            self._save_vgnn_model_with_params(
                use_fairness=use_fairness,
                use_frank_wolfe=use_frank_wolfe,
                kl_weight=self.model.kl_weight,
                # fairness_beta=self.model.fairness_beta,
                # temperature=self.model.temperature,
                learning_rate=self.model.learning_rate,
                data_type=data_type,
                num_clusters=self.model.num_clusters,
                epochs=epochs,
                pretrain_epochs=pretrain_epochs,
                results=results
            )
        
        print("\n" + "=" * 60)
        print("å˜åˆ†æ·±åº¦å›¾èšç±»è®­ç»ƒå®Œæˆï¼")
        print("=" * 60)
        
        return results
    
    def _save_vgnn_model_with_params(self, use_fairness: bool = True, use_frank_wolfe: bool = True,
                                    kl_weight: float = 0.1, fairness_beta: float = 1.0,
                                    temperature: float = 0.1, learning_rate: float = 0.001,
                                    data_type: str = "vgnn", num_clusters: int = 5, 
                                    epochs: int = 100, pretrain_epochs: int = 50,
                                    results: Optional[Dict] = None) -> str:
        """
        ä¿å­˜è®­ç»ƒå¥½çš„å˜åˆ†GNNæ¨¡å‹åŠå…¶å‚æ•°ä¿¡æ¯åˆ°pthæ–‡ä»¶å¤¹
        
        Args:
            use_fairness: æ˜¯å¦ä½¿ç”¨å…¬å¹³æ€§æŸå¤±
            use_frank_wolfe: æ˜¯å¦ä½¿ç”¨Frank-Wolfeç®—æ³•
            kl_weight: KLæ•£åº¦æŸå¤±æƒé‡
            fairness_beta: å…¬å¹³æ€§æŸå¤±betaå‚æ•°
            temperature: å¯¹æ¯”å­¦ä¹ æ¸©åº¦å‚æ•°
            learning_rate: å­¦ä¹ ç‡
            data_type: æ•°æ®ç±»å‹æ ‡è¯†
            num_clusters: èšç±»æ•°é‡
            epochs: ä¸»è®­ç»ƒè½®æ•°
            pretrain_epochs: é¢„è®­ç»ƒè½®æ•°
            results: è®­ç»ƒç»“æœå­—å…¸
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        import datetime
        import json
        
        # åˆ›å»ºpthæ–‡ä»¶å¤¹
        pth_folder = Path("pth")
        pth_folder.mkdir(exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«å…³é”®å‚æ•°ä¿¡æ¯ï¼‰
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        fairness_tag = "fair" if use_fairness else "nofair"
        fw_tag = "fw" if use_frank_wolfe else "nofw"
        filename = f"vgnn_clustering_{data_type}_{fairness_tag}_{fw_tag}_c{num_clusters}_e{epochs}_{timestamp}"
        
        # å‡†å¤‡ä¿å­˜çš„æ•°æ®
        save_data = {
            # æ¨¡å‹çŠ¶æ€
            'model_state_dict': self.model.state_dict(),
            'model_config': {
                'input_dim': self.model.input_dim,
                'embedding_dim': self.model.embedding_dim,
                'num_clusters': self.model.num_clusters,
                'encoder_type': self.model.encoder_type,
                'hidden_dims': self.model.hidden_dims,
                'dropout': self.model.dropout,
                # 'temperature': self.model.temperature,
                # 'fairness_beta': self.model.fairness_beta,
                'use_frank_wolfe': self.model.use_frank_wolfe,
                'kl_weight': self.model.kl_weight,
                'use_batch_norm': self.model.use_batch_norm,
                'model_type': self.model.__class__.__name__
            },
            
            # è®­ç»ƒå‚æ•°
            'training_params': {
                'use_fairness': use_fairness,
                'use_frank_wolfe': use_frank_wolfe,
                'kl_weight': kl_weight,
                'fairness_beta': fairness_beta,
                'temperature': temperature,
                'learning_rate': learning_rate,
                'epochs': epochs,
                'pretrain_epochs': pretrain_epochs,
                'data_type': data_type,
                'device': str(self.device)
            },
            
            # è®­ç»ƒç»“æœï¼ˆå¦‚æœæä¾›ï¼‰
            'training_results': results if results else {},
            
            # å…ƒä¿¡æ¯
            'metadata': {
                'timestamp': timestamp,
                'torch_version': torch.__version__,
                'saved_datetime': datetime.datetime.now().isoformat(),
                'fairness_enabled': use_fairness,
                'frank_wolfe_enabled': use_frank_wolfe,
                'model_architecture': 'VariationalGNN'
            }
        }
        
        # ä¿å­˜æ¨¡å‹æ–‡ä»¶
        model_path = pth_folder / f"{filename}.pth"
        torch.save(save_data, model_path, _use_new_zipfile_serialization=False)
        
        # ä¿å­˜å‚æ•°ä¿¡æ¯åˆ°JSONæ–‡ä»¶ï¼ˆä¾¿äºæŸ¥çœ‹ï¼‰
        params_info = {
            'model_file': f"{filename}.pth",
            'model_architecture': 'Variational Graph Neural Network',
            'training_config': {
                'fairness_loss': use_fairness,
                'frank_wolfe_algorithm': use_frank_wolfe,
                'variational_parameters': {
                    'kl_weight': kl_weight,
                    'temperature': temperature,
                    'fairness_beta': fairness_beta
                },
                'training_settings': {
                    'main_epochs': epochs,
                    'pretrain_epochs': pretrain_epochs,
                    'learning_rate': learning_rate,
                    'num_clusters': num_clusters,
                    'data_type': data_type,
                    'device': str(self.device)
                }
            },
            'model_structure': {
                'encoder_type': self.model.encoder_type,
                'input_dim': self.model.input_dim,
                'embedding_dim': self.model.embedding_dim,
                'hidden_dims': self.model.hidden_dims,
                'dropout': self.model.dropout,
                'use_batch_norm': self.model.use_batch_norm
            },
            'final_results': {
                'training_completed': True,
                'final_metrics': results.get('final_metrics', {}) if results else {},
                'pretrain_results': results.get('pretrain_results', {}) if results else {}
            },
            'metadata': save_data['metadata']
        }
        
        # æ¸…ç†numpyæ•°ç»„ï¼Œåªä¿ç•™åŸºç¡€æ•°æ®ç±»å‹
        if 'final_metrics' in params_info['final_results']:
            metrics = params_info['final_results']['final_metrics']
            if metrics and any(hasattr(v, 'tolist') for v in metrics.values() if v is not None):
                params_info['final_results']['metrics_summary'] = {
                    k: float(v) if hasattr(v, 'item') else v 
                    for k, v in metrics.items() if v is not None
                }
        
        # ä¿å­˜JSONå‚æ•°æ–‡ä»¶
        json_path = pth_folder / f"{filename}_params.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(params_info, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ å˜åˆ†GNNæ¨¡å‹å·²ä¿å­˜:")
        print(f"   æ¨¡å‹æ–‡ä»¶: {model_path}")
        print(f"   å‚æ•°æ–‡ä»¶: {json_path}")
        print(f"   è®­ç»ƒé…ç½®:")
        print(f"     - æ¨¡å‹æ¶æ„: å˜åˆ†å›¾ç¥ç»ç½‘ç»œ ({self.model.encoder_type.upper()})")
        print(f"     - å…¬å¹³æ€§æŸå¤±: {'âœ…' if use_fairness else 'âŒ'}")
        print(f"     - Frank-Wolfe: {'âœ…' if use_frank_wolfe else 'âŒ'}")
        print(f"     - èšç±»æ•°é‡: {num_clusters}")
        print(f"     - ä¸»è®­ç»ƒè½®æ•°: {epochs}")
        print(f"     - é¢„è®­ç»ƒè½®æ•°: {pretrain_epochs}")
        print(f"     - æ•°æ®ç±»å‹: {data_type}")
        print(f"     - KLæƒé‡: {kl_weight}")
        print(f"     - æ¸©åº¦å‚æ•°: {temperature}")
        print(f"     - å…¬å¹³æ€§Beta: {fairness_beta}")
        
        return str(model_path)

    @staticmethod
    def load_vgnn_model_with_params(model_path: str, device: str = 'cpu') -> Tuple['VariationalDeepGraphClustering', Dict]:
        """
        åŠ è½½ä¿å­˜çš„å˜åˆ†GNNæ¨¡å‹åŠå…¶å‚æ•°ä¿¡æ¯
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            device: åŠ è½½åˆ°çš„è®¾å¤‡
            
        Returns:
            (model, params_dict): åŠ è½½çš„æ¨¡å‹å’Œå‚æ•°å­—å…¸
        """
        # åŠ è½½ä¿å­˜çš„æ•°æ®
        saved_data = torch.load(model_path, map_location=device, weights_only=False)
        
        # é‡å»ºæ¨¡å‹
        model_config = saved_data['model_config']
        if saved_data['model_config']['model_type'] == 'VariationalDeepGraphClustering':
            model = VariationalDeepGraphClustering(
                input_dim=model_config['input_dim'],
                embedding_dim=model_config['embedding_dim'],
                num_clusters=model_config['num_clusters'],
                encoder_type=model_config.get('encoder_type', 'gcn'),
                hidden_dims=model_config.get('hidden_dims', [64, 32]),
                dropout=model_config.get('dropout', 0.1),
                temperature=model_config.get('temperature', 0.1),
                fairness_beta=model_config.get('fairness_beta', 1.0),
                use_frank_wolfe=model_config.get('use_frank_wolfe', False),
                kl_weight=model_config.get('kl_weight', 0.1),
                use_batch_norm=model_config.get('use_batch_norm', True)
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_config['model_type']}")
        
        # åŠ è½½æ¨¡å‹çŠ¶æ€
        model.load_state_dict(saved_data['model_state_dict'])
        model.to(device)
        model.eval()
        
        print(f"âœ… å˜åˆ†GNNæ¨¡å‹å·²åŠ è½½: {model_path}")
        print(f"   æ¨¡å‹é…ç½®:")
        print(f"     - æ¶æ„: {model_config['encoder_type'].upper()} å˜åˆ†ç¼–ç å™¨")
        print(f"     - åµŒå…¥ç»´åº¦: {model_config['embedding_dim']}")
        print(f"     - èšç±»æ•°é‡: {model_config['num_clusters']}")
        
        training_params = saved_data['training_params']
        print(f"   è®­ç»ƒå‚æ•°:")
        print(f"     - å…¬å¹³æ€§æŸå¤±: {'âœ…' if training_params['use_fairness'] else 'âŒ'}")
        print(f"     - Frank-Wolfe: {'âœ…' if training_params['use_frank_wolfe'] else 'âŒ'}")
        print(f"     - KLæƒé‡: {training_params.get('kl_weight', 'N/A')}")
        print(f"     - æ¸©åº¦å‚æ•°: {training_params.get('temperature', 'N/A')}")
        print(f"     - æ•°æ®ç±»å‹: {training_params.get('data_type', 'N/A')}")
        
        return model, saved_data
    
    def train_enhanced_vgnn(self, data: Data, od_data: pd.DataFrame, adj_matrix: torch.Tensor,
                           charge_num: Optional[torch.Tensor] = None, num_clusters: int = 5,
                           epochs: int = 200, pretrain_epochs: int = 50,
                           use_fairness: bool = True, use_frank_wolfe: bool = True,
                           kl_weight: float = 0.1, fairness_beta: float = 1.0,
                           temperature: float = 0.1, data_type: str = "vgnn",
                           save_model: bool = True, verbose: bool = True) -> Tuple['VariationalDeepGraphClustering', 'VariationalClusteringTrainer', Dict]:
        """
        è®­ç»ƒå¢å¼ºçš„å˜åˆ†GNNèšç±»æ¨¡å‹ï¼Œæ”¯æŒå…¨é¢çš„å‚æ•°é…ç½®
        
        Args:
            data: å›¾æ•°æ®
            od_data: ODæ•°æ®
            adj_matrix: é‚»æ¥çŸ©é˜µ
            charge_num: å……ç”µæ¡©æ•°é‡ç‰¹å¾
            num_clusters: èšç±»æ•°é‡
            epochs: ä¸»è®­ç»ƒè½®æ•°
            pretrain_epochs: é¢„è®­ç»ƒè½®æ•°
            use_fairness: æ˜¯å¦ä½¿ç”¨å…¬å¹³æ€§æŸå¤±
            use_frank_wolfe: æ˜¯å¦ä½¿ç”¨Frank-Wolfeç®—æ³•
            kl_weight: KLæ•£åº¦æŸå¤±æƒé‡
            fairness_beta: å…¬å¹³æ€§æŸå¤±betaå‚æ•°
            temperature: å¯¹æ¯”å­¦ä¹ æ¸©åº¦å‚æ•°
            data_type: æ•°æ®ç±»å‹æ ‡è¯†
            save_model: æ˜¯å¦ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            (model, trainer, results): è®­ç»ƒå¥½çš„æ¨¡å‹ã€è®­ç»ƒå™¨å’Œç»“æœ
        """
        print(f"\nè®­ç»ƒå¢å¼ºå˜åˆ†GNNèšç±»æ¨¡å‹...")
        print(f"å…¬å¹³æ€§: {use_fairness}, Frank-Wolfe: {use_frank_wolfe}")
        print(f"KLæƒé‡: {kl_weight}, å…¬å¹³æ€§Beta: {fairness_beta}, æ¸©åº¦: {temperature}")
        print(f"èšç±»æ•°é‡: {num_clusters}, ä¸»è®­ç»ƒ: {epochs}è½®, é¢„è®­ç»ƒ: {pretrain_epochs}è½®")
        
        # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        data = data.to(self.device)
        adj_matrix = adj_matrix.to(self.device)
        if charge_num is not None:
            charge_num = charge_num.to(self.device)
        
        # æ›´æ–°æ¨¡å‹å‚æ•°
        self.model.kl_weight = kl_weight
        self.model.fairness_beta = fairness_beta
        self.model.temperature = temperature
        self.model.use_frank_wolfe = use_frank_wolfe
        
        # è¿è¡Œå®Œæ•´çš„è®­ç»ƒå·¥ä½œæµç¨‹
        results = self.train_full_workflow(
            data=data,
            epochs=epochs,
            pretrain_epochs=pretrain_epochs,
            em_max_iter=100,
            update_target_freq=10,
            verbose=verbose,
            use_fairness=use_fairness,
            use_frank_wolfe=use_frank_wolfe,
            data_type=data_type,
            save_model=save_model
        )
        
        # æ·»åŠ é¢å¤–çš„è®­ç»ƒä¿¡æ¯åˆ°ç»“æœä¸­
        results['training_config'] = {
            'use_fairness': use_fairness,
            'use_frank_wolfe': use_frank_wolfe,
            'kl_weight': kl_weight,
            'fairness_beta': fairness_beta,
            'temperature': temperature,
            'num_clusters': num_clusters,
            'epochs': epochs,
            'pretrain_epochs': pretrain_epochs,
            'data_type': data_type
        }
        
        print(f"\nâœ… å¢å¼ºå˜åˆ†GNNèšç±»è®­ç»ƒå®Œæˆ!")
        print(f"   - æ¨¡å‹æ¶æ„: {self.model.encoder_type.upper()} å˜åˆ†ç¼–ç å™¨")
        print(f"   - æœ€ç»ˆèšç±»è´¨é‡æŒ‡æ ‡:")
        if 'final_metrics' in results:
            for metric, value in results['final_metrics'].items():
                print(f"     â€¢ {metric}: {value:.4f}")
        
        return self.model, self, results