"""
ADP Training Module - ç”µåŠ¨è½¦å……ç”µä¼˜åŒ–è®­ç»ƒå™¨
"""
import os
import sys
import numpy as np
import matplotlib.pyplot as plt
import random
from pathlib import Path
import time
from collections import defaultdict, deque
import pandas as pd
from datetime import datetime
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from datetime import datetime
from src.ChargingIntegrationVisualization import ChargingIntegrationVisualization
# å¯¼å…¥é…ç½®ç®¡ç†å™¨
from config.config_manager import ConfigManager, get_config, get_training_config, get_sampling_config

# å¯¼å…¥æ ¸å¿ƒç»„ä»¶
from .Environment import ChargingIntegratedEnvironment
from .ValueFunction_pytorch import PyTorchChargingValueFunction
from .Action import Action, ChargingAction, ServiceAction
from .Request import Request
from .charging_station import ChargingStationManager, ChargingStation
from .CentralAgent import CentralAgent
from .SpatialVisualization import SpatialVisualization


class ADPTrainer:
    """ADPè®­ç»ƒå™¨ç±» - è´Ÿè´£ç”µåŠ¨è½¦å……ç”µä¼˜åŒ–çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒ"""
    
    def __init__(self, config_manager=None):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config_manager: é…ç½®ç®¡ç†å™¨å®ä¾‹
        """
        self.config_manager = config_manager or ConfigManager()
        self.training_config = self.config_manager.get_training_config()
        self.env_config = self.config_manager.get_environment_config()
        self.sampling_config = self.config_manager.get_sampling_config()
        self.adp_value = self.training_config.get('adp_value', 0)
        self.assignmentgurobi = self.training_config.get('assignmentgurobi', True)
        # è®­ç»ƒçŠ¶æ€  
        self.env = None
        self.batch_size = self.training_config.get('batch_size', 256)
        self.value_function = None
        self.training_history = {
            'episode_rewards': [],
            'training_losses': [],
            'q_values': [],
            'exploration_rates': []
        }
        
        print("ğŸš€ ADPTraineråˆå§‹åŒ–å®Œæˆ")
        print(f"   - é…ç½®åŠ è½½: {self.config_manager.config_path}")
    
    def setup_environment(self, num_vehicles=None, num_stations=None):
        """
        è®¾ç½®è®­ç»ƒç¯å¢ƒ
        
        Args:
            num_vehicles: è½¦è¾†æ•°é‡ï¼Œé»˜è®¤ä»é…ç½®è·å–
            num_stations: å……ç”µç«™æ•°é‡ï¼Œé»˜è®¤ä»é…ç½®è·å–
        """
        num_vehicles = num_vehicles or self.env_config.get('max_vehicles', 40)
        num_stations = num_stations or self.env_config.get('max_charging_stations', 12)
        
        self.env = ChargingIntegratedEnvironment(
            num_vehicles=num_vehicles, 
            num_stations=num_stations
        )
        
        print(f"âœ“ ç¯å¢ƒè®¾ç½®å®Œæˆ: {num_vehicles}è¾†è½¦, {num_stations}ä¸ªå……ç”µç«™")
        return self.env
    
    def setup_value_function(self):
        """
        è®¾ç½®ä»·å€¼å‡½æ•°
        
        Args:
            adp_value: ADPå‚æ•°å€¼
            use_neural_network: æ˜¯å¦ä½¿ç”¨ç¥ç»ç½‘ç»œ
        """
        use_neural_network = self.adp_value > 0 and self.assignmentgurobi
        if use_neural_network and self.adp_value > 0:
            network_config = self.config_manager.get_network_config()
            
            self.value_function = PyTorchChargingValueFunction(
                grid_size=self.env.grid_size,
                num_vehicles=self.env.num_vehicles,
                device='cuda' if torch.cuda.is_available() else 'cpu',
                episode_length=self.env.episode_length,
                max_requests=1000,
            )
            
            # è®¾ç½®ä»·å€¼å‡½æ•°åˆ°ç¯å¢ƒ
            self.env.set_value_function(self.value_function)
            
            print(f"âœ“ ç¥ç»ç½‘ç»œä»·å€¼å‡½æ•°åˆå§‹åŒ–å®Œæˆ")
            print(f"   - ç½‘ç»œå‚æ•°æ•°é‡: {sum(p.numel() for p in self.value_function.network.parameters())}")
            print(f"   - è®¾å¤‡: {self.value_function.device}")
        else:
            self.value_function = None
            print(f"âœ“ ä¸ä½¿ç”¨ç¥ç»ç½‘ç»œ (ADP={self.adp_value})")

        return self.value_function
    
