"""
ç®€åŒ–çš„Zone-ADPé›†æˆæµ‹è¯•è„šæœ¬

æ­¤ç‰ˆæœ¬å»æ‰äº†å¤æ‚çš„ä¾èµ–ï¼Œä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½æµ‹è¯•
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯

print("å¼€å§‹Zone-ADPç®€åŒ–æµ‹è¯•")

def test_basic_pytorch():
    """æµ‹è¯•åŸºæœ¬PyTorchåŠŸèƒ½"""
    print("\n=== æµ‹è¯•åŸºæœ¬PyTorchåŠŸèƒ½ ===")
    
    # æ£€æŸ¥PyTorchç‰ˆæœ¬å’ŒCUDA
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDAè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
        print(f"å½“å‰CUDAè®¾å¤‡: {torch.cuda.current_device()}")
    
    # åˆ›å»ºç®€å•å¼ é‡
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    x = torch.randn(5, 3).to(device)
    y = torch.randn(3, 4).to(device)
    z = torch.mm(x, y)
    
    print(f"å¼ é‡è¿ç®—æˆåŠŸï¼Œè®¾å¤‡: {device}")
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}, {y.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {z.shape}")
    
    return device

def test_simple_network():
    """æµ‹è¯•ç®€å•ç¥ç»ç½‘ç»œ"""
    print("\n=== æµ‹è¯•ç®€å•ç¥ç»ç½‘ç»œ ===")
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    class SimpleNetwork(nn.Module):
        def __init__(self, input_dim, hidden_dim, output_dim):
            super().__init__()
            self.layers = nn.Sequential(
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, output_dim)
            )
        
        def forward(self, x):
            return self.layers(x)
    
    # åˆ›å»ºç½‘ç»œ
    net = SimpleNetwork(10, 64, 1).to(device)
    print(f"ç½‘ç»œå‚æ•°æ•°é‡: {sum(p.numel() for p in net.parameters())}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size = 32
    x = torch.randn(batch_size, 10).to(device)
    y = net(x)
    
    print(f"å‰å‘ä¼ æ’­æˆåŠŸ")
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {y.shape}")
    print(f"è¾“å‡ºèŒƒå›´: [{y.min().item():.4f}, {y.max().item():.4f}]")
    
    # æµ‹è¯•åå‘ä¼ æ’­
    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)
    target = torch.randn(batch_size, 1).to(device)
    loss = F.mse_loss(y, target)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    print(f"åå‘ä¼ æ’­æˆåŠŸï¼ŒæŸå¤±: {loss.item():.4f}")
    
    return net

def test_zone_environment_simple():
    """æµ‹è¯•ç®€åŒ–çš„Zoneç¯å¢ƒ"""
    print("\n=== æµ‹è¯•ç®€åŒ–Zoneç¯å¢ƒ ===")
    
    class SimpleZoneEnvironment:
        def __init__(self, num_agents=5, num_zones=3, num_locations=20):
            self.num_agents = num_agents
            self.num_zones = num_zones
            self.num_locations = num_locations
            
            # çŠ¶æ€ï¼šæ™ºèƒ½ä½“ä½ç½®å’ŒåŒºåŸŸåˆ†é…
            self.agent_positions = np.random.randint(0, num_locations, num_agents)
            self.zone_assignments = np.random.randint(0, num_zones, num_agents)
            
            # éœ€æ±‚çŸ©é˜µ
            self.demand_matrix = np.random.exponential(1.0, (num_locations, num_locations))
            
            self.current_step = 0
            self.max_steps = 100
        
        def reset(self):
            self.agent_positions = np.random.randint(0, self.num_locations, self.num_agents)
            self.zone_assignments = np.random.randint(0, self.num_zones, self.num_agents)
            self.current_step = 0
            return self.get_state()
        
        def get_state(self):
            # ç®€åŒ–çŠ¶æ€ï¼š[æ™ºèƒ½ä½“ä½ç½®, åŒºåŸŸåˆ†é…, èšåˆéœ€æ±‚]
            state = np.concatenate([
                self.agent_positions / self.num_locations,  # å½’ä¸€åŒ–ä½ç½®
                np.eye(self.num_zones)[self.zone_assignments].flatten(),  # one-hotåŒºåŸŸ
                [self.demand_matrix.sum() / 1000]  # å½’ä¸€åŒ–æ€»éœ€æ±‚
            ])
            return state.astype(np.float32)
        
        def step(self, actions):
            # actions: æ¯ä¸ªæ™ºèƒ½ä½“çš„æ–°åŒºåŸŸåˆ†é…
            self.zone_assignments = actions
            
            # ç®€å•çš„ä½ç½®æ›´æ–°
            self.agent_positions += np.random.randint(-1, 2, self.num_agents)
            self.agent_positions = np.clip(self.agent_positions, 0, self.num_locations - 1)
            
            # è®¡ç®—å¥–åŠ±
            reward = self.calculate_reward()
            
            self.current_step += 1
            done = self.current_step >= self.max_steps
            
            info = {
                'zone_distribution': np.bincount(self.zone_assignments, minlength=self.num_zones),
                'average_position': self.agent_positions.mean()
            }
            
            return self.get_state(), reward, done, info
        
        def calculate_reward(self):
            # åŒºåŸŸå¹³è¡¡å¥–åŠ±
            zone_counts = np.bincount(self.zone_assignments, minlength=self.num_zones)
            balance_reward = -np.var(zone_counts)
            
            # éœ€æ±‚æ»¡è¶³å¥–åŠ±ï¼ˆç®€åŒ–ï¼‰
            demand_reward = np.random.random() * 10
            
            return balance_reward + demand_reward
    
    # æµ‹è¯•ç¯å¢ƒ
    env = SimpleZoneEnvironment()
    state = env.reset()
    
    print(f"ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
    print(f"æ™ºèƒ½ä½“æ•°é‡: {env.num_agents}")
    print(f"åŒºåŸŸæ•°é‡: {env.num_zones}")
    print(f"ä½ç½®æ•°é‡: {env.num_locations}")
    print(f"çŠ¶æ€ç»´åº¦: {len(state)}")
    
    # æµ‹è¯•å‡ ä¸ªæ­¥éª¤
    total_reward = 0
    for step in range(5):
        action = np.random.randint(0, env.num_zones, env.num_agents)
        next_state, reward, done, info = env.step(action)
        total_reward += reward
        
        print(f"æ­¥éª¤ {step}: å¥–åŠ±={reward:.2f}, åŒºåŸŸåˆ†å¸ƒ={info['zone_distribution']}")
        
        if done:
            break
    
    print(f"æ€»å¥–åŠ±: {total_reward:.2f}")
    
    return env

def test_charging_actions():
    """æµ‹è¯•å……ç”µåŠ¨ä½œåŠŸèƒ½"""
    print("\n=== æµ‹è¯•å……ç”µåŠ¨ä½œåŠŸèƒ½ ===")
    
    # å¯¼å…¥å……ç”µç›¸å…³æ¨¡å—
    import sys
    sys.path.append('src')
    
    try:
        from Action import Action, ChargingAction
        from Request import Request
        from charging_station import ChargingStation, ChargingStationManager
        
        print("âœ“ æˆåŠŸå¯¼å…¥å……ç”µç›¸å…³æ¨¡å—")
        
        # åˆ›å»ºå……ç”µç«™ç®¡ç†å™¨
        station_manager = ChargingStationManager()
        
        # æ·»åŠ å……ç”µç«™
        station_manager.add_station(1, location=10, capacity=2)
        station_manager.add_station(2, location=20, capacity=3)
        station_manager.add_station(3, location=30, capacity=4)
        
        print(f"âœ“ åˆ›å»ºäº† {len(station_manager.stations)} ä¸ªå……ç”µç«™")
        
        # æµ‹è¯•åŸºæœ¬åŠ¨ä½œ
        requests = []  # ç©ºè¯·æ±‚åˆ—è¡¨
        basic_action = Action(requests)
        print(f"âœ“ åˆ›å»ºåŸºæœ¬åŠ¨ä½œ: {type(basic_action).__name__}")
        
        # æµ‹è¯•å……ç”µåŠ¨ä½œ
        charging_action1 = ChargingAction(requests, charging_station_id=1, charging_duration=25.0)
        charging_action2 = ChargingAction(requests, charging_station_id=2, charging_duration=30.0)
        
        print(f"âœ“ åˆ›å»ºå……ç”µåŠ¨ä½œ: {type(charging_action1).__name__}")
        print(f"  - å……ç”µç«™1: {charging_action1.get_charging_info()}")
        print(f"  - å……ç”µç«™2: {charging_action2.get_charging_info()}")
        
        # æµ‹è¯•åŠ¨ä½œç›¸ç­‰æ€§
        charging_action3 = ChargingAction(requests, charging_station_id=1, charging_duration=25.0)
        print(f"âœ“ åŠ¨ä½œç›¸ç­‰æ€§æµ‹è¯•: {charging_action1 == charging_action3}")
        print(f"âœ“ åŠ¨ä½œä¸ç­‰æ€§æµ‹è¯•: {charging_action1 == charging_action2}")
        
        # æµ‹è¯•å……ç”µç«™åŠŸèƒ½
        station1 = station_manager.stations[1]
        
        # æ¨¡æ‹Ÿè½¦è¾†å……ç”µ
        vehicle_ids = ['vehicle_1', 'vehicle_2', 'vehicle_3']
        
        for vehicle_id in vehicle_ids:
            success = station1.start_charging(vehicle_id)
            print(f"  - è½¦è¾† {vehicle_id} å¼€å§‹å……ç”µ: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
        
        # æ£€æŸ¥å……ç”µç«™çŠ¶æ€
        status = station1.get_station_status()
        print(f"âœ“ å……ç”µç«™1çŠ¶æ€: åˆ©ç”¨ç‡{status['utilization_rate']:.1%}, é˜Ÿåˆ—é•¿åº¦{status['queue_length']}")
        
        # æµ‹è¯•å¯»æ‰¾æœ€è¿‘çš„å¯ç”¨å……ç”µç«™
        nearest_station = station_manager.get_nearest_available_station(vehicle_location=15)
        if nearest_station:
            print(f"âœ“ ä½ç½®15æœ€è¿‘çš„å¯ç”¨å……ç”µç«™: {nearest_station.id}")
        else:
            print("âœ“ æ²¡æœ‰å¯ç”¨çš„å……ç”µç«™")
        
        # å®Œæˆä¸€ä¸ªè½¦è¾†çš„å……ç”µ
        station1.stop_charging('vehicle_1')
        
        # å†æ¬¡æ£€æŸ¥çŠ¶æ€
        status = station1.get_station_status()
        print(f"âœ“ å……ç”µå®ŒæˆåçŠ¶æ€: åˆ©ç”¨ç‡{status['utilization_rate']:.1%}, é˜Ÿåˆ—é•¿åº¦{status['queue_length']}")
        
        return True
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"âŒ å……ç”µåŠ¨ä½œæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_charging_integration():
    """æµ‹è¯•å……ç”µåŠ¨ä½œä¸ç¯å¢ƒçš„é›†æˆ"""
    print("\n=== æµ‹è¯•å……ç”µåŠ¨ä½œé›†æˆ ===")
    
    import sys
    sys.path.append('src')
    
    try:
        from Action import Action, ChargingAction
        from charging_station import ChargingStationManager
        
        # åˆ›å»ºç®€å•çš„æ™ºèƒ½ä½“ç±»æ¥æµ‹è¯•åŠ¨ä½œ
        class SimpleAgent:
            def __init__(self, agent_id: str, location: int):
                self.agent_id = agent_id
                self.location = location
                self.battery_level = np.random.uniform(0.1, 0.9)  # ç”µæ± ç”µé‡ 10%-90%
                self.is_charging = False
                
            def needs_charging(self) -> bool:
                """åˆ¤æ–­æ˜¯å¦éœ€è¦å……ç”µ"""
                return self.battery_level < 0.3
            
            def can_act(self, action) -> bool:
                """æ£€æŸ¥æ˜¯å¦å¯ä»¥æ‰§è¡ŒåŠ¨ä½œ"""
                if isinstance(action, ChargingAction):
                    return self.needs_charging() and not self.is_charging
                return True  # å…¶ä»–åŠ¨ä½œé»˜è®¤å¯ä»¥æ‰§è¡Œ
            
            def execute_action(self, action) -> bool:
                """æ‰§è¡ŒåŠ¨ä½œ"""
                if isinstance(action, ChargingAction):
                    if self.can_act(action):
                        self.is_charging = True
                        print(f"æ™ºèƒ½ä½“ {self.agent_id} æ‰§è¡Œå……ç”µåŠ¨ä½œ -> å……ç”µç«™ {action.charging_station_id}")
                        return True
                    else:
                        print(f"æ™ºèƒ½ä½“ {self.agent_id} æ— æ³•æ‰§è¡Œå……ç”µåŠ¨ä½œ (ç”µé‡: {self.battery_level:.1%})")
                        return False
                else:
                    print(f"æ™ºèƒ½ä½“ {self.agent_id} æ‰§è¡Œæ™®é€šåŠ¨ä½œ")
                    return True
        
        # åˆ›å»ºæµ‹è¯•æ™ºèƒ½ä½“
        agents = [
            SimpleAgent("agent_1", 10),  # ä½ç”µé‡
            SimpleAgent("agent_2", 20),  # ä¸­ç­‰ç”µé‡ 
            SimpleAgent("agent_3", 30),  # é«˜ç”µé‡
        ]
        
        # è®¾ç½®ä¸€äº›æ™ºèƒ½ä½“ä¸ºä½ç”µé‡
        agents[0].battery_level = 0.15  # éœ€è¦å……ç”µ
        agents[1].battery_level = 0.25  # éœ€è¦å……ç”µ
        agents[2].battery_level = 0.80  # ä¸éœ€è¦å……ç”µ
        
        print(f"âœ“ åˆ›å»ºäº† {len(agents)} ä¸ªæµ‹è¯•æ™ºèƒ½ä½“")
        for agent in agents:
            print(f"  - {agent.agent_id}: ä½ç½®{agent.location}, ç”µé‡{agent.battery_level:.1%}, éœ€è¦å……ç”µ: {agent.needs_charging()}")
        
        # åˆ›å»ºåŠ¨ä½œé€‰é¡¹
        actions = [
            Action([]),  # æ™®é€šåŠ¨ä½œ
            ChargingAction([], charging_station_id=1, charging_duration=20.0),
            ChargingAction([], charging_station_id=2, charging_duration=25.0),
        ]
        
        print(f"âœ“ åˆ›å»ºäº† {len(actions)} ä¸ªåŠ¨ä½œé€‰é¡¹")
        
        # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“åˆ†é…åŠ¨ä½œ
        action_results = []
        for i, agent in enumerate(agents):
            # æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œé€»è¾‘
            if agent.needs_charging():
                # é€‰æ‹©å……ç”µåŠ¨ä½œ
                chosen_action = actions[1] if i % 2 == 0 else actions[2]
            else:
                # é€‰æ‹©æ™®é€šåŠ¨ä½œ
                chosen_action = actions[0]
            
            # æ‰§è¡ŒåŠ¨ä½œ
            success = agent.execute_action(chosen_action)
            action_results.append((agent.agent_id, chosen_action, success))
        
        # æ€»ç»“ç»“æœ
        successful_actions = sum(1 for _, _, success in action_results if success)
        charging_actions = sum(1 for _, action, _ in action_results if isinstance(action, ChargingAction))
        
        print(f"âœ“ åŠ¨ä½œæ‰§è¡Œæ€»ç»“:")
        print(f"  - æ€»åŠ¨ä½œæ•°: {len(action_results)}")
        print(f"  - æˆåŠŸæ‰§è¡Œ: {successful_actions}")
        print(f"  - å……ç”µåŠ¨ä½œæ•°: {charging_actions}")
        
        return successful_actions == len(action_results)
        
    except Exception as e:
        print(f"âŒ å……ç”µé›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_simple_dqn():
    """æµ‹è¯•ç®€å•çš„DQNæ™ºèƒ½ä½“"""
    print("\n=== æµ‹è¯•ç®€å•DQNæ™ºèƒ½ä½“ ===")
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    class SimpleDQN(nn.Module):
        def __init__(self, state_dim, action_dim, hidden_dim=64):
            super().__init__()
            self.network = nn.Sequential(
                nn.Linear(state_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, action_dim)
            )
        
        def forward(self, x):
            return self.network(x)
    
    class SimpleAgent:
        def __init__(self, state_dim, action_dim, lr=0.001):
            self.q_network = SimpleDQN(state_dim, action_dim).to(device)
            self.target_network = SimpleDQN(state_dim, action_dim).to(device)
            self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=lr)
            
            # å¤åˆ¶æƒé‡åˆ°ç›®æ ‡ç½‘ç»œ
            self.target_network.load_state_dict(self.q_network.state_dict())
            
            # æ”¹è¿›çš„æ¢ç´¢ç­–ç•¥ï¼šå¼€å§‹æ—¶é«˜æ¢ç´¢ï¼Œé€æ¸å‡å°‘
            self.epsilon_start = 0.9
            self.epsilon_end = 0.05
            self.epsilon_decay = 0.995
            self.epsilon = self.epsilon_start
            self.gamma = 0.99
            self.step_count = 0
        
        def select_action(self, state, num_agents, num_zones):
            # åŠ¨æ€è°ƒæ•´epsilonï¼ˆæ¢ç´¢ç‡ï¼‰
            self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)
            
            if np.random.random() < self.epsilon:
                # éšæœºç­–ç•¥
                action = np.random.randint(0, num_zones, num_agents)
                action_type = "random"
            else:
                # ADPç­–ç•¥ï¼šåŸºäºå­¦ä¹ åˆ°çš„Qå€¼
                with torch.no_grad():
                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
                    q_values = self.q_network(state_tensor)
                    
                    # æ”¹è¿›ï¼šä¸ºæ¯ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹é€‰æ‹©æœ€ä¼˜åŒºåŸŸ
                    # è¿™é‡Œç®€åŒ–ä¸ºé€‰æ‹©Qå€¼æœ€é«˜çš„åŒºåŸŸ
                    best_zone = q_values.argmax().item() % num_zones
                    action = np.full(num_agents, best_zone)
                    action_type = "adp"
            
            self.step_count += 1
            return action, action_type        
        def train_step(self, state, action, reward, next_state, done):
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(device)
            reward_tensor = torch.FloatTensor([reward]).to(device)
            done_tensor = torch.BoolTensor([done]).to(device)
            
            # å½“å‰Qå€¼
            current_q = self.q_network(state_tensor)[0, 0]  # ç®€åŒ–
            
            # ç›®æ ‡Qå€¼
            with torch.no_grad():
                next_q = self.target_network(next_state_tensor).max(1)[0]
                target_q = reward_tensor + self.gamma * next_q * ~done_tensor
            
            # æŸå¤±å’Œä¼˜åŒ–
            loss = F.mse_loss(current_q.unsqueeze(0), target_q)
            
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            
            return loss.item()
      # åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“ï¼ˆä½¿ç”¨å±€éƒ¨å®šä¹‰çš„ç±»ï¼‰
    class LocalSimpleZoneEnvironment:
        def __init__(self, num_agents=5, num_zones=3, num_locations=20):
            self.num_agents = num_agents
            self.num_zones = num_zones
            self.num_locations = num_locations
            
            self.agent_positions = np.random.randint(0, num_locations, num_agents)
            self.zone_assignments = np.random.randint(0, num_zones, num_agents)
            self.demand_matrix = np.random.exponential(1.0, (num_locations, num_locations))
            
            self.current_step = 0
            self.max_steps = 100
        
        def reset(self):
            self.agent_positions = np.random.randint(0, self.num_locations, self.num_agents)
            self.zone_assignments = np.random.randint(0, self.num_zones, self.num_agents)
            self.current_step = 0
            return self.get_state()
        
        def get_state(self):
            state = np.concatenate([
                self.agent_positions / self.num_locations,
                np.eye(self.num_zones)[self.zone_assignments].flatten(),
                [self.demand_matrix.sum() / 1000]
            ])
            return state.astype(np.float32)
        
        def step(self, actions):
            self.zone_assignments = actions
            self.agent_positions += np.random.randint(-1, 2, self.num_agents)
            self.agent_positions = np.clip(self.agent_positions, 0, self.num_locations - 1)
            
            zone_counts = np.bincount(self.zone_assignments, minlength=self.num_zones)
            balance_reward = -np.var(zone_counts)
            demand_reward = np.random.random() * 10
            reward = balance_reward + demand_reward
            
            self.current_step += 1
            done = self.current_step >= self.max_steps
            
            info = {
                'zone_distribution': zone_counts,
                'average_position': self.agent_positions.mean()
            }
            
            return self.get_state(), reward, done, info
    
    env = LocalSimpleZoneEnvironment(num_agents=3, num_zones=2)
    state = env.reset()
    
    agent = SimpleAgent(
        state_dim=len(state),
        action_dim=env.num_zones,
        lr=0.001
    )
    
    print(f"DQNæ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ")
    print(f"ç½‘ç»œå‚æ•°: {sum(p.numel() for p in agent.q_network.parameters())}")
    print(f"çŠ¶æ€ç»´åº¦: {len(state)}")
    print(f"åŠ¨ä½œç»´åº¦: {env.num_zones}")
    
    # è®­ç»ƒå‡ ä¸ªå›åˆ
    episode_rewards = []
    episode_losses = []
    moving_average_rewards = []
    epsilon_history = []
    action_type_counts = {'random': 0, 'adp': 0}
    
    print("å¼€å§‹è®­ç»ƒæ™ºèƒ½ä½“...")
    print(f"åˆå§‹æ¢ç´¢ç‡: {agent.epsilon:.3f}")
    
    for episode in range(500):  # å¢åŠ è®­ç»ƒå›åˆæ•°ä»¥ä¾¿è§‚å¯Ÿè¶‹åŠ¿
        state = env.reset()
        episode_reward = 0
        episode_loss = 0
        step_count = 0
        episode_random_count = 0
        episode_adp_count = 0
        
        for step in range(30):  # å¢åŠ æ¯å›åˆæ­¥æ•°
            action, action_type = agent.select_action(state, env.num_agents, env.num_zones)
            
            # è®°å½•åŠ¨ä½œç±»å‹
            if action_type == 'random':
                episode_random_count += 1
                action_type_counts['random'] += 1
            else:
                episode_adp_count += 1
                action_type_counts['adp'] += 1
                
            next_state, reward, done, info = env.step(action)
            
            # è®­ç»ƒ
            loss = agent.train_step(state, action, reward, next_state, done)
            
            state = next_state
            episode_reward += reward
            episode_loss += loss
            step_count += 1
            
            if done:
                break
        
        # è®°å½•å†å²æ•°æ®
        avg_episode_loss = episode_loss / step_count if step_count > 0 else 0
        episode_rewards.append(episode_reward)
        episode_losses.append(avg_episode_loss)
        epsilon_history.append(agent.epsilon)
        
        # è®¡ç®—ç§»åŠ¨å¹³å‡å¥–åŠ±ï¼ˆçª—å£å¤§å°ä¸º5ï¼‰
        window_size = 5
        if len(episode_rewards) >= window_size:
            moving_avg = np.mean(episode_rewards[-window_size:])
            moving_average_rewards.append(moving_avg)
        else:
            moving_average_rewards.append(np.mean(episode_rewards))
        
        # æ¯10ä¸ªå›åˆæ›´æ–°ä¸€æ¬¡ç›®æ ‡ç½‘ç»œ
        if episode % 10 == 0:
            agent.target_network.load_state_dict(agent.q_network.state_dict())
            
        if episode % 10 == 0:
            adp_percentage = (episode_adp_count / (episode_adp_count + episode_random_count)) * 100 if (episode_adp_count + episode_random_count) > 0 else 0
            print(f"å›åˆ {episode}: å¥–åŠ±={episode_reward:.2f}, ç§»åŠ¨å¹³å‡={moving_average_rewards[-1]:.2f}, " +
                  f"æŸå¤±={avg_episode_loss:.4f}, æ¢ç´¢ç‡={agent.epsilon:.3f}, ADPä½¿ç”¨ç‡={adp_percentage:.1f}%")
    
    # å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
    plt.figure(figsize=(20, 10))
    
    # å­å›¾1: å¥–åŠ±æ›²çº¿
    plt.subplot(2, 3, 1)
    plt.plot(episode_rewards, alpha=0.6, label='Episode Reward', color='lightblue')
    plt.plot(moving_average_rewards, label='Moving Average', color='darkblue', linewidth=2)
    plt.xlabel('Training Episode')
    plt.ylabel('Reward')
    plt.title('Agent Training Reward Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å­å›¾2: æŸå¤±æ›²çº¿
    plt.subplot(2, 3, 2)
    plt.plot(episode_losses, label='Training Loss', color='red', alpha=0.7)
    plt.xlabel('Training Episode')
    plt.ylabel('Loss')
    plt.title('Agent Training Loss Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å­å›¾3: æ¢ç´¢ç‡å˜åŒ–
    plt.subplot(2, 3, 3)
    plt.plot(epsilon_history, label='Exploration Rate', color='green', linewidth=2)
    plt.xlabel('Training Episode')
    plt.ylabel('Epsilon')
    plt.title('Exploration Rate Decay Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å­å›¾4: å¥–åŠ±åˆ†å¸ƒ
    plt.subplot(2, 3, 4)
    plt.hist(episode_rewards, bins=15, alpha=0.7, color='orange', edgecolor='black')
    plt.xlabel('Reward Value')
    plt.ylabel('Frequency')
    plt.title('Reward Distribution Histogram')
    plt.grid(True, alpha=0.3)
    
    # å­å›¾5: ç­–ç•¥ä½¿ç”¨ç»Ÿè®¡
    plt.subplot(2, 3, 5)
    total_actions = sum(action_type_counts.values())
    if total_actions > 0:
        adp_percentage = (action_type_counts['adp'] / total_actions) * 100
        random_percentage = (action_type_counts['random'] / total_actions) * 100
        
        plt.pie([adp_percentage, random_percentage], 
                labels=[f'ADP Policy ({adp_percentage:.1f}%)', f'Random Policy ({random_percentage:.1f}%)'],
                autopct='%1.1f%%', colors=['skyblue', 'lightcoral'])
        plt.title('Policy Usage Distribution')
    
    # å­å›¾6: å¥–åŠ±è¶‹åŠ¿åˆ†æ
    plt.subplot(2, 3, 6)
    if len(episode_rewards) >= 10:
        early_rewards = episode_rewards[:10]
        late_rewards = episode_rewards[-10:]
        
        plt.boxplot([early_rewards, late_rewards], labels=['Early (1-10)', 'Late (41-50)'])
        plt.ylabel('Reward')
        plt.title('Early vs Late Reward Comparison')
        plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾åƒ
    results_dir = Path("results/simple_tests")
    results_dir.mkdir(parents=True, exist_ok=True)
    
    plot_path = results_dir / "training_curves.png"
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    # è®¡ç®—è®­ç»ƒç»Ÿè®¡
    avg_reward = np.mean(episode_rewards)
    final_avg_reward = np.mean(episode_rewards[-10:])  # æœ€å10å›åˆçš„å¹³å‡å¥–åŠ±
    initial_avg_reward = np.mean(episode_rewards[:10])  # å‰10å›åˆçš„å¹³å‡å¥–åŠ±
    improvement = final_avg_reward - initial_avg_reward
    total_actions = sum(action_type_counts.values())
    adp_usage_percentage = (action_type_counts['adp'] / total_actions) * 100 if total_actions > 0 else 0
    
    print(f"\n=== è®­ç»ƒç»“æœåˆ†æ ===")
    print(f"æ€»å›åˆæ•°: {len(episode_rewards)}")
    print(f"å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
    print(f"åˆæœŸå¹³å‡å¥–åŠ±(å‰10å›åˆ): {initial_avg_reward:.2f}")
    print(f"åæœŸå¹³å‡å¥–åŠ±(å10å›åˆ): {final_avg_reward:.2f}")
    print(f"å¥–åŠ±æå‡: {improvement:.2f} ({improvement/abs(initial_avg_reward)*100:.1f}%)")
    print(f"æœ€é«˜å¥–åŠ±: {max(episode_rewards):.2f}")
    print(f"æœ€ä½å¥–åŠ±: {min(episode_rewards):.2f}")
    print(f"æœ€ç»ˆæ¢ç´¢ç‡: {agent.epsilon:.3f}")
    print(f"ADPç­–ç•¥ä½¿ç”¨ç‡: {adp_usage_percentage:.1f}%")
    print(f"å¥–åŠ±è¶‹åŠ¿: {'ä¸Šå‡' if improvement > 0 else 'ä¸‹é™' if improvement < 0 else 'ç¨³å®š'}")
    print(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜è‡³: {plot_path}")
    
    # éªŒè¯å¥–åŠ±æ˜¯å¦æœ‰ä¸Šå‡è¶‹åŠ¿
    reward_trend_test = improvement > 0
    adp_usage_test = adp_usage_percentage > 30  # ADPä½¿ç”¨ç‡åº”è¯¥è¶…è¿‡30%
    print(f"âœ“ Reward Improvement Test: {'PASS' if reward_trend_test else 'FAIL'}")
    print(f"âœ“ ADP Policy Usage Test: {'PASS' if adp_usage_test else 'FAIL'}")
    
    # ADPç­–ç•¥ä½¿ç”¨ç‡è§£é‡Š
    print(f"\n=== ADP Strategy Usage Explanation ===")
    print(f"ADP (Approximate Dynamic Programming) Policy Usage Rate: {adp_usage_percentage:.1f}%")
    print(f"- ADP actions: {action_type_counts['adp']} (learned policy)")
    print(f"- Random actions: {action_type_counts['random']} (exploration)")
    print(f"- Total actions: {total_actions}")
    print(f"\nWhat does ADP usage rate mean:")
    print(f"â€¢ ADPç­–ç•¥ä½¿ç”¨ç‡è¡¨ç¤ºæ™ºèƒ½ä½“ä½¿ç”¨å­¦ä¹ åˆ°çš„ç­–ç•¥ï¼ˆè€Œééšæœºç­–ç•¥ï¼‰çš„ç™¾åˆ†æ¯”")
    print(f"â€¢ Higher ADP usage indicates the agent is relying more on learned knowledge")
    print(f"â€¢ Lower usage means more exploration (random actions)")
    print(f"â€¢ Good training should show increasing ADP usage over time")
    print(f"â€¢ Our agent achieved {adp_usage_percentage:.1f}% ADP usage, showing it learned to prefer")
    print(f"  the trained policy over random actions")
    
    return agent, {
        'episode_rewards': episode_rewards,
        'moving_average_rewards': moving_average_rewards,
        'episode_losses': episode_losses,
        'epsilon_history': epsilon_history,
        'action_type_counts': action_type_counts,
        'improvement': improvement,
        'trend_positive': reward_trend_test,
        'adp_usage_percentage': adp_usage_percentage,
        'adp_usage_sufficient': adp_usage_test
    }

def test_save_load():
    """æµ‹è¯•æ¨¡å‹ä¿å­˜å’ŒåŠ è½½"""
    print("\n=== æµ‹è¯•æ¨¡å‹ä¿å­˜å’ŒåŠ è½½ ===")
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # åˆ›å»ºç®€å•æ¨¡å‹
    model = nn.Sequential(
        nn.Linear(10, 64),
        nn.ReLU(),
        nn.Linear(64, 1)
    ).to(device)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    x = torch.randn(5, 10).to(device)
    original_output = model(x)
    
    # ä¿å­˜æ¨¡å‹
    save_dir = Path("results/test_models")
    save_dir.mkdir(parents=True, exist_ok=True)
    
    model_path = save_dir / "test_model.pth"
    torch.save(model.state_dict(), model_path)
    
    print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
      # åˆ›å»ºæ–°æ¨¡å‹å¹¶åŠ è½½
    new_model = nn.Sequential(
        nn.Linear(10, 64),
        nn.ReLU(),
        nn.Linear(64, 1)
    ).to(device)
    
    new_model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))
    new_output = new_model(x)
    
    # éªŒè¯è¾“å‡ºç›¸åŒ
    diff = torch.abs(original_output - new_output).max().item()
    print(f"è¾“å‡ºå·®å¼‚: {diff:.8f}")
    
    success = diff < 1e-6
    print(f"ä¿å­˜/åŠ è½½æµ‹è¯•: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
    
    return success

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("="*60)
    print("Zone-ADPç®€åŒ–é›†æˆæµ‹è¯•")
    print("="*60)
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    try:
        # è¿è¡Œæµ‹è¯•
        print(f"å¼€å§‹æµ‹è¯•... {np.datetime64('now')}" if hasattr(np, 'datetime64') else "å¼€å§‹æµ‹è¯•...")
        
        device = test_basic_pytorch()
        net = test_simple_network()
        env = test_zone_environment_simple()
        
        # åˆ›å»ºå±€éƒ¨å‡½æ•°æ¥é¿å…å¯¼å…¥é—®é¢˜
        class SimpleZoneEnvironment:
            def __init__(self, num_agents=5, num_zones=3, num_locations=20):
                self.num_agents = num_agents
                self.num_zones = num_zones
                self.num_locations = num_locations
                
                self.agent_positions = np.random.randint(0, num_locations, num_agents)
                self.zone_assignments = np.random.randint(0, num_zones, num_agents)
                self.demand_matrix = np.random.exponential(1.0, (num_locations, num_locations))
                
                self.current_step = 0
                self.max_steps = 100
            
            def reset(self):
                self.agent_positions = np.random.randint(0, self.num_locations, self.num_agents)
                self.zone_assignments = np.random.randint(0, self.num_zones, self.num_agents)
                self.current_step = 0
                return self.get_state()
            
            def get_state(self):
                state = np.concatenate([
                    self.agent_positions / self.num_locations,
                    np.eye(self.num_zones)[self.zone_assignments].flatten(),
                    [self.demand_matrix.sum() / 1000]
                ])
                return state.astype(np.float32)
            
            def step(self, actions):
                self.zone_assignments = actions
                self.agent_positions += np.random.randint(-1, 2, self.num_agents)
                self.agent_positions = np.clip(self.agent_positions, 0, self.num_locations - 1)
                
                zone_counts = np.bincount(self.zone_assignments, minlength=self.num_zones)
                balance_reward = -np.var(zone_counts)
                demand_reward = np.random.random() * 10
                reward = balance_reward + demand_reward
                
                self.current_step += 1
                done = self.current_step >= self.max_steps
                
                info = {
                    'zone_distribution': zone_counts,
                    'average_position': self.agent_positions.mean()
                }
                
                return self.get_state(), reward, done, info
        
        # å°†ç¯å¢ƒç±»æ·»åŠ åˆ°å…¨å±€å‘½åç©ºé—´ä»¥ä¾›DQNæµ‹è¯•ä½¿ç”¨
        globals()['SimpleZoneEnvironment'] = SimpleZoneEnvironment
        
        agent, training_results = test_simple_dqn()
        save_success = test_save_load()
        
        # æ–°å¢å……ç”µåŠŸèƒ½æµ‹è¯•
        charging_basic_success = test_charging_actions()
        charging_integration_success = test_charging_integration()
        
        # åˆ†æè®­ç»ƒç»“æœ
        reward_improvement = training_results['improvement']
        reward_trend_positive = training_results['trend_positive']
        adp_usage_percentage = training_results['adp_usage_percentage']
        adp_usage_sufficient = training_results['adp_usage_sufficient']
        
        print("\n" + "="*60)
        print("æµ‹è¯•ç»“æœæ€»ç»“:")
        print("âœ“ PyTorchåŸºç¡€åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        print("âœ“ ç®€å•ç¥ç»ç½‘ç»œæµ‹è¯•é€šè¿‡")
        print("âœ“ Zoneç¯å¢ƒæµ‹è¯•é€šè¿‡")
        print(f"âœ“ DQNæ™ºèƒ½ä½“æµ‹è¯•é€šè¿‡ (å¥–åŠ±{'ä¸Šå‡' if reward_trend_positive else 'æœªä¸Šå‡'}: {reward_improvement:+.2f})")
        print(f"âœ“ ç­–ç•¥é€‰æ‹©æµ‹è¯•é€šè¿‡ (ADPä½¿ç”¨ç‡: {adp_usage_percentage:.1f}%)")
        print(f"{'âœ“' if save_success else 'âœ—'} æ¨¡å‹ä¿å­˜/åŠ è½½æµ‹è¯•{'é€šè¿‡' if save_success else 'å¤±è´¥'}")
        print(f"{'âœ“' if charging_basic_success else 'âœ—'} å……ç”µåŠ¨ä½œåŸºç¡€æµ‹è¯•{'é€šè¿‡' if charging_basic_success else 'å¤±è´¥'}")
        print(f"{'âœ“' if charging_integration_success else 'âœ—'} å……ç”µåŠ¨ä½œé›†æˆæµ‹è¯•{'é€šè¿‡' if charging_integration_success else 'å¤±è´¥'}")
        print(f"{'âœ“' if reward_trend_positive else 'âœ—'} å¥–åŠ±ä¸Šå‡è¶‹åŠ¿æµ‹è¯•{'é€šè¿‡' if reward_trend_positive else 'å¤±è´¥'}")
        print(f"{'âœ“' if adp_usage_sufficient else 'âœ—'} ADPç­–ç•¥ä½¿ç”¨ç‡æµ‹è¯•{'é€šè¿‡' if adp_usage_sufficient else 'å¤±è´¥'}")
        print("\nğŸ‰ æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•å®Œæˆ!")
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        # åˆ›å»ºè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š
        results_dir = Path("results/simple_tests")
        results_dir.mkdir(parents=True, exist_ok=True)
        
        report_path = results_dir / "test_report.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("Zone-ADPç®€åŒ–æµ‹è¯•æŠ¥å‘Šï¼ˆåŒ…å«å……ç”µåŠŸèƒ½å’Œå¯è§†åŒ–ï¼‰\n")
            f.write("="*50 + "\n")
            f.write(f"æµ‹è¯•è®¾å¤‡: {device}\n")
            f.write(f"PyTorchç‰ˆæœ¬: {torch.__version__}\n")
            f.write("åŸºç¡€åŠŸèƒ½æµ‹è¯•: é€šè¿‡\n")
            f.write(f"å……ç”µåŸºç¡€åŠŸèƒ½: {'é€šè¿‡' if charging_basic_success else 'å¤±è´¥'}\n")
            f.write(f"å……ç”µé›†æˆåŠŸèƒ½: {'é€šè¿‡' if charging_integration_success else 'å¤±è´¥'}\n")
            f.write(f"æ™ºèƒ½ä½“è®­ç»ƒ: {'é€šè¿‡' if reward_trend_positive else 'å¤±è´¥'}\n")
            f.write(f"å¥–åŠ±æ”¹å–„: {reward_improvement:+.2f}\n")
            f.write(f"ADPç­–ç•¥ä½¿ç”¨ç‡: {adp_usage_percentage:.1f}%\n")
            f.write(f"ç­–ç•¥æµ‹è¯•: {'é€šè¿‡' if adp_usage_sufficient else 'å¤±è´¥'}\n")
            f.write("å¯è§†åŒ–å›¾è¡¨: training_curves.png\n")
        
        print(f"æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
