"""
å¯å‘å¼ç­–ç•¥ vs å¼ºåŒ–å­¦ä¹ ç­–ç•¥æ€§èƒ½å¯¹æ¯”æµ‹è¯•
"""

import os
import sys
import time
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.Environment import ChargingIntegratedEnvironment
from src.LearningAgent import LearningAgent
from src.Heuristic import HeuristicPolicy
from src.ChargingIntegrationVisualization import ChargingIntegrationVisualization


class PolicyComparison:
    """ç­–ç•¥å¯¹æ¯”æµ‹è¯•ç±»"""
    
    def __init__(self, config=None):
        self.config = config or self._get_default_config()
        self.results = {}
        
    def _get_default_config(self):
        """è·å–é»˜è®¤æµ‹è¯•é…ç½®"""
        return {
            'grid_size': 10,
            'num_vehicles': 12,
            'num_stations': 3,
            'use_intense_requests': True,
            'episode_length': 200,
            'num_test_episodes': 5,  # å‡å°‘æµ‹è¯•è½®æ¬¡ä»¥åŠ å¿«æµ‹è¯•
            'value_function_path': './logs/charging_nn/PyTorchChargingValueFunction/value_function_20241214_190439.pth'
        }
    
    def run_comparison(self):
        """è¿è¡Œå®Œæ•´çš„å¯¹æ¯”æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹ç­–ç•¥å¯¹æ¯”æµ‹è¯•...")
        
        # æµ‹è¯•å¯å‘å¼ç­–ç•¥
        print("\nğŸ“Š æµ‹è¯•å¯å‘å¼ç­–ç•¥...")
        heuristic_results = self._test_heuristic_policy()
        
        # æµ‹è¯•å¼ºåŒ–å­¦ä¹ ç­–ç•¥
        print("\nğŸ§  æµ‹è¯•å¼ºåŒ–å­¦ä¹ ç­–ç•¥...")
        rl_results = self._test_rl_policy()
        
        # ä¿å­˜ç»“æœ
        self.results = {
            'heuristic': heuristic_results,
            'rl': rl_results,
            'config': self.config
        }
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        self._generate_comparison_report()
        
        return self.results
    
    def _test_heuristic_policy(self):
        """æµ‹è¯•å¯å‘å¼ç­–ç•¥"""
        policy = HeuristicPolicy(battery_threshold=0.5, max_service_distance=8)
        return self._run_policy_test(policy, "Heuristic")
    
    def _test_rl_policy(self):
        """æµ‹è¯•å¼ºåŒ–å­¦ä¹ ç­–ç•¥"""
        # åˆ›å»ºç¯å¢ƒ
        env = ChargingIntegratedEnvironment(
            grid_size=self.config['grid_size'],
            num_vehicles=self.config['num_vehicles'],
            num_stations=self.config['num_stations'],
            use_intense_requests=self.config['use_intense_requests']
        )
        
        # åˆ›å»ºç®€å•çš„RLç­–ç•¥ä»£ç†
        rl_agent = SimpleRLAgent(env)
        
        # åŠ è½½è®­ç»ƒå¥½çš„ä»·å€¼å‡½æ•°
        if os.path.exists(self.config['value_function_path']):
            from src.ValueFunction_pytorch import PyTorchValueFunction
            value_function = PyTorchValueFunction(env)
            value_function.load_model(self.config['value_function_path'])
            env.set_value_function(value_function)
            print(f"âœ… å·²åŠ è½½ä»·å€¼å‡½æ•°: {self.config['value_function_path']}")
        else:
            print(f"âš ï¸  ä»·å€¼å‡½æ•°æ–‡ä»¶ä¸å­˜åœ¨: {self.config['value_function_path']}")
            print("ä½¿ç”¨éšæœºç­–ç•¥ä½œä¸ºRLåŸºå‡†")
        
        return self._run_rl_test(rl_agent, env)
    
    def _run_policy_test(self, policy, policy_name):
        """è¿è¡Œç­–ç•¥æµ‹è¯•"""
        episode_results = []
        
        for episode in range(self.config['num_test_episodes']):
            print(f"  Episode {episode + 1}/{self.config['num_test_episodes']}")
            
            # åˆ›å»ºç¯å¢ƒ
            env = ChargingIntegratedEnvironment(
                grid_size=self.config['grid_size'],
                num_vehicles=self.config['num_vehicles'],
                num_stations=self.config['num_stations'],
                use_intense_requests=self.config['use_intense_requests']
            )
            
            # è¿è¡Œå•ä¸ªepisode
            episode_result = self._run_single_episode_heuristic(env, policy)
            episode_result['episode'] = episode
            episode_result['policy'] = policy_name
            episode_results.append(episode_result)
        
        return episode_results
    
    def _run_rl_test(self, agent, env):
        """è¿è¡Œå¼ºåŒ–å­¦ä¹ æµ‹è¯•"""
        episode_results = []
        
        for episode in range(self.config['num_test_episodes']):
            print(f"  Episode {episode + 1}/{self.config['num_test_episodes']}")
            
            # é‡ç½®ç¯å¢ƒ
            env.reset()
            
            # è¿è¡Œå•ä¸ªepisode
            episode_result = self._run_single_episode_rl(env, agent)
            episode_result['episode'] = episode
            episode_result['policy'] = 'RL'
            episode_results.append(episode_result)
        
        return episode_results
    
    def _run_single_episode_heuristic(self, env, policy):
        """è¿è¡Œå•ä¸ªepisode - å¯å‘å¼ç­–ç•¥"""
        env.reset()
        
        total_reward = 0
        charging_events = 0
        
        step_rewards = []
        battery_levels = []
        utilization_rates = []
        
        # è·Ÿè¸ªè¯·æ±‚çŠ¶æ€
        initial_completed = len(env.completed_requests)
        initial_rejected = len(env.rejected_requests)
        
        for step in range(self.config['episode_length']):
            # è®°å½•çŠ¶æ€
            avg_battery = np.mean([v['battery'] for v in env.vehicles.values()])
            battery_levels.append(avg_battery)
            
            # è·å–å¯å‘å¼åŠ¨ä½œ
            actions = policy.get_actions(env)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_states, step_rewards_dict, done, info = env.step(actions)
            
            # è®¡ç®—æ€»æ­¥éª¤å¥–åŠ±
            step_reward = sum(step_rewards_dict.values()) if isinstance(step_rewards_dict, dict) else step_rewards_dict
            total_reward += step_reward
            step_rewards.append(step_reward)
            
            # ç»Ÿè®¡å……ç”µäº‹ä»¶
            charging_actions = sum(1 for action in actions.values() 
                                 if hasattr(action, 'charging_station_id') and action.charging_station_id is not None)
            charging_events += charging_actions
            
            # è®¡ç®—åˆ©ç”¨ç‡
            if env.charging_manager.stations:
                total_capacity = sum(station.max_capacity for station in env.charging_manager.stations.values())
                used_capacity = sum(len(station.current_vehicles) for station in env.charging_manager.stations.values())
                utilization = used_capacity / total_capacity if total_capacity > 0 else 0
            else:
                utilization = 0
            utilization_rates.append(utilization)
        
        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        total_served = len(env.completed_requests) - initial_completed
        total_rejected = len(env.rejected_requests) - initial_rejected
        
        return {
            'total_reward': total_reward,
            'total_served': total_served,
            'total_rejected': total_rejected,
            'total_charging_cost': 0,  # æš‚æ—¶è®¾ä¸º0ï¼Œç¨åå¯ä»¥æ·»åŠ è®¡ç®—
            'total_service_reward': 0,  # æš‚æ—¶è®¾ä¸º0ï¼Œç¨åå¯ä»¥æ·»åŠ è®¡ç®—
            'charging_events': charging_events,
            'avg_battery': np.mean(battery_levels),
            'min_battery': np.min(battery_levels),
            'avg_utilization': np.mean(utilization_rates),
            'service_rate': total_served / (total_served + total_rejected) if (total_served + total_rejected) > 0 else 0,
            'step_rewards': step_rewards,
            'battery_levels': battery_levels,
            'utilization_rates': utilization_rates
        }
    
    def _run_single_episode_rl(self, env, agent):
        """è¿è¡Œå•ä¸ªepisode - å¼ºåŒ–å­¦ä¹ ç­–ç•¥"""
        total_reward = 0
        charging_events = 0
        
        step_rewards = []
        battery_levels = []
        utilization_rates = []
        
        # è·Ÿè¸ªè¯·æ±‚çŠ¶æ€
        initial_completed = len(env.completed_requests)
        initial_rejected = len(env.rejected_requests)
        
        for step in range(self.config['episode_length']):
            # è®°å½•çŠ¶æ€
            avg_battery = np.mean([v['battery'] for v in env.vehicles.values()])
            battery_levels.append(avg_battery)
            
            # è·å–RLåŠ¨ä½œ
            actions = agent.get_actions(env)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_states, step_rewards_dict, done, info = env.step(actions)
            
            # è®¡ç®—æ€»æ­¥éª¤å¥–åŠ±
            step_reward = sum(step_rewards_dict.values()) if isinstance(step_rewards_dict, dict) else step_rewards_dict
            total_reward += step_reward
            step_rewards.append(step_reward)
            
            # ç»Ÿè®¡å……ç”µäº‹ä»¶
            charging_actions = sum(1 for action in actions.values() 
                                 if hasattr(action, 'charging_station_id') and action.charging_station_id is not None)
            charging_events += charging_actions
            
            # è®¡ç®—åˆ©ç”¨ç‡
            if env.charging_manager.stations:
                total_capacity = sum(station.max_capacity for station in env.charging_manager.stations.values())
                used_capacity = sum(len(station.current_vehicles) for station in env.charging_manager.stations.values())
                utilization = used_capacity / total_capacity if total_capacity > 0 else 0
            else:
                utilization = 0
            utilization_rates.append(utilization)
        
        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        total_served = len(env.completed_requests) - initial_completed
        total_rejected = len(env.rejected_requests) - initial_rejected
        
        return {
            'total_reward': total_reward,
            'total_served': total_served,
            'total_rejected': total_rejected,
            'total_charging_cost': 0,  # æš‚æ—¶è®¾ä¸º0ï¼Œç¨åå¯ä»¥æ·»åŠ è®¡ç®—
            'total_service_reward': 0,  # æš‚æ—¶è®¾ä¸º0ï¼Œç¨åå¯ä»¥æ·»åŠ è®¡ç®—
            'charging_events': charging_events,
            'avg_battery': np.mean(battery_levels),
            'min_battery': np.min(battery_levels),
            'avg_utilization': np.mean(utilization_rates),
            'service_rate': total_served / (total_served + total_rejected) if (total_served + total_rejected) > 0 else 0,
            'step_rewards': step_rewards,
            'battery_levels': battery_levels,
            'utilization_rates': utilization_rates
        }
    
    def _generate_comparison_report(self):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        print("\nğŸ“ˆ ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š...")
        
        # åˆ›å»ºDataFrame
        all_results = []
        for policy_name, results in [('Heuristic', self.results['heuristic']), 
                                   ('RL', self.results['rl'])]:
            for result in results:
                result_copy = result.copy()
                result_copy['policy'] = policy_name
                all_results.append(result_copy)
        
        df = pd.DataFrame(all_results)
        
        # è®¡ç®—ç»Ÿè®¡æ±‡æ€»
        summary_stats = df.groupby('policy').agg({
            'total_reward': ['mean', 'std'],
            'total_served': ['mean', 'std'],
            'total_rejected': ['mean', 'std'],
            'service_rate': ['mean', 'std'],
            'charging_events': ['mean', 'std'],
            'avg_battery': ['mean', 'std'],
            'avg_utilization': ['mean', 'std']
        }).round(3)
        
        # ä¿å­˜ç»Ÿè®¡ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        df.to_csv(f'results/heuristic_vs_rl_detailed_{timestamp}.csv', index=False)
        
        # ä¿å­˜æ±‡æ€»ç»Ÿè®¡
        with open(f'results/heuristic_vs_rl_summary_{timestamp}.txt', 'w', encoding='utf-8') as f:
            f.write("ç­–ç•¥å¯¹æ¯”æ±‡æ€»ç»Ÿè®¡\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"æµ‹è¯•é…ç½®:\n")
            f.write(f"- ç½‘æ ¼å¤§å°: {self.config['grid_size']}x{self.config['grid_size']}\n")
            f.write(f"- è½¦è¾†æ•°é‡: {self.config['num_vehicles']}\n")
            f.write(f"- å……ç”µç«™æ•°é‡: {self.config['num_stations']}\n")
            f.write(f"- å¯†é›†è¯·æ±‚æ¨¡å¼: {self.config['use_intense_requests']}\n")
            f.write(f"- æµ‹è¯•è½®æ¬¡: {self.config['num_test_episodes']}\n")
            f.write(f"- Episodeé•¿åº¦: {self.config['episode_length']}\n\n")
            
            f.write("ç»Ÿè®¡ç»“æœ:\n")
            f.write(str(summary_stats))
            f.write("\n\n")
            
            # æ·»åŠ æ€§èƒ½å¯¹æ¯”
            heuristic_reward = summary_stats.loc['Heuristic', ('total_reward', 'mean')]
            rl_reward = summary_stats.loc['RL', ('total_reward', 'mean')]
            improvement = ((rl_reward - heuristic_reward) / abs(heuristic_reward)) * 100
            
            f.write(f"æ€§èƒ½å¯¹æ¯”:\n")
            f.write(f"- å¯å‘å¼ç­–ç•¥å¹³å‡å¥–åŠ±: {heuristic_reward:.2f}\n")
            f.write(f"- å¼ºåŒ–å­¦ä¹ ç­–ç•¥å¹³å‡å¥–åŠ±: {rl_reward:.2f}\n")
            f.write(f"- å¼ºåŒ–å­¦ä¹ ç›¸å¯¹æ”¹è¿›: {improvement:.2f}%\n")
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self._generate_comparison_plots(df, timestamp)
        
        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜:")
        print(f"   è¯¦ç»†ç»“æœ: results/heuristic_vs_rl_detailed_{timestamp}.csv")
        print(f"   æ±‡æ€»ç»Ÿè®¡: results/heuristic_vs_rl_summary_{timestamp}.txt")
        print(f"   å¯è§†åŒ–å›¾è¡¨: results/policy_comparison_plots_{timestamp}.png")
        
        # æ‰“å°å…³é”®ç»“æœ
        print(f"\nğŸ¯ å…³é”®å¯¹æ¯”ç»“æœ:")
        print(f"å¯å‘å¼ç­–ç•¥ - å¹³å‡å¥–åŠ±: {heuristic_reward:.2f}")
        print(f"å¼ºåŒ–å­¦ä¹ ç­–ç•¥ - å¹³å‡å¥–åŠ±: {rl_reward:.2f}")
        print(f"å¼ºåŒ–å­¦ä¹ ç›¸å¯¹æ”¹è¿›: {improvement:.2f}%")
    
    def _generate_comparison_plots(self, df, timestamp):
        """ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–å›¾è¡¨"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('ç­–ç•¥æ€§èƒ½å¯¹æ¯”åˆ†æ', fontsize=16, fontweight='bold')
        
        # 1. æ€»å¥–åŠ±å¯¹æ¯”
        sns.boxplot(data=df, x='policy', y='total_reward', ax=axes[0, 0])
        axes[0, 0].set_title('æ€»å¥–åŠ±å¯¹æ¯”')
        axes[0, 0].set_ylabel('æ€»å¥–åŠ±')
        
        # 2. æœåŠ¡è¯·æ±‚æ•°å¯¹æ¯”
        sns.boxplot(data=df, x='policy', y='total_served', ax=axes[0, 1])
        axes[0, 1].set_title('æœåŠ¡è¯·æ±‚æ•°å¯¹æ¯”')
        axes[0, 1].set_ylabel('æœåŠ¡è¯·æ±‚æ•°')
        
        # 3. æœåŠ¡ç‡å¯¹æ¯”
        sns.boxplot(data=df, x='policy', y='service_rate', ax=axes[0, 2])
        axes[0, 2].set_title('æœåŠ¡ç‡å¯¹æ¯”')
        axes[0, 2].set_ylabel('æœåŠ¡ç‡')
        
        # 4. å……ç”µäº‹ä»¶å¯¹æ¯”
        sns.boxplot(data=df, x='policy', y='charging_events', ax=axes[1, 0])
        axes[1, 0].set_title('å……ç”µäº‹ä»¶æ•°å¯¹æ¯”')
        axes[1, 0].set_ylabel('å……ç”µäº‹ä»¶æ•°')
        
        # 5. å¹³å‡ç”µæ± æ°´å¹³å¯¹æ¯”
        sns.boxplot(data=df, x='policy', y='avg_battery', ax=axes[1, 1])
        axes[1, 1].set_title('å¹³å‡ç”µæ± æ°´å¹³å¯¹æ¯”')
        axes[1, 1].set_ylabel('å¹³å‡ç”µæ± æ°´å¹³')
        
        # 6. å……ç”µç«™åˆ©ç”¨ç‡å¯¹æ¯”
        sns.boxplot(data=df, x='policy', y='avg_utilization', ax=axes[1, 2])
        axes[1, 2].set_title('å……ç”µç«™å¹³å‡åˆ©ç”¨ç‡å¯¹æ¯”')
        axes[1, 2].set_ylabel('å¹³å‡åˆ©ç”¨ç‡')
        
        plt.tight_layout()
        plt.savefig(f'results/policy_comparison_plots_{timestamp}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # ç”Ÿæˆæ—¶é—´åºåˆ—å¯¹æ¯”å›¾
        self._generate_time_series_plots(timestamp)
    
    def _generate_time_series_plots(self, timestamp):
        """ç”Ÿæˆæ—¶é—´åºåˆ—å¯¹æ¯”å›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('ç­–ç•¥æ€§èƒ½æ—¶é—´åºåˆ—å¯¹æ¯”', fontsize=16, fontweight='bold')
        
        # è·å–ç¬¬ä¸€ä¸ªepisodeçš„æ—¶é—´åºåˆ—æ•°æ®ä½œä¸ºç¤ºä¾‹
        heuristic_sample = self.results['heuristic'][0]
        rl_sample = self.results['rl'][0]
        
        steps = range(len(heuristic_sample['step_rewards']))
        
        # 1. æ­¥éª¤å¥–åŠ±å¯¹æ¯”
        axes[0, 0].plot(steps, heuristic_sample['step_rewards'], 
                       label='Heuristic', alpha=0.7, linewidth=1)
        axes[0, 0].plot(steps, rl_sample['step_rewards'], 
                       label='RL', alpha=0.7, linewidth=1)
        axes[0, 0].set_title('æ­¥éª¤å¥–åŠ±å¯¹æ¯” (ç¤ºä¾‹Episode)')
        axes[0, 0].set_xlabel('æ—¶é—´æ­¥')
        axes[0, 0].set_ylabel('æ­¥éª¤å¥–åŠ±')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. ç”µæ± æ°´å¹³å¯¹æ¯”
        axes[0, 1].plot(steps, heuristic_sample['battery_levels'], 
                       label='Heuristic', alpha=0.7, linewidth=1)
        axes[0, 1].plot(steps, rl_sample['battery_levels'], 
                       label='RL', alpha=0.7, linewidth=1)
        axes[0, 1].set_title('å¹³å‡ç”µæ± æ°´å¹³å¯¹æ¯”')
        axes[0, 1].set_xlabel('æ—¶é—´æ­¥')
        axes[0, 1].set_ylabel('å¹³å‡ç”µæ± æ°´å¹³')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. å……ç”µç«™åˆ©ç”¨ç‡å¯¹æ¯”
        axes[1, 0].plot(steps, heuristic_sample['utilization_rates'], 
                       label='Heuristic', alpha=0.7, linewidth=1)
        axes[1, 0].plot(steps, rl_sample['utilization_rates'], 
                       label='RL', alpha=0.7, linewidth=1)
        axes[1, 0].set_title('å……ç”µç«™åˆ©ç”¨ç‡å¯¹æ¯”')
        axes[1, 0].set_xlabel('æ—¶é—´æ­¥')
        axes[1, 0].set_ylabel('åˆ©ç”¨ç‡')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. ç´¯ç§¯å¥–åŠ±å¯¹æ¯”
        heuristic_cumulative = np.cumsum(heuristic_sample['step_rewards'])
        rl_cumulative = np.cumsum(rl_sample['step_rewards'])
        
        axes[1, 1].plot(steps, heuristic_cumulative, 
                       label='Heuristic', alpha=0.7, linewidth=2)
        axes[1, 1].plot(steps, rl_cumulative, 
                       label='RL', alpha=0.7, linewidth=2)
        axes[1, 1].set_title('ç´¯ç§¯å¥–åŠ±å¯¹æ¯”')
        axes[1, 1].set_xlabel('æ—¶é—´æ­¥')
        axes[1, 1].set_ylabel('ç´¯ç§¯å¥–åŠ±')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'results/time_series_comparison_{timestamp}.png', dpi=300, bbox_inches='tight')
        plt.close()


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ å¯å‘å¼ç­–ç•¥ vs å¼ºåŒ–å­¦ä¹ ç­–ç•¥æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    
    # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
    os.makedirs('results', exist_ok=True)
    
    # åˆ›å»ºå¯¹æ¯”æµ‹è¯•å®ä¾‹
    comparison = PolicyComparison()
    
    # è¿è¡Œå¯¹æ¯”æµ‹è¯•
    start_time = time.time()
    results = comparison.run_comparison()
    end_time = time.time()
    
    print(f"\nâ±ï¸  æ€»æµ‹è¯•æ—¶é—´: {end_time - start_time:.2f} ç§’")
    print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()
