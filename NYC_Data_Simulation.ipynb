{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6144bafa",
   "metadata": {},
   "source": [
    "# NYCç”µåŠ¨å‡ºç§Ÿè½¦æ•°æ®ä¸‹è½½ä¸ä»¿çœŸç¯å¢ƒ\n",
    "\n",
    "æœ¬Notebookç”¨äºä¸‹è½½NYCå‡ºç§Ÿè½¦æ•°æ®ï¼Œå‚¨å­˜æ•°æ®ï¼Œå¹¶é€šè¿‡NYCEnvironmentç”Ÿæˆä»¿çœŸç¯å¢ƒé¢„è®¢å•ã€‚\n",
    "\n",
    "## ç›®æ ‡\n",
    "1. ğŸš• ä¸‹è½½çœŸå®çš„NYCå‡ºç§Ÿè½¦æ•°æ®\n",
    "2. ğŸ’¾ å­˜å‚¨å’Œé¢„å¤„ç†æ•°æ®\n",
    "3. ğŸ™ï¸ åˆå§‹åŒ–NYCä»¿çœŸç¯å¢ƒ\n",
    "4. ğŸ“‹ ç”Ÿæˆé¢„è®¢å•æ•°æ®\n",
    "5. ğŸ“Š å¯è§†åŒ–åˆ†æç»“æœ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552c4dff",
   "metadata": {},
   "source": [
    "## 1. å¯¼å…¥æ‰€éœ€åº“\n",
    "é¦–å…ˆå¯¼å…¥æ‰€æœ‰å¿…è¦çš„Pythonåº“ï¼ŒåŒ…æ‹¬æ•°æ®å¤„ç†ã€ç¯å¢ƒä»¿çœŸå’Œå¯è§†åŒ–ç›¸å…³çš„æ¨¡å—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bdb470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºç¡€åº“\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# æ·»åŠ é¡¹ç›®è·¯å¾„\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "# æ•°æ®ä¸‹è½½å’Œå¤„ç†\n",
    "try:\n",
    "    from data.download_data import download_data, describedata\n",
    "    print(\"âœ“ æ•°æ®ä¸‹è½½æ¨¡å—å¯¼å…¥æˆåŠŸ\")\n",
    "    DATA_DOWNLOAD_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ æ•°æ®ä¸‹è½½æ¨¡å—å¯¼å…¥å¤±è´¥: {e}\")\n",
    "    DATA_DOWNLOAD_AVAILABLE = False\n",
    "\n",
    "# NYCç¯å¢ƒå’Œè¯·æ±‚ç±»\n",
    "try:\n",
    "    from src.NYEEnvironment import NYEEnvironment\n",
    "    print(\"âœ“ NYCç¯å¢ƒæ¨¡å—å¯¼å…¥æˆåŠŸ\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ NYCç¯å¢ƒæ¨¡å—å¯¼å…¥å¤±è´¥: {e}\")\n",
    "\n",
    "try:\n",
    "    from src.NYCRequest import NYCRequest, NYCRequestGenerator\n",
    "    print(\"âœ“ NYCè¯·æ±‚æ¨¡å—å¯¼å…¥æˆåŠŸ\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ NYCè¯·æ±‚æ¨¡å—å¯¼å…¥å¤±è´¥: {e}\")\n",
    "\n",
    "# å¯è§†åŒ–ç›¸å…³\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    print(\"âœ“ Plotlyå¯è§†åŒ–åº“å¯¼å…¥æˆåŠŸ\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ Plotlyæœªå®‰è£…ï¼Œå°†ä½¿ç”¨matplotlib\")\n",
    "\n",
    "# åœ°ç†ç›¸å…³\n",
    "try:\n",
    "    from geopy.distance import geodesic\n",
    "    import folium\n",
    "    print(\"âœ“ åœ°ç†å¤„ç†åº“å¯¼å…¥æˆåŠŸ\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ åœ°ç†å¤„ç†åº“å¯¼å…¥å¤±è´¥\")\n",
    "\n",
    "print(f\"\\nğŸ“… å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"ğŸ Pythonç‰ˆæœ¬: {sys.version}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a99a740",
   "metadata": {},
   "source": [
    "## 2. è®¾ç½®æ•°æ®ä¸‹è½½å‚æ•°\n",
    "é…ç½®NYCå‡ºç§Ÿè½¦æ•°æ®çš„ä¸‹è½½å‚æ•°ï¼ŒåŒ…æ‹¬æ—¶é—´èŒƒå›´ã€æ•°æ®ç±»å‹ç­‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53387c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®ä¸‹è½½é…ç½®å‚æ•°\n",
    "DATA_CONFIG = {\n",
    "    # æ•°æ®æ—¶é—´èŒƒå›´\n",
    "    'start_date': '2024-01',  # YYYY-MM æ ¼å¼\n",
    "    'end_date': '2024-02',\n",
    "    \n",
    "    # æ•°æ®ç±»å‹ (Yellow Taxi)\n",
    "    'data_type': 'yellow_taxi',\n",
    "    \n",
    "    # é‡‡æ ·å‚æ•°\n",
    "    'sample_size': 10000,  # é‡‡æ ·æ•°æ®é‡\n",
    "    'random_seed': 42,\n",
    "    \n",
    "    # å­˜å‚¨è·¯å¾„\n",
    "    'data_dir': 'data/nyc_cache',\n",
    "    'processed_dir': 'data/processed',\n",
    "    \n",
    "    # åœ°ç†èŒƒå›´ (Manhattan)\n",
    "    'geo_bounds': {\n",
    "        'min_lat': 40.7000,\n",
    "        'max_lat': 40.8800,\n",
    "        'min_lon': -74.0200,\n",
    "        'max_lon': -73.9300\n",
    "    }\n",
    "}\n",
    "\n",
    "# ä»¿çœŸç¯å¢ƒé…ç½®\n",
    "SIMULATION_CONFIG = {\n",
    "    # ä»¿çœŸæ—¶é—´è®¾ç½®\n",
    "    'start_time': datetime(2024, 10, 20, 8, 0),  # æ—©ä¸Š8ç‚¹å¼€å§‹\n",
    "    'duration_hours': 12,  # ä»¿çœŸ12å°æ—¶\n",
    "    'time_step_minutes': 1,  # 1åˆ†é’Ÿæ­¥é•¿\n",
    "    \n",
    "    # è½¦é˜Ÿé…ç½®\n",
    "    'num_vehicles': 50,\n",
    "    'initial_battery': 0.8,  # åˆå§‹ç”µé‡80%\n",
    "    \n",
    "    # éœ€æ±‚é…ç½®\n",
    "    'requests_per_hour': 100,  # æ¯å°æ—¶100ä¸ªè¯·æ±‚\n",
    "    'use_real_data': True,  # æ˜¯å¦ä½¿ç”¨çœŸå®æ•°æ®\n",
    "}\n",
    "\n",
    "# åˆ›å»ºå¿…è¦çš„ç›®å½•\n",
    "os.makedirs(DATA_CONFIG['data_dir'], exist_ok=True)\n",
    "os.makedirs(DATA_CONFIG['processed_dir'], exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“‹ æ•°æ®ä¸‹è½½é…ç½®:\")\n",
    "for key, value in DATA_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\nğŸš— ä»¿çœŸç¯å¢ƒé…ç½®:\")\n",
    "for key, value in SIMULATION_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(f\"\\nğŸ“ æ•°æ®å­˜å‚¨è·¯å¾„: {os.path.abspath(DATA_CONFIG['data_dir'])}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b1e3ff",
   "metadata": {},
   "source": [
    "## 3. ä¸‹è½½NYCç¯å¢ƒæ•°æ®\n",
    "ä½¿ç”¨download_dataæ¨¡å—ä¸‹è½½çœŸå®çš„NYC Yellow Taxiæ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c0df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è½½NYCå‡ºç§Ÿè½¦æ•°æ®\n",
    "print(\"ğŸš• å¼€å§‹ä¸‹è½½NYC Yellow Taxiæ•°æ®...\")\n",
    "downloaded_files = []\n",
    "\n",
    "def download_nyc_data_wrapper(year_month_str):\n",
    "    \"\"\"\n",
    "    åŒ…è£…å‡½æ•°ï¼Œå°†'YYYY-MM'æ ¼å¼è½¬æ¢ä¸ºå¹´å’Œæœˆ\n",
    "    \"\"\"\n",
    "    year, month = year_month_str.split('-')\n",
    "    return download_data(int(year), int(month), 'nyc')\n",
    "\n",
    "if DATA_DOWNLOAD_AVAILABLE:\n",
    "    try:\n",
    "        # ä¸‹è½½æŒ‡å®šæ—¶é—´æ®µçš„æ•°æ®\n",
    "        for month_str in ['2025-01', '2025-02']:\n",
    "            print(f\"\\nğŸ“… ä¸‹è½½ {month_str} æ•°æ®...\")\n",
    "            \n",
    "            try:\n",
    "                year, month = month_str.split('-')\n",
    "                file_path = download_data(int(year), int(month), 'nyc')\n",
    "                \n",
    "                # æ„é€ é¢„æœŸçš„æ–‡ä»¶è·¯å¾„\n",
    "                expected_file = f\"data/parquet/yellow_tripdata_{month_str}.parquet\"\n",
    "                \n",
    "                if os.path.exists(expected_file):\n",
    "                    downloaded_files.append(expected_file)\n",
    "                    file_size = os.path.getsize(expected_file) / (1024 * 1024)  # MB\n",
    "                    print(f\"   âœ“ æˆåŠŸä¸‹è½½: {expected_file}\")\n",
    "                    print(f\"   ğŸ“¦ æ–‡ä»¶å¤§å°: {file_size:.1f} MB\")\n",
    "                else:\n",
    "                    print(f\"   âŒ ä¸‹è½½å¤±è´¥: {month_str}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ ä¸‹è½½ {month_str} æ—¶å‡ºé”™: {e}\")\n",
    "                \n",
    "        print(f\"\\nğŸ“Š æ€»å…±ä¸‹è½½äº† {len(downloaded_files)} ä¸ªæ–‡ä»¶\")\n",
    "        \n",
    "        # å¦‚æœæœ‰ä¸‹è½½æˆåŠŸçš„æ–‡ä»¶ï¼Œæ˜¾ç¤ºæ•°æ®æ¦‚è¿°\n",
    "        if downloaded_files:\n",
    "            print(\"\\nğŸ“‹ æ•°æ®æ¦‚è¿°:\")\n",
    "            try:\n",
    "                # è¯»å–ç¬¬ä¸€ä¸ªæ–‡ä»¶æŸ¥çœ‹æ•°æ®ç»“æ„\n",
    "                sample_file = downloaded_files[0]\n",
    "                # ä¿®å¤ï¼šä½¿ç”¨head()æ–¹æ³•è€Œä¸æ˜¯nrowså‚æ•°\n",
    "                df_sample = pd.read_parquet(sample_file)\n",
    "                df_sample = df_sample.head(1000)  # åªå–å‰1000è¡Œ\n",
    "                \n",
    "                print(f\"   ğŸ”¢ æ ·æœ¬æ•°æ®å½¢çŠ¶: {df_sample.shape}\")\n",
    "                print(f\"   ğŸ“Š åˆ—å: {list(df_sample.columns)}\")\n",
    "                if 'tpep_pickup_datetime' in df_sample.columns:\n",
    "                    print(f\"   ğŸ“… æ—¶é—´èŒƒå›´: {df_sample['tpep_pickup_datetime'].min()} åˆ° {df_sample['tpep_pickup_datetime'].max()}\")\n",
    "                \n",
    "                # ä½¿ç”¨describedataå‡½æ•°è·å–æ›´å¤šä¿¡æ¯\n",
    "                try:\n",
    "                    year, month = '2025-01'.split('-')\n",
    "                    print(f\"\\nğŸ“Š ä½¿ç”¨describedataåˆ†ææ•°æ®...\")\n",
    "                    describedata(int(year), int(month), 'nyc')\n",
    "                except Exception as desc_error:\n",
    "                    print(f\"   âš ï¸ describedataè°ƒç”¨å¤±è´¥: {desc_error}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸ è¯»å–æ•°æ®æ¦‚è¿°æ—¶å‡ºé”™: {e}\")\n",
    "        else:\n",
    "            print(\"âŒ æ²¡æœ‰æˆåŠŸä¸‹è½½ä»»ä½•æ–‡ä»¶\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ•°æ®ä¸‹è½½è¿‡ç¨‹å‡ºé”™: {e}\")\n",
    "        print(\"å°†ç»§ç»­ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œæ¼”ç¤º\")\n",
    "else:\n",
    "    print(\"âš ï¸ æ•°æ®ä¸‹è½½æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡çœŸå®æ•°æ®ä¸‹è½½\")\n",
    "    print(\"å°†ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œæ¼”ç¤º\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276884dc",
   "metadata": {},
   "source": [
    "## 4. æ•°æ®é¢„å¤„ç†å’Œæ¸…ç†\n",
    "å¯¹ä¸‹è½½çš„åŸå§‹æ•°æ®è¿›è¡Œæ¸…ç†ã€æ ¼å¼åŒ–å’Œé¢„å¤„ç†ï¼Œç¡®ä¿æ•°æ®è´¨é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0740193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥å®é™…æ•°æ®åˆ—ç»“æ„\n",
    "print(\"ğŸ” æ£€æŸ¥å®é™…ä¸‹è½½æ•°æ®çš„åˆ—ç»“æ„...\")\n",
    "\n",
    "if downloaded_files:\n",
    "    try:\n",
    "        # è¯»å–ç¬¬ä¸€ä¸ªæ–‡ä»¶æŸ¥çœ‹åˆ—å\n",
    "        sample_file = downloaded_files[0]\n",
    "        df_sample = pd.read_parquet(sample_file).head(1000)\n",
    "        \n",
    "        print(f\"ğŸ“Š æ–‡ä»¶: {os.path.basename(sample_file)}\")\n",
    "        print(f\"ğŸ“‹ æ•°æ®å½¢çŠ¶: {df_sample.shape}\")\n",
    "        print(f\"ğŸ“ æ‰€æœ‰åˆ—å:\")\n",
    "        for i, col in enumerate(df_sample.columns):\n",
    "            print(f\"   {i+1:2d}. {col}\")\n",
    "        \n",
    "        # æŸ¥æ‰¾å¯èƒ½çš„åæ ‡åˆ—\n",
    "        coordinate_cols = [col for col in df_sample.columns if \n",
    "                          any(keyword in col.lower() for keyword in \n",
    "                              ['lat', 'lon', 'pickup', 'dropoff', 'location'])]\n",
    "        \n",
    "        print(f\"\\nğŸ—ºï¸ å¯èƒ½çš„åæ ‡ç›¸å…³åˆ—:\")\n",
    "        for col in coordinate_cols:\n",
    "            print(f\"   â€¢ {col}\")\n",
    "            if not df_sample[col].isnull().all():\n",
    "                print(f\"     æ ·æœ¬å€¼: {df_sample[col].dropna().iloc[0]}\")\n",
    "        \n",
    "        # æ˜¾ç¤ºæ•°æ®æ ·æœ¬\n",
    "        print(f\"\\nğŸ‘€ æ•°æ®æ ·æœ¬ (å‰3è¡Œ):\")\n",
    "        print(df_sample.head(3))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ£€æŸ¥æ•°æ®ç»“æ„æ—¶å‡ºé”™: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"âš ï¸ æ²¡æœ‰ä¸‹è½½çš„æ–‡ä»¶å¯ä¾›æ£€æŸ¥\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a0dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®é¢„å¤„ç†å‡½æ•° (æ›´æ–°ç‰ˆ - é€‚é…2025å¹´æ•°æ®æ ¼å¼)\n",
    "def clean_nyc_data(df, geo_bounds=None):\n",
    "    \"\"\"\n",
    "    æ¸…ç†NYCå‡ºç§Ÿè½¦æ•°æ® (2025å¹´æ–°æ ¼å¼)\n",
    "    \n",
    "    Args:\n",
    "        df: åŸå§‹æ•°æ®DataFrame\n",
    "        geo_bounds: åœ°ç†è¾¹ç•Œå­—å…¸ (å¯é€‰ï¼Œå› ä¸ºæ–°æ ¼å¼ä½¿ç”¨LocationID)\n",
    "    \n",
    "    Returns:\n",
    "        æ¸…ç†åçš„DataFrame\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“Š åŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}\")\n",
    "    \n",
    "    # 1. åˆ é™¤å…³é”®å­—æ®µçš„ç©ºå€¼\n",
    "    initial_shape = df.shape[0]\n",
    "    \n",
    "    # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨\n",
    "    required_cols = ['PULocationID', 'DOLocationID', 'trip_distance', 'fare_amount']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"âŒ ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}\")\n",
    "        return df\n",
    "    \n",
    "    # åˆ é™¤LocationIDä¸ºç©ºçš„è¡Œ\n",
    "    df = df.dropna(subset=['PULocationID', 'DOLocationID'])\n",
    "    print(f\"ğŸ§¹ åˆ é™¤LocationIDç©ºå€¼: {initial_shape - df.shape[0]} è¡Œ\")\n",
    "    \n",
    "    # 2. è¿‡æ»¤æ— æ•ˆçš„LocationID\n",
    "    # NYC LocationIDé€šå¸¸åœ¨1-263èŒƒå›´å†…\n",
    "    location_filter = (\n",
    "        (df['PULocationID'] >= 1) & (df['PULocationID'] <= 263) &\n",
    "        (df['DOLocationID'] >= 1) & (df['DOLocationID'] <= 263)\n",
    "    )\n",
    "    \n",
    "    before_location = df.shape[0]\n",
    "    df = df[location_filter]\n",
    "    print(f\"ğŸ—ºï¸ LocationIDèŒƒå›´è¿‡æ»¤: ä¿ç•™ {df.shape[0]} / {before_location} è¡Œ\")\n",
    "    \n",
    "    # 3. æ—¶é—´æ•°æ®å¤„ç†\n",
    "    if 'tpep_pickup_datetime' in df.columns:\n",
    "        df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "        df['pickup_hour'] = df['tpep_pickup_datetime'].dt.hour\n",
    "        df['pickup_weekday'] = df['tpep_pickup_datetime'].dt.weekday\n",
    "        \n",
    "        # æ·»åŠ è¡Œç¨‹æ—¶é•¿è®¡ç®—\n",
    "        if 'tpep_dropoff_datetime' in df.columns:\n",
    "            df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
    "            df['trip_duration'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "            \n",
    "            # è¿‡æ»¤å¼‚å¸¸è¡Œç¨‹æ—¶é•¿ (1åˆ†é’Ÿåˆ°4å°æ—¶)\n",
    "            duration_filter = (df['trip_duration'] >= 1) & (df['trip_duration'] <= 240)\n",
    "            before_duration = df.shape[0]\n",
    "            df = df[duration_filter]\n",
    "            print(f\"â±ï¸ è¡Œç¨‹æ—¶é•¿è¿‡æ»¤: ä¿ç•™ {df.shape[0]} / {before_duration} è¡Œ\")\n",
    "    \n",
    "    # 4. è¡Œç¨‹è·ç¦»è¿‡æ»¤\n",
    "    if 'trip_distance' in df.columns:\n",
    "        # è¿‡æ»¤å¼‚å¸¸è·ç¦» (0.1km - 50km)\n",
    "        distance_filter = (df['trip_distance'] >= 0.1) & (df['trip_distance'] <= 50)\n",
    "        before_distance = df.shape[0]\n",
    "        df = df[distance_filter]\n",
    "        print(f\"ğŸ“ è·ç¦»è¿‡æ»¤: ä¿ç•™ {df.shape[0]} / {before_distance} è¡Œ\")\n",
    "    \n",
    "    # 5. è´¹ç”¨è¿‡æ»¤\n",
    "    if 'fare_amount' in df.columns:\n",
    "        # è¿‡æ»¤å¼‚å¸¸è´¹ç”¨ ($1 - $200)\n",
    "        fare_filter = (df['fare_amount'] >= 1) & (df['fare_amount'] <= 200)\n",
    "        before_fare = df.shape[0]\n",
    "        df = df[fare_filter]\n",
    "        print(f\"ğŸ’° è´¹ç”¨è¿‡æ»¤: ä¿ç•™ {df.shape[0]} / {before_fare} è¡Œ\")\n",
    "    \n",
    "    # 6. ä¹˜å®¢æ•°é‡è¿‡æ»¤\n",
    "    if 'passenger_count' in df.columns:\n",
    "        # è¿‡æ»¤å¼‚å¸¸ä¹˜å®¢æ•° (1-6äºº)\n",
    "        passenger_filter = (df['passenger_count'] >= 1) & (df['passenger_count'] <= 6)\n",
    "        before_passenger = df.shape[0]\n",
    "        df = df[passenger_filter]\n",
    "        print(f\"ğŸ‘¥ ä¹˜å®¢æ•°è¿‡æ»¤: ä¿ç•™ {df.shape[0]} / {before_passenger} è¡Œ\")\n",
    "    \n",
    "    # 7. æ·»åŠ ç®€åŒ–çš„åæ ‡ä¿¡æ¯ (ä½¿ç”¨LocationIDçš„è¿‘ä¼¼åæ ‡)\n",
    "    # è¿™é‡Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€å•çš„LocationIDåˆ°åæ ‡çš„æ˜ å°„\n",
    "    df = add_approximate_coordinates(df)\n",
    "    \n",
    "    print(f\"âœ… æ¸…ç†å®Œæˆï¼Œæœ€ç»ˆæ•°æ®å½¢çŠ¶: {df.shape}\")\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def add_approximate_coordinates(df):\n",
    "    \"\"\"\n",
    "    ä¸ºLocationIDæ·»åŠ è¿‘ä¼¼åæ ‡\n",
    "    è¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€åŒ–çš„æ˜ å°„ï¼Œå®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨å®˜æ–¹çš„zone lookupè¡¨\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„LocationIDåˆ°åæ ‡çš„æ˜ å°„\n",
    "        # è¿™é‡Œä½¿ç”¨æ›¼å“ˆé¡¿åŒºåŸŸçš„ç½‘æ ¼è¿‘ä¼¼\n",
    "        manhattan_bounds = [(40.7000, -74.0200), (40.8800, -73.9300)]\n",
    "        \n",
    "        # ä¸ºå¸¸è§çš„LocationIDåˆ›å»ºè¿‘ä¼¼åæ ‡\n",
    "        def location_to_coords(location_id):\n",
    "            # ç®€åŒ–ç®—æ³•ï¼šå°†LocationIDæ˜ å°„åˆ°æ›¼å“ˆé¡¿ç½‘æ ¼\n",
    "            if pd.isna(location_id) or location_id < 1 or location_id > 263:\n",
    "                return None, None\n",
    "            \n",
    "            # å°†IDæ˜ å°„åˆ°æ›¼å“ˆé¡¿åŒºåŸŸ\n",
    "            lat_range = manhattan_bounds[1][0] - manhattan_bounds[0][0]  # çº¬åº¦èŒƒå›´\n",
    "            lon_range = manhattan_bounds[1][1] - manhattan_bounds[0][1]  # ç»åº¦èŒƒå›´\n",
    "            \n",
    "            # ä½¿ç”¨ç®€å•çš„ç½‘æ ¼æ˜ å°„\n",
    "            grid_size = 16  # å‡è®¾16x16ç½‘æ ¼\n",
    "            row = int((location_id - 1) // grid_size) % grid_size\n",
    "            col = int((location_id - 1) % grid_size)\n",
    "            \n",
    "            lat = manhattan_bounds[0][0] + (row / grid_size) * lat_range\n",
    "            lon = manhattan_bounds[0][1] + (col / grid_size) * lon_range\n",
    "            \n",
    "            # æ·»åŠ ä¸€äº›éšæœºæ€§æ¨¡æ‹Ÿå®é™…ä½ç½®å·®å¼‚\n",
    "            lat += np.random.normal(0, 0.001)  # çº¦100ç±³çš„æ ‡å‡†å·®\n",
    "            lon += np.random.normal(0, 0.001)\n",
    "            \n",
    "            return lat, lon\n",
    "        \n",
    "        # åº”ç”¨åæ ‡æ˜ å°„\n",
    "        pickup_coords = df['PULocationID'].apply(location_to_coords)\n",
    "        dropoff_coords = df['DOLocationID'].apply(location_to_coords)\n",
    "        \n",
    "        # åˆ†ç¦»çº¬åº¦å’Œç»åº¦\n",
    "        df['pickup_latitude'] = [coord[0] for coord in pickup_coords]\n",
    "        df['pickup_longitude'] = [coord[1] for coord in pickup_coords]\n",
    "        df['dropoff_latitude'] = [coord[0] for coord in dropoff_coords]\n",
    "        df['dropoff_longitude'] = [coord[1] for coord in dropoff_coords]\n",
    "        \n",
    "        # åˆ é™¤åæ ‡ä¸ºNoneçš„è¡Œ\n",
    "        coord_filter = (\n",
    "            df['pickup_latitude'].notna() & df['pickup_longitude'].notna() &\n",
    "            df['dropoff_latitude'].notna() & df['dropoff_longitude'].notna()\n",
    "        )\n",
    "        df = df[coord_filter]\n",
    "        \n",
    "        print(f\"ğŸ—ºï¸ æ·»åŠ è¿‘ä¼¼åæ ‡: {df.shape[0]} æ¡è®°å½•è·å¾—åæ ‡\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ åæ ‡æ˜ å°„å¤±è´¥: {e}\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "# å¤„ç†ä¸‹è½½çš„æ•°æ® (æ›´æ–°ç‰ˆ)\n",
    "processed_data = []\n",
    "\n",
    "if downloaded_files:\n",
    "    print(\"ğŸ”§ å¼€å§‹æ•°æ®é¢„å¤„ç† (2025å¹´æ–°æ ¼å¼)...\")\n",
    "    \n",
    "    for file_path in downloaded_files:\n",
    "        try:\n",
    "            print(f\"\\nğŸ“‚ å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}\")\n",
    "            \n",
    "            # è¯»å–æ•°æ®\n",
    "            df_raw = pd.read_parquet(file_path)\n",
    "            \n",
    "            # æ¸…ç†æ•°æ® (ä½¿ç”¨æ–°çš„æ¸…ç†å‡½æ•°)\n",
    "            df_clean = clean_nyc_data(df_raw, DATA_CONFIG.get('geo_bounds'))\n",
    "            \n",
    "            # é‡‡æ ·æ•°æ® (å¦‚æœæ•°æ®é‡å¤ªå¤§)\n",
    "            if df_clean.shape[0] > DATA_CONFIG['sample_size']:\n",
    "                df_sample = df_clean.sample(n=DATA_CONFIG['sample_size'], \n",
    "                                          random_state=DATA_CONFIG['random_seed'])\n",
    "                print(f\"ğŸ² éšæœºé‡‡æ ·: {DATA_CONFIG['sample_size']} è¡Œ\")\n",
    "            else:\n",
    "                df_sample = df_clean\n",
    "            \n",
    "            processed_data.append(df_sample)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # åˆå¹¶æ‰€æœ‰å¤„ç†åçš„æ•°æ®\n",
    "    if processed_data:\n",
    "        df_final = pd.concat(processed_data, ignore_index=True)\n",
    "        print(f\"\\nğŸ“Š åˆå¹¶åçš„æœ€ç»ˆæ•°æ®å½¢çŠ¶: {df_final.shape}\")\n",
    "        \n",
    "        # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡ä¿¡æ¯\n",
    "        print(\"\\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:\")\n",
    "        if 'trip_distance' in df_final.columns:\n",
    "            print(f\"   å¹³å‡è¡Œç¨‹è·ç¦»: {df_final['trip_distance'].mean():.2f} km\")\n",
    "        if 'fare_amount' in df_final.columns:\n",
    "            print(f\"   å¹³å‡è´¹ç”¨: ${df_final['fare_amount'].mean():.2f}\")\n",
    "        if 'passenger_count' in df_final.columns:\n",
    "            print(f\"   å¹³å‡ä¹˜å®¢æ•°: {df_final['passenger_count'].mean():.1f}\")\n",
    "        if 'trip_duration' in df_final.columns:\n",
    "            print(f\"   å¹³å‡è¡Œç¨‹æ—¶é•¿: {df_final['trip_duration'].mean():.1f} åˆ†é’Ÿ\")\n",
    "        \n",
    "        # æ˜¾ç¤ºLocationIDåˆ†å¸ƒ\n",
    "        print(f\"\\nğŸ—ºï¸ LocationIDåˆ†å¸ƒ:\")\n",
    "        print(f\"   ä¸Šè½¦åœ°ç‚¹èŒƒå›´: {df_final['PULocationID'].min():.0f} - {df_final['PULocationID'].max():.0f}\")\n",
    "        print(f\"   ä¸‹è½¦åœ°ç‚¹èŒƒå›´: {df_final['DOLocationID'].min():.0f} - {df_final['DOLocationID'].max():.0f}\")\n",
    "        print(f\"   çƒ­é—¨ä¸Šè½¦åœ°ç‚¹: {df_final['PULocationID'].mode().iloc[0]:.0f} (ID)\")\n",
    "        \n",
    "    else:\n",
    "        df_final = pd.DataFrame()\n",
    "        print(\"âŒ æ²¡æœ‰æˆåŠŸå¤„ç†çš„æ•°æ®\")\n",
    "else:\n",
    "    print(\"âš ï¸ æ²¡æœ‰ä¸‹è½½æ•°æ®ï¼Œå°†ä½¿ç”¨åˆæˆæ•°æ®\")\n",
    "    df_final = pd.DataFrame()\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52db2d4f",
   "metadata": {},
   "source": [
    "## 5. æ•°æ®å­˜å‚¨åˆ°æœ¬åœ°æ–‡ä»¶\n",
    "å°†å¤„ç†åçš„æ•°æ®ä¿å­˜åˆ°æœ¬åœ°ï¼Œä¾¿äºåç»­ä½¿ç”¨å’Œåˆ†æã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d5b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜å¤„ç†åçš„æ•°æ®\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "if not df_final.empty:\n",
    "    # ä¿å­˜ä¸ºå¤šç§æ ¼å¼\n",
    "    save_paths = {}\n",
    "    \n",
    "    try:\n",
    "        # 1. CSVæ ¼å¼ (ä¾¿äºæŸ¥çœ‹å’Œåˆ†æ)\n",
    "        csv_path = os.path.join(DATA_CONFIG['processed_dir'], f'nyc_taxi_processed_{timestamp}.csv')\n",
    "        df_final.to_csv(csv_path, index=False)\n",
    "        save_paths['csv'] = csv_path\n",
    "        print(f\"ğŸ’¾ CSVæ–‡ä»¶å·²ä¿å­˜: {csv_path}\")\n",
    "        \n",
    "        # 2. Parquetæ ¼å¼ (é«˜æ•ˆå­˜å‚¨)\n",
    "        parquet_path = os.path.join(DATA_CONFIG['processed_dir'], f'nyc_taxi_processed_{timestamp}.parquet')\n",
    "        df_final.to_parquet(parquet_path, index=False)\n",
    "        save_paths['parquet'] = parquet_path\n",
    "        print(f\"ğŸ’¾ Parquetæ–‡ä»¶å·²ä¿å­˜: {parquet_path}\")\n",
    "        \n",
    "        # 3. æ•°æ®æ‘˜è¦æŠ¥å‘Š\n",
    "        summary_path = os.path.join(DATA_CONFIG['processed_dir'], f'data_summary_{timestamp}.txt')\n",
    "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"NYC Taxi Data Processing Summary\\n\")\n",
    "            f.write(\"=\" * 40 + \"\\n\")\n",
    "            f.write(f\"Processing Time: {datetime.now()}\\n\")\n",
    "            f.write(f\"Data Shape: {df_final.shape}\\n\")\n",
    "            f.write(f\"Columns: {list(df_final.columns)}\\n\\n\")\n",
    "            \n",
    "            # æ•°æ®ç»Ÿè®¡\n",
    "            f.write(\"Data Statistics:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            if 'trip_distance' in df_final.columns:\n",
    "                f.write(f\"Trip Distance - Mean: {df_final['trip_distance'].mean():.2f}km, \"\n",
    "                       f\"Std: {df_final['trip_distance'].std():.2f}km\\n\")\n",
    "            if 'fare_amount' in df_final.columns:\n",
    "                f.write(f\"Fare Amount - Mean: ${df_final['fare_amount'].mean():.2f}, \"\n",
    "                       f\"Std: ${df_final['fare_amount'].std():.2f}\\n\")\n",
    "            \n",
    "            # æ—¶é—´åˆ†å¸ƒ\n",
    "            if 'pickup_hour' in df_final.columns:\n",
    "                f.write(\"\\nHourly Distribution:\\n\")\n",
    "                hourly_dist = df_final['pickup_hour'].value_counts().sort_index()\n",
    "                for hour, count in hourly_dist.items():\n",
    "                    f.write(f\"  Hour {hour:2d}: {count:4d} trips\\n\")\n",
    "        \n",
    "        save_paths['summary'] = summary_path\n",
    "        print(f\"ğŸ“‹ æ•°æ®æ‘˜è¦å·²ä¿å­˜: {summary_path}\")\n",
    "        \n",
    "        # æ˜¾ç¤ºæ–‡ä»¶å¤§å°\n",
    "        for format_name, path in save_paths.items():\n",
    "            size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "            print(f\"   {format_name.upper()}: {size_mb:.1f} MB\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ä¿å­˜æ•°æ®æ—¶å‡ºé”™: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜\")\n",
    "\n",
    "# æ˜¾ç¤ºå·²ä¿å­˜çš„æ–‡ä»¶\n",
    "processed_files = [f for f in os.listdir(DATA_CONFIG['processed_dir']) \n",
    "                  if f.endswith(('.csv', '.parquet', '.txt'))]\n",
    "\n",
    "print(f\"\\nğŸ“ å¤„ç†åçš„æ•°æ®ç›®å½• ({DATA_CONFIG['processed_dir']}):\")\n",
    "for file in sorted(processed_files)[-5:]:  # æ˜¾ç¤ºæœ€æ–°çš„5ä¸ªæ–‡ä»¶\n",
    "    file_path = os.path.join(DATA_CONFIG['processed_dir'], file)\n",
    "    size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "    mod_time = datetime.fromtimestamp(os.path.getmtime(file_path))\n",
    "    print(f\"   ğŸ“„ {file} ({size_mb:.1f}MB, {mod_time.strftime('%Y-%m-%d %H:%M')})\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6da9a37",
   "metadata": {},
   "source": [
    "## 6. åˆå§‹åŒ–NYCä»¿çœŸç¯å¢ƒ\n",
    "ä½¿ç”¨NYCEnvironmentç±»åˆå§‹åŒ–ä»¿çœŸç¯å¢ƒï¼ŒåŠ è½½åœ°å›¾å’ŒåŸºç¡€è®¾æ–½æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2132dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–NYCä»¿çœŸç¯å¢ƒ\n",
    "print(\"ğŸ™ï¸ åˆå§‹åŒ–NYCç”µåŠ¨å‡ºç§Ÿè½¦ä»¿çœŸç¯å¢ƒ...\")\n",
    "\n",
    "try:\n",
    "    # åˆ›å»ºNYCç¯å¢ƒå®ä¾‹\n",
    "    nyc_env = NYEEnvironment()\n",
    "    print(\"âœ“ NYCç¯å¢ƒåˆ›å»ºæˆåŠŸ\")\n",
    "    \n",
    "    # æ˜¾ç¤ºç¯å¢ƒåŸºæœ¬ä¿¡æ¯\n",
    "    print(f\"\\nğŸ—ï¸ ç¯å¢ƒé…ç½®ä¿¡æ¯:\")\n",
    "    print(f\"   ğŸ—ºï¸ åœ°ç†èŒƒå›´: æ›¼å“ˆé¡¿ ({nyc_env.manhattan_bounds})\")\n",
    "    print(f\"   ğŸ”‹ å……ç”µç«™æ•°é‡: {len(nyc_env.charging_stations)}\")\n",
    "    print(f\"   ğŸš— è½¦è¾†æ•°é‡: {len(nyc_env.vehicles)}\")\n",
    "    # ä¿®å¤ï¼šä½¿ç”¨current_timeè€Œä¸æ˜¯current_time_minutes\n",
    "    print(f\"   â° å½“å‰æ—¶é—´: {nyc_env.current_time}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºå……ç”µç«™ä½ç½®\n",
    "    if nyc_env.charging_stations:\n",
    "        print(f\"\\nâš¡ å……ç”µç«™åˆ†å¸ƒ:\")\n",
    "        for i, (station_id, station) in enumerate(list(nyc_env.charging_stations.items())[:5]):  # æ˜¾ç¤ºå‰5ä¸ª\n",
    "            # ä¿®å¤ï¼šæ£€æŸ¥stationçš„æ•°æ®ç»“æ„\n",
    "            if hasattr(station, 'location'):\n",
    "                lat, lon = station.location\n",
    "                capacity = station.capacity if hasattr(station, 'capacity') else 'N/A'\n",
    "            elif isinstance(station, dict):\n",
    "                lat, lon = station.get('location', (0, 0))\n",
    "                capacity = station.get('capacity', 'N/A')\n",
    "            else:\n",
    "                lat, lon = 0, 0\n",
    "                capacity = 'N/A'\n",
    "            \n",
    "            print(f\"   ç«™ç‚¹ {i+1}: ({lat:.4f}, {lon:.4f}) - {capacity}ä¸ªå……ç”µæ¡©\")\n",
    "        if len(nyc_env.charging_stations) > 5:\n",
    "            print(f\"   ... è¿˜æœ‰ {len(nyc_env.charging_stations) - 5} ä¸ªå……ç”µç«™\")\n",
    "    else:\n",
    "        print(f\"\\nâš¡ å……ç”µç«™åˆ†å¸ƒ: æœªåˆå§‹åŒ–\")\n",
    "    \n",
    "    # æ˜¾ç¤ºè½¦è¾†åˆå§‹çŠ¶æ€\n",
    "    print(f\"\\nğŸš— è½¦é˜ŸçŠ¶æ€:\")\n",
    "    if nyc_env.vehicles:\n",
    "        available_vehicles = [v for v in nyc_env.vehicles.values() if hasattr(v, 'available') and v.available]\n",
    "        charging_vehicles = [v for v in nyc_env.vehicles.values() if hasattr(v, 'is_charging') and v.is_charging]\n",
    "        \n",
    "        print(f\"   å¯ç”¨è½¦è¾†: {len(available_vehicles)}\")\n",
    "        print(f\"   å……ç”µä¸­è½¦è¾†: {len(charging_vehicles)}\")\n",
    "        \n",
    "        # æ˜¾ç¤ºç¤ºä¾‹è½¦è¾†ä¿¡æ¯\n",
    "        sample_vehicle = list(nyc_env.vehicles.values())[0]\n",
    "        if hasattr(sample_vehicle, 'location'):\n",
    "            lat, lon = sample_vehicle.location\n",
    "            print(f\"   ç¤ºä¾‹è½¦è¾†ä½ç½®: ({lat:.4f}, {lon:.4f})\")\n",
    "        if hasattr(sample_vehicle, 'battery_level'):\n",
    "            print(f\"   ç¤ºä¾‹è½¦è¾†ç”µé‡: {sample_vehicle.battery_level:.1%}\")\n",
    "    else:\n",
    "        print(f\"   è½¦è¾†æœªåˆå§‹åŒ–\")\n",
    "    \n",
    "    print(\"âœ… ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "    print(\"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    nyc_env = None\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3322146",
   "metadata": {},
   "source": [
    "## 7. é…ç½®ç¯å¢ƒå‚æ•°\n",
    "è®¾ç½®ä»¿çœŸç¯å¢ƒçš„å„ç§å‚æ•°ï¼ŒåŒ…æ‹¬è½¦è¾†é…ç½®ã€éœ€æ±‚æ¨¡å¼ç­‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c038026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é…ç½®ä»¿çœŸç¯å¢ƒå‚æ•°\n",
    "if nyc_env is not None:\n",
    "    print(\"âš™ï¸ é…ç½®ä»¿çœŸç¯å¢ƒå‚æ•°...\")\n",
    "    \n",
    "    try:\n",
    "        # 1. æ—¶é—´è®¾ç½® - ä¿®å¤ï¼šä½¿ç”¨current_timeè€Œä¸æ˜¯current_time_minutes\n",
    "        nyc_env.current_time = SIMULATION_CONFIG['start_time']\n",
    "        print(f\"â° è®¾ç½®å¼€å§‹æ—¶é—´: {SIMULATION_CONFIG['start_time'].strftime('%Y-%m-%d %H:%M')}\")\n",
    "        \n",
    "        # 2. è½¦è¾†å‚æ•°é…ç½®\n",
    "        vehicle_config_applied = 0\n",
    "        if nyc_env.vehicles:\n",
    "            for vehicle in nyc_env.vehicles.values():\n",
    "                # ä¿®å¤ï¼šæ£€æŸ¥vehicleçš„æ•°æ®ç»“æ„\n",
    "                if hasattr(vehicle, 'battery_level'):\n",
    "                    vehicle.battery_level = SIMULATION_CONFIG['initial_battery']\n",
    "                elif isinstance(vehicle, dict):\n",
    "                    vehicle['battery_level'] = SIMULATION_CONFIG['initial_battery']\n",
    "                vehicle_config_applied += 1\n",
    "        \n",
    "        print(f\"ğŸ”‹ é…ç½® {vehicle_config_applied} è¾†è½¦çš„åˆå§‹ç”µé‡: {SIMULATION_CONFIG['initial_battery']:.0%}\")\n",
    "        \n",
    "        # 3. åˆ›å»ºè¯·æ±‚ç”Ÿæˆå™¨\n",
    "        try:\n",
    "            request_generator = NYCRequestGenerator()\n",
    "            print(\"âœ“ è¯·æ±‚ç”Ÿæˆå™¨åˆ›å»ºæˆåŠŸ\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ è¯·æ±‚ç”Ÿæˆå™¨åˆ›å»ºå¤±è´¥: {e}\")\n",
    "            request_generator = None\n",
    "        \n",
    "        # æ ¹æ®æ˜¯å¦æœ‰çœŸå®æ•°æ®è°ƒæ•´ç”Ÿæˆå™¨\n",
    "        if not df_final.empty and SIMULATION_CONFIG['use_real_data']:\n",
    "            print(\"ğŸ“Š å°†ä½¿ç”¨çœŸå®æ•°æ®æ¨¡å¼ç”Ÿæˆè¯·æ±‚\")\n",
    "            # è¿™é‡Œå¯ä»¥ç”¨çœŸå®æ•°æ®è®­ç»ƒç”Ÿæˆå™¨æˆ–ç›´æ¥ä½¿ç”¨æ•°æ®\n",
    "        else:\n",
    "            print(\"ğŸ² å°†ä½¿ç”¨åˆæˆæ•°æ®æ¨¡å¼ç”Ÿæˆè¯·æ±‚\")\n",
    "        \n",
    "        # 4. éœ€æ±‚æ¨¡å¼é…ç½®\n",
    "        demand_config = {\n",
    "            'base_requests_per_hour': SIMULATION_CONFIG['requests_per_hour'],\n",
    "            'time_step_minutes': SIMULATION_CONFIG['time_step_minutes'],\n",
    "            'duration_hours': SIMULATION_CONFIG['duration_hours']\n",
    "        }\n",
    "        \n",
    "        print(f\"ğŸ“‹ éœ€æ±‚é…ç½®:\")\n",
    "        for key, value in demand_config.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        \n",
    "        # 5. éªŒè¯ç¯å¢ƒçŠ¶æ€\n",
    "        print(f\"\\nğŸ” ç¯å¢ƒçŠ¶æ€éªŒè¯:\")\n",
    "        print(f\"   è½¦è¾†æ€»æ•°: {len(nyc_env.vehicles)}\")\n",
    "        \n",
    "        if nyc_env.vehicles:\n",
    "            # ä¿®å¤ï¼šå¤„ç†è½¦è¾†å¯ç”¨æ€§æ£€æŸ¥\n",
    "            available_count = 0\n",
    "            battery_levels = []\n",
    "            \n",
    "            for vehicle in nyc_env.vehicles.values():\n",
    "                if hasattr(vehicle, 'available'):\n",
    "                    if vehicle.available:\n",
    "                        available_count += 1\n",
    "                elif isinstance(vehicle, dict):\n",
    "                    if vehicle.get('available', True):\n",
    "                        available_count += 1\n",
    "                \n",
    "                if hasattr(vehicle, 'battery_level'):\n",
    "                    battery_levels.append(vehicle.battery_level)\n",
    "                elif isinstance(vehicle, dict):\n",
    "                    battery_levels.append(vehicle.get('battery_level', 0.8))\n",
    "            \n",
    "            print(f\"   å¯ç”¨è½¦è¾†: {available_count}\")\n",
    "            if battery_levels:\n",
    "                print(f\"   å¹³å‡ç”µé‡: {np.mean(battery_levels):.1%}\")\n",
    "        else:\n",
    "            print(f\"   å¯ç”¨è½¦è¾†: 0 (è½¦è¾†æœªåˆå§‹åŒ–)\")\n",
    "            \n",
    "        print(f\"   å……ç”µç«™: {len(nyc_env.charging_stations)} ä¸ª\")\n",
    "        \n",
    "        # è®¡ç®—ä»¿çœŸæ€»æ­¥æ•°\n",
    "        total_steps = SIMULATION_CONFIG['duration_hours'] * 60 // SIMULATION_CONFIG['time_step_minutes']\n",
    "        print(f\"   é¢„è®¡ä»¿çœŸæ­¥æ•°: {total_steps}\")\n",
    "        \n",
    "        print(\"âœ… ç¯å¢ƒå‚æ•°é…ç½®å®Œæˆ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ç¯å¢ƒé…ç½®å¤±è´¥: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ ç¯å¢ƒæœªåˆå§‹åŒ–ï¼Œè·³è¿‡å‚æ•°é…ç½®\")\n",
    "    request_generator = None\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf815611",
   "metadata": {},
   "source": [
    "## 8. ç”Ÿæˆé¢„è®¢å•æ•°æ®\n",
    "åœ¨ä»¿çœŸç¯å¢ƒä¸­ç”Ÿæˆå‡ºç§Ÿè½¦é¢„è®¢å•ï¼ŒåŒ…æ‹¬èµ·ç‚¹ã€ç»ˆç‚¹ã€æ—¶é—´ç­‰ä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07105eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆé¢„è®¢å•æ•°æ®\n",
    "print(\"ğŸ“‹ å¼€å§‹ç”Ÿæˆé¢„è®¢å•æ•°æ®...\")\n",
    "\n",
    "if nyc_env is not None and request_generator is not None:\n",
    "    \n",
    "    # å­˜å‚¨ç”Ÿæˆçš„è®¢å•\n",
    "    generated_orders = []\n",
    "    simulation_log = []\n",
    "    \n",
    "    try:\n",
    "        # ä»¿çœŸå‚æ•°\n",
    "        current_time = SIMULATION_CONFIG['start_time']\n",
    "        end_time = current_time + timedelta(hours=SIMULATION_CONFIG['duration_hours'])\n",
    "        step_minutes = SIMULATION_CONFIG['time_step_minutes']\n",
    "        \n",
    "        step_count = 0\n",
    "        total_requests = 0\n",
    "        \n",
    "        print(f\"â° ä»¿çœŸæ—¶é—´: {current_time.strftime('%H:%M')} - {end_time.strftime('%H:%M')}\")\n",
    "        print(f\"âš¡ æ—¶é—´æ­¥é•¿: {step_minutes} åˆ†é’Ÿ\")\n",
    "        \n",
    "        # ä»¿çœŸå¾ªç¯\n",
    "        while current_time < end_time and step_count < 100:  # é™åˆ¶æ­¥æ•°é¿å…æ— é™å¾ªç¯\n",
    "            \n",
    "            # 1. ç”Ÿæˆå½“å‰æ—¶é—´æ­¥çš„è®¢å•\n",
    "            try:\n",
    "                # æ ¹æ®æ—¶é—´å’Œéœ€æ±‚æ¨¡å¼ç”Ÿæˆè®¢å•æ•°é‡\n",
    "                hour = current_time.hour\n",
    "                if hour in [7, 8, 17, 18, 19]:  # é«˜å³°æœŸ\n",
    "                    num_orders = np.random.poisson(SIMULATION_CONFIG['requests_per_hour'] / 60 * step_minutes * 1.5)\n",
    "                elif hour in [22, 23, 0, 1, 2, 3, 4, 5]:  # æ·±å¤œ\n",
    "                    num_orders = np.random.poisson(SIMULATION_CONFIG['requests_per_hour'] / 60 * step_minutes * 0.3)\n",
    "                else:  # æ­£å¸¸æ—¶æ®µ\n",
    "                    num_orders = np.random.poisson(SIMULATION_CONFIG['requests_per_hour'] / 60 * step_minutes)\n",
    "                \n",
    "                # ç”Ÿæˆè®¢å•\n",
    "                step_orders = []\n",
    "                for _ in range(num_orders):\n",
    "                    try:\n",
    "                        order = request_generator.generate_request(current_time)\n",
    "                        step_orders.append(order)\n",
    "                        total_requests += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"âš ï¸ ç”Ÿæˆè®¢å•æ—¶å‡ºé”™: {e}\")\n",
    "                \n",
    "                generated_orders.extend(step_orders)\n",
    "                \n",
    "                # è®°å½•ä»¿çœŸæ­¥éª¤ - ä¿®å¤ï¼šå¤„ç†è½¦è¾†å¯ç”¨æ€§æ£€æŸ¥\n",
    "                available_vehicle_count = 0\n",
    "                for v in nyc_env.vehicles.values():\n",
    "                    if hasattr(v, 'available'):\n",
    "                        if v.available:\n",
    "                            available_vehicle_count += 1\n",
    "                    elif isinstance(v, dict):\n",
    "                        if v.get('available', True):\n",
    "                            available_vehicle_count += 1\n",
    "                \n",
    "                log_entry = {\n",
    "                    'step': step_count,\n",
    "                    'time': current_time,\n",
    "                    'hour': current_time.hour,\n",
    "                    'orders_generated': len(step_orders),\n",
    "                    'total_orders': len(generated_orders),\n",
    "                    'available_vehicles': available_vehicle_count\n",
    "                }\n",
    "                simulation_log.append(log_entry)\n",
    "                \n",
    "                # æ¯10æ­¥æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦\n",
    "                if step_count % 10 == 0:\n",
    "                    print(f\"   Step {step_count}: {current_time.strftime('%H:%M')} - \"\n",
    "                          f\"ç”Ÿæˆ {len(step_orders)} ä¸ªè®¢å• (æ€»è®¡: {total_requests})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ç¬¬ {step_count} æ­¥å‡ºé”™: {e}\")\n",
    "            \n",
    "            # 2. æ›´æ–°ä»¿çœŸæ—¶é—´\n",
    "            current_time += timedelta(minutes=step_minutes)\n",
    "            # ä¿®å¤ï¼šæ›´æ–°ç¯å¢ƒæ—¶é—´ï¼Œä½¿ç”¨current_timeè€Œä¸æ˜¯current_time_minutes\n",
    "            nyc_env.current_time = current_time\n",
    "            step_count += 1\n",
    "        \n",
    "        print(f\"\\nâœ… ä»¿çœŸå®Œæˆ!\")\n",
    "        print(f\"   æ€»æ­¥æ•°: {step_count}\")\n",
    "        print(f\"   ç”Ÿæˆè®¢å•æ•°: {len(generated_orders)}\")\n",
    "        print(f\"   ä»¿çœŸæ—¶é•¿: {step_count * step_minutes} åˆ†é’Ÿ\")\n",
    "        \n",
    "        # åˆ†æç”Ÿæˆçš„è®¢å•\n",
    "        if generated_orders:\n",
    "            print(f\"\\nğŸ“Š è®¢å•æ•°æ®åˆ†æ:\")\n",
    "            \n",
    "            # æ—¶é—´åˆ†å¸ƒ\n",
    "            order_hours = [order.request_time.hour for order in generated_orders]\n",
    "            hour_counts = pd.Series(order_hours).value_counts().sort_index()\n",
    "            \n",
    "            print(f\"   æ—¶é—´åˆ†å¸ƒ (å‰5ä¸ªé«˜å³°æ—¶æ®µ):\")\n",
    "            for hour, count in hour_counts.head().items():\n",
    "                print(f\"     {hour:2d}ç‚¹: {count:3d} ä¸ªè®¢å•\")\n",
    "            \n",
    "            # è·ç¦»åˆ†æ\n",
    "            distances = [order.trip_distance for order in generated_orders]\n",
    "            print(f\"   è¡Œç¨‹è·ç¦»: å¹³å‡ {np.mean(distances):.2f}km, æœ€å¤§ {np.max(distances):.2f}km\")\n",
    "            \n",
    "            # è´¹ç”¨åˆ†æ\n",
    "            fares = [order.total_amount for order in generated_orders]\n",
    "            print(f\"   è®¢å•ä»·å€¼: å¹³å‡ ${np.mean(fares):.2f}, æ€»è®¡ ${np.sum(fares):.2f}\")\n",
    "            \n",
    "            # åœ°ç†åˆ†å¸ƒ\n",
    "            pickup_lats = [order.pickup_location[0] for order in generated_orders]\n",
    "            pickup_lons = [order.pickup_location[1] for order in generated_orders]\n",
    "            print(f\"   åœ°ç†èŒƒå›´: çº¬åº¦ {min(pickup_lats):.4f}-{max(pickup_lats):.4f}, \"\n",
    "                  f\"ç»åº¦ {min(pickup_lons):.4f}-{max(pickup_lons):.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è®¢å•ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸ ç¯å¢ƒæˆ–ç”Ÿæˆå™¨æœªåˆå§‹åŒ–ï¼Œç”Ÿæˆç¤ºä¾‹è®¢å•æ•°æ®...\")\n",
    "    \n",
    "    # ç”Ÿæˆç¤ºä¾‹æ•°æ®\n",
    "    generated_orders = []\n",
    "    try:\n",
    "        sample_generator = NYCRequestGenerator()\n",
    "        for i in range(50):  # ç”Ÿæˆ50ä¸ªç¤ºä¾‹è®¢å•\n",
    "            sample_time = datetime(2024, 10, 20, 8, 0) + timedelta(minutes=i*5)\n",
    "            order = sample_generator.generate_request(sample_time)\n",
    "            generated_orders.append(order)\n",
    "        \n",
    "        print(f\"âœ… ç”Ÿæˆäº† {len(generated_orders)} ä¸ªç¤ºä¾‹è®¢å•\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ç¤ºä¾‹è®¢å•ç”Ÿæˆå¤±è´¥: {e}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daa29f8",
   "metadata": {},
   "source": [
    "## 9. ä¿å­˜ä»¿çœŸç»“æœ\n",
    "å°†ç”Ÿæˆçš„é¢„è®¢å•æ•°æ®å’Œä»¿çœŸç»“æœå¯¼å‡ºä¿å­˜ï¼Œç”¨äºè¿›ä¸€æ­¥åˆ†æå’Œå¯è§†åŒ–ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87705d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜ä»¿çœŸç»“æœ\n",
    "print(\"ğŸ’¾ ä¿å­˜ä»¿çœŸç»“æœ...\")\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "results_dir = os.path.join('results', 'simulation_orders')\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "if generated_orders:\n",
    "    try:\n",
    "        # 1. è½¬æ¢è®¢å•æ•°æ®ä¸ºDataFrame\n",
    "        orders_data = []\n",
    "        for order in generated_orders:\n",
    "            order_dict = order.to_dict()\n",
    "            orders_data.append(order_dict)\n",
    "        \n",
    "        df_orders = pd.DataFrame(orders_data)\n",
    "        \n",
    "        # 2. ä¿å­˜è®¢å•æ•°æ®\n",
    "        orders_csv_path = os.path.join(results_dir, f'simulation_orders_{timestamp}.csv')\n",
    "        df_orders.to_csv(orders_csv_path, index=False)\n",
    "        print(f\"ğŸ“‹ è®¢å•æ•°æ®å·²ä¿å­˜: {orders_csv_path}\")\n",
    "        \n",
    "        orders_parquet_path = os.path.join(results_dir, f'simulation_orders_{timestamp}.parquet')\n",
    "        df_orders.to_parquet(orders_parquet_path, index=False)\n",
    "        print(f\"ğŸ“‹ è®¢å•æ•°æ® (Parquet): {orders_parquet_path}\")\n",
    "        \n",
    "        # 3. ä¿å­˜ä»¿çœŸæ—¥å¿— (å¦‚æœæœ‰)\n",
    "        if 'simulation_log' in locals() and simulation_log:\n",
    "            log_df = pd.DataFrame(simulation_log)\n",
    "            log_path = os.path.join(results_dir, f'simulation_log_{timestamp}.csv')\n",
    "            log_df.to_csv(log_path, index=False)\n",
    "            print(f\"ğŸ“Š ä»¿çœŸæ—¥å¿—å·²ä¿å­˜: {log_path}\")\n",
    "        \n",
    "        # 4. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š\n",
    "        report_path = os.path.join(results_dir, f'simulation_report_{timestamp}.txt')\n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"NYC Electric Taxi Simulation Report\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\")\n",
    "            f.write(f\"Generated: {datetime.now()}\\n\")\n",
    "            f.write(f\"Simulation Config: {SIMULATION_CONFIG}\\n\\n\")\n",
    "            \n",
    "            # è®¢å•ç»Ÿè®¡\n",
    "            f.write(\"Order Statistics:\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            f.write(f\"Total Orders: {len(generated_orders)}\\n\")\n",
    "            f.write(f\"Time Range: {min(order.request_time for order in generated_orders)} to \"\n",
    "                   f\"{max(order.request_time for order in generated_orders)}\\n\")\n",
    "            \n",
    "            # åœ°ç†ç»Ÿè®¡\n",
    "            pickup_lats = [order.pickup_location[0] for order in generated_orders]\n",
    "            pickup_lons = [order.pickup_location[1] for order in generated_orders]\n",
    "            f.write(f\"Geographic Range:\\n\")\n",
    "            f.write(f\"  Latitude: {min(pickup_lats):.6f} to {max(pickup_lats):.6f}\\n\")\n",
    "            f.write(f\"  Longitude: {min(pickup_lons):.6f} to {max(pickup_lons):.6f}\\n\")\n",
    "            \n",
    "            # è·ç¦»å’Œè´¹ç”¨ç»Ÿè®¡\n",
    "            distances = [order.trip_distance for order in generated_orders]\n",
    "            fares = [order.total_amount for order in generated_orders]\n",
    "            f.write(f\"\\nTrip Statistics:\\n\")\n",
    "            f.write(f\"  Average Distance: {np.mean(distances):.2f} km\\n\")\n",
    "            f.write(f\"  Average Fare: ${np.mean(fares):.2f}\\n\")\n",
    "            f.write(f\"  Total Revenue: ${np.sum(fares):.2f}\\n\")\n",
    "            \n",
    "            # æ—¶é—´åˆ†å¸ƒ\n",
    "            hour_distribution = pd.Series([order.request_time.hour for order in generated_orders]).value_counts().sort_index()\n",
    "            f.write(f\"\\nHourly Distribution:\\n\")\n",
    "            for hour, count in hour_distribution.items():\n",
    "                f.write(f\"  {hour:2d}:00 - {count:3d} orders\\n\")\n",
    "        \n",
    "        print(f\"ğŸ“„ ä»¿çœŸæŠ¥å‘Šå·²ä¿å­˜: {report_path}\")\n",
    "        \n",
    "        # 5. æ˜¾ç¤ºä¿å­˜çš„æ–‡ä»¶ä¿¡æ¯\n",
    "        saved_files = [\n",
    "            ('è®¢å•æ•°æ® (CSV)', orders_csv_path),\n",
    "            ('è®¢å•æ•°æ® (Parquet)', orders_parquet_path),\n",
    "            ('ä»¿çœŸæŠ¥å‘Š', report_path)\n",
    "        ]\n",
    "        \n",
    "        if 'simulation_log' in locals() and simulation_log:\n",
    "            saved_files.append(('ä»¿çœŸæ—¥å¿—', log_path))\n",
    "        \n",
    "        print(f\"\\nğŸ“ ä¿å­˜çš„æ–‡ä»¶:\")\n",
    "        total_size = 0\n",
    "        for desc, path in saved_files:\n",
    "            size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "            total_size += size_mb\n",
    "            print(f\"   {desc}: {os.path.basename(path)} ({size_mb:.2f} MB)\")\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ æ€»æ–‡ä»¶å¤§å°: {total_size:.2f} MB\")\n",
    "        \n",
    "        # 6. å¿«é€Ÿæ•°æ®é¢„è§ˆ\n",
    "        print(f\"\\nğŸ‘€ è®¢å•æ•°æ®é¢„è§ˆ (å‰5æ¡):\")\n",
    "        print(df_orders[['request_id', 'pickup_latitude', 'pickup_longitude', \n",
    "                        'dropoff_latitude', 'dropoff_longitude', 'total_amount']].head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ä¿å­˜ç»“æœæ—¶å‡ºé”™: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ æ²¡æœ‰è®¢å•æ•°æ®éœ€è¦ä¿å­˜\")\n",
    "\n",
    "# æ˜¾ç¤ºç»“æœç›®å½•å†…å®¹\n",
    "if os.path.exists(results_dir):\n",
    "    result_files = os.listdir(results_dir)\n",
    "    print(f\"\\nğŸ“‚ ä»¿çœŸç»“æœç›®å½• ({results_dir}):\")\n",
    "    for file in sorted(result_files)[-10:]:  # æ˜¾ç¤ºæœ€æ–°çš„10ä¸ªæ–‡ä»¶\n",
    "        file_path = os.path.join(results_dir, file)\n",
    "        size_kb = os.path.getsize(file_path) / 1024\n",
    "        mod_time = datetime.fromtimestamp(os.path.getmtime(file_path))\n",
    "        print(f\"   ğŸ“„ {file} ({size_kb:.1f}KB, {mod_time.strftime('%m-%d %H:%M')})\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ddd4c2",
   "metadata": {},
   "source": [
    "## 10. æ•°æ®å¯è§†åŒ–åˆ†æ\n",
    "å¯¹ç”Ÿæˆçš„è®¢å•æ•°æ®è¿›è¡Œå¯è§†åŒ–åˆ†æï¼ŒåŒ…æ‹¬æ—¶é—´åˆ†å¸ƒã€åœ°ç†åˆ†å¸ƒã€éœ€æ±‚çƒ­åŠ›å›¾ç­‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf9dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®å¯è§†åŒ–åˆ†æ\n",
    "print(\"ğŸ“Š å¼€å§‹æ•°æ®å¯è§†åŒ–åˆ†æ...\")\n",
    "\n",
    "if generated_orders and len(generated_orders) > 0:\n",
    "    \n",
    "    # è®¾ç½®matplotlibä¸­æ–‡å­—ä½“å’Œæ ·å¼\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    plt.style.use('default')\n",
    "    \n",
    "    # åˆ›å»ºå›¾å½¢ç›®å½•\n",
    "    viz_dir = os.path.join('results', 'visualizations')\n",
    "    os.makedirs(viz_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # å‡†å¤‡æ•°æ®\n",
    "        hours = [order.request_time.hour for order in generated_orders]\n",
    "        distances = [order.trip_distance for order in generated_orders]\n",
    "        fares = [order.total_amount for order in generated_orders]\n",
    "        pickup_lats = [order.pickup_location[0] for order in generated_orders]\n",
    "        pickup_lons = [order.pickup_location[1] for order in generated_orders]\n",
    "        \n",
    "        # 1. æ—¶é—´åˆ†å¸ƒå›¾\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('NYC Electric Taxi Order Analysis', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # æ—¶é—´åˆ†å¸ƒæŸ±çŠ¶å›¾\n",
    "        hour_counts = pd.Series(hours).value_counts().sort_index()\n",
    "        axes[0, 0].bar(hour_counts.index, hour_counts.values, color='steelblue', alpha=0.7)\n",
    "        axes[0, 0].set_title('Hourly Order Distribution')\n",
    "        axes[0, 0].set_xlabel('Hour of Day')\n",
    "        axes[0, 0].set_ylabel('Number of Orders')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # è·ç¦»åˆ†å¸ƒç›´æ–¹å›¾\n",
    "        axes[0, 1].hist(distances, bins=30, color='green', alpha=0.7, edgecolor='black')\n",
    "        axes[0, 1].set_title('Trip Distance Distribution')\n",
    "        axes[0, 1].set_xlabel('Distance (km)')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].axvline(np.mean(distances), color='red', linestyle='--', \n",
    "                          label=f'Mean: {np.mean(distances):.1f}km')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # è´¹ç”¨åˆ†å¸ƒç›´æ–¹å›¾\n",
    "        axes[1, 0].hist(fares, bins=30, color='orange', alpha=0.7, edgecolor='black')\n",
    "        axes[1, 0].set_title('Fare Distribution')\n",
    "        axes[1, 0].set_xlabel('Fare Amount ($)')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].axvline(np.mean(fares), color='red', linestyle='--',\n",
    "                          label=f'Mean: ${np.mean(fares):.1f}')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # åœ°ç†åˆ†å¸ƒæ•£ç‚¹å›¾\n",
    "        axes[1, 1].scatter(pickup_lons, pickup_lats, alpha=0.6, s=1, c='red')\n",
    "        axes[1, 1].set_title('Pickup Locations Geographic Distribution')\n",
    "        axes[1, 1].set_xlabel('Longitude')\n",
    "        axes[1, 1].set_ylabel('Latitude')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # è®¾ç½®Manhattanè¾¹ç•Œ\n",
    "        if nyc_env:\n",
    "            bounds = nyc_env.manhattan_bounds\n",
    "            axes[1, 1].set_xlim(bounds[0][1], bounds[1][1])\n",
    "            axes[1, 1].set_ylim(bounds[0][0], bounds[1][0])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # ä¿å­˜å›¾ç‰‡\n",
    "        viz_path = os.path.join(viz_dir, f'order_analysis_{timestamp}.png')\n",
    "        plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"ğŸ“ˆ è®¢å•åˆ†æå›¾å·²ä¿å­˜: {viz_path}\")\n",
    "        plt.show()\n",
    "        \n",
    "        # 2. éœ€æ±‚çƒ­åŠ›å›¾ (å¦‚æœæ•°æ®è¶³å¤Ÿå¤š)\n",
    "        if len(generated_orders) > 50:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "            \n",
    "            # åˆ›å»ºçƒ­åŠ›å›¾ç½‘æ ¼\n",
    "            from scipy.stats import gaussian_kde\n",
    "            \n",
    "            # è®¡ç®—å¯†åº¦\n",
    "            xy = np.vstack([pickup_lons, pickup_lats])\n",
    "            density = gaussian_kde(xy)(xy)\n",
    "            \n",
    "            scatter = ax.scatter(pickup_lons, pickup_lats, c=density, \n",
    "                               s=10, alpha=0.6, cmap='YlOrRd')\n",
    "            ax.set_title('Order Demand Heatmap', fontsize=14, fontweight='bold')\n",
    "            ax.set_xlabel('Longitude')\n",
    "            ax.set_ylabel('Latitude')\n",
    "            \n",
    "            # æ·»åŠ é¢œè‰²æ¡\n",
    "            cbar = plt.colorbar(scatter, ax=ax)\n",
    "            cbar.set_label('Demand Density')\n",
    "            \n",
    "            # æ·»åŠ å……ç”µç«™ä½ç½® (å¦‚æœæœ‰)\n",
    "            if nyc_env and nyc_env.charging_stations:\n",
    "                station_lats = [s['location'][0] for s in nyc_env.charging_stations]\n",
    "                station_lons = [s['location'][1] for s in nyc_env.charging_stations]\n",
    "                ax.scatter(station_lons, station_lats, c='blue', s=100, \n",
    "                          marker='^', label='Charging Stations', alpha=0.8)\n",
    "                ax.legend()\n",
    "            \n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            heatmap_path = os.path.join(viz_dir, f'demand_heatmap_{timestamp}.png')\n",
    "            plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"ğŸ”¥ éœ€æ±‚çƒ­åŠ›å›¾å·²ä¿å­˜: {heatmap_path}\")\n",
    "            plt.show()\n",
    "        \n",
    "        # 3. ç»Ÿè®¡æ‘˜è¦è¡¨\n",
    "        print(f\"\\nğŸ“‹ è®¢å•æ•°æ®ç»Ÿè®¡æ‘˜è¦:\")\n",
    "        print(f\"{'æŒ‡æ ‡':<20} {'æ•°å€¼':<15} {'å¤‡æ³¨'}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"{'è®¢å•æ€»æ•°':<18} {len(generated_orders):<15} {'ä¸ª'}\")\n",
    "        print(f\"{'å¹³å‡è·ç¦»':<18} {np.mean(distances):<15.2f} {'km'}\")\n",
    "        print(f\"{'å¹³å‡è´¹ç”¨':<18} ${np.mean(fares):<14.2f} {'USD'}\")\n",
    "        print(f\"{'æ€»æ”¶å…¥':<18} ${np.sum(fares):<14.2f} {'USD'}\")\n",
    "        print(f\"{'æœ€é•¿è·ç¦»':<18} {np.max(distances):<15.2f} {'km'}\")\n",
    "        print(f\"{'æœ€é«˜è´¹ç”¨':<18} ${np.max(fares):<14.2f} {'USD'}\")\n",
    "        \n",
    "        # é«˜å³°æ—¶æ®µåˆ†æ\n",
    "        peak_hours = pd.Series(hours).value_counts().head(3)\n",
    "        print(f\"\\nğŸ• é«˜å³°æ—¶æ®µ:\")\n",
    "        for hour, count in peak_hours.items():\n",
    "            print(f\"   {hour:2d}:00 - {count:3d} è®¢å•\")\n",
    "        \n",
    "        # åœ°ç†è¦†ç›–èŒƒå›´\n",
    "        lat_range = max(pickup_lats) - min(pickup_lats)\n",
    "        lon_range = max(pickup_lons) - min(pickup_lons)\n",
    "        print(f\"\\nğŸ—ºï¸ åœ°ç†è¦†ç›–:\")\n",
    "        print(f\"   çº¬åº¦èŒƒå›´: {lat_range:.4f}Â° ({lat_range*111:.1f}km)\")\n",
    "        print(f\"   ç»åº¦èŒƒå›´: {lon_range:.4f}Â° ({lon_range*85:.1f}km)\")  # çº½çº¦çº¬åº¦çº¦85km/åº¦\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å¯è§†åŒ–è¿‡ç¨‹å‡ºé”™: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸ æ²¡æœ‰è®¢å•æ•°æ®å¯ä¾›å¯è§†åŒ–\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
