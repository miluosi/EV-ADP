# Mean Field Multi-Agent Q-Learning å®ç°æ€»ç»“

## 1. å®ç°æ¦‚è¿°

å·²å®Œæˆ Mean Field Q-Learning çš„å®Œæ•´å®ç°ï¼ŒåŒ…æ‹¬ï¼š

### Environment.py ä¸­çš„å®ç°
- âœ… `batch_evaluate_service_options_meanfield()`: æ‰¹é‡è¯„ä¼°å¸¦æœ‰ mean field çš„æœåŠ¡é€‰é¡¹

### ValueFunction_pytorch_mf.py ä¸­çš„å®ç°
- âœ… `MeanFieldQNetwork`: Mean Field Q-Network ç¥ç»ç½‘ç»œ
- âœ… `MeanFieldExperienceReplay`: ç»éªŒå›æ”¾ç¼“å†²åŒºï¼ˆåŒ…å« mean field ä¿¡æ¯ï¼‰
- âœ… `MeanFieldAgent`: Mean Field Q-Learning æ™ºèƒ½ä½“
- âœ… `PyTorchChargingValueFunction` çš„æ‰©å±•æ–¹æ³•ï¼š
  - `compute_mean_field()`: è®¡ç®—é‚»å±…æ™ºèƒ½ä½“çš„å¹³å‡åŠ¨ä½œåˆ†å¸ƒ
  - `batch_get_q_value_with_mean_field()`: æ‰¹é‡è®¡ç®— Q(s, a, Î¼)
  - `update_agent_action_distribution()`: æ›´æ–°æ™ºèƒ½ä½“åŠ¨ä½œåˆ†å¸ƒå†å²

## 2. Mean Field Q-Learning æ ¸å¿ƒæ¦‚å¿µ

### ç†è®ºåŸºç¡€
Mean Field Multi-Agent RL (Yang et al., 2018) çš„æ ¸å¿ƒæ€æƒ³ï¼š
- ä¸æ˜¯å»ºæ¨¡æ‰€æœ‰ N-1 ä¸ªå…¶ä»–æ™ºèƒ½ä½“ï¼Œè€Œæ˜¯å»ºæ¨¡å®ƒä»¬çš„**å¹³å‡åŠ¨ä½œåˆ†å¸ƒ** Î¼
- Qå‡½æ•°ï¼š`Q(s, a, Î¼)` å…¶ä¸­ Î¼ æ˜¯é‚»å±…çš„ mean field
- ä¼˜åŠ¿ï¼šå°†å¤æ‚åº¦ä» O(N^2) é™ä½åˆ° O(N)

### å®ç°ç»†èŠ‚

#### 1. Mean Field è®¡ç®—
```python
def compute_mean_field(environment, agent_id, agent_locations, neighbor_radius=5.0):
    """
    è®¡ç®—é‚»å±…æ™ºèƒ½ä½“çš„å¹³å‡åŠ¨ä½œåˆ†å¸ƒ
    
    æ­¥éª¤ï¼š
    1. æ‰¾åˆ°å½“å‰agentä½ç½®å‘¨å›´neighbor_radiuså†…çš„æ‰€æœ‰é‚»å±…
    2. è·å–æ¯ä¸ªé‚»å±…çš„å†å²åŠ¨ä½œåˆ†å¸ƒ
    3. è®¡ç®—å¹³å‡å€¼å¾—åˆ° mean field Î¼
    """
```

**è¾“å…¥**: 
- å½“å‰agentä½ç½®
- æ‰€æœ‰agentä½ç½®å­—å…¸
- é‚»å±…åŠå¾„

**è¾“å‡º**: 
- `[action_dim]` ç»´çš„åŠ¨ä½œåˆ†å¸ƒå‘é‡

#### 2. Mean Field Q-Network ç»“æ„

```
è¾“å…¥:
â”œâ”€â”€ Vehicle Features [batch, 8]        # è½¦è¾†çŠ¶æ€
â”œâ”€â”€ Request Features [batch, 6]        # è¯·æ±‚ç‰¹å¾
â”œâ”€â”€ Global Features [batch, 4]         # å…¨å±€çŠ¶æ€
â””â”€â”€ Mean Field [batch, action_dim]     # é‚»å±…å¹³å‡åŠ¨ä½œåˆ†å¸ƒ

ç¼–ç å™¨:
â”œâ”€â”€ Vehicle Encoder â†’ [batch, hidden//4]
â”œâ”€â”€ Request Encoder â†’ [batch, hidden//4]
â”œâ”€â”€ Global Encoder â†’ [batch, hidden//8]
â””â”€â”€ Mean Field Encoder â†’ [batch, mean_field_dim]

åŠ¨ä½œåµŒå…¥:
â””â”€â”€ Action Embedding â†’ [batch, hidden//4]

èåˆ:
â””â”€â”€ Concatenate all â†’ [batch, total_feature_dim]

Q-Network:
â”œâ”€â”€ MLP layers with ReLU and Dropout
â””â”€â”€ Dueling Architecture:
    â”œâ”€â”€ Value Stream â†’ V(s, Î¼)
    â””â”€â”€ Advantage Stream â†’ A(s, a, Î¼)

è¾“å‡º:
â””â”€â”€ Q(s, a, Î¼) = V(s, Î¼) + A(s, a, Î¼) - mean(A)
```

#### 3. è®­ç»ƒæ›´æ–°è§„åˆ™

Mean Field Q-Learning æ›´æ–°:
```
Q(s, a, Î¼) â† r + Î³ * E_{a'~Ï€(Â·|s',Î¼')}[Q(s', a', Î¼')]
```

å®ç°ä½¿ç”¨ Double DQN with Mean Field:
1. Policy Network é€‰æ‹©åŠ¨ä½œ
2. Target Network è¯„ä¼° Q å€¼
3. Soft update target network

## 3. ä»£ç æ£€æŸ¥æ¸…å•

### âœ… Environment.py
- [x] `batch_evaluate_service_options_meanfield` æ­£ç¡®å®ç°
- [x] è°ƒç”¨ `value_function.compute_mean_field()` è®¡ç®— mean field
- [x] è°ƒç”¨ `value_function.batch_get_q_value_with_mean_field()` è®¡ç®— Q å€¼
- [x] é”™è¯¯å¤„ç†ï¼šå›é€€åˆ°æ™®é€šæ–¹æ³•
- [x] æ”¯æŒ EV å’Œ AEV ä¸¤ç§ value function

### âœ… ValueFunction_pytorch_mf.py

#### MeanFieldQNetwork ç±»
- [x] æ­£ç¡®çš„ç½‘ç»œç»“æ„ï¼ˆvehicle/request/global/mean_field encodersï¼‰
- [x] Mean field encoder æ¥å— `[action_dim]` ç»´è¾“å…¥
- [x] Action embedding å±‚
- [x] Dueling architecture (value + advantage streams)
- [x] `forward()` æ–¹æ³•æ”¯æŒå•ä¸ªåŠ¨ä½œæˆ–æ‰€æœ‰åŠ¨ä½œ
- [x] `forward_dueling()` æ–¹æ³•é«˜æ•ˆè®¡ç®—æ‰€æœ‰åŠ¨ä½œçš„ Q å€¼

#### MeanFieldAgent ç±»
- [x] Policy network å’Œ Target network
- [x] `compute_mean_field()` æ–¹æ³•
- [x] `compute_action_distribution()` ä½¿ç”¨ Boltzmann policy
- [x] `select_action()` ä½¿ç”¨ epsilon-greedy
- [x] `train_step()` å®ç° Mean Field Q-Learning æ›´æ–°
- [x] Soft update target network

#### MeanFieldExperienceReplay ç±»
- [x] å­˜å‚¨ (state, action, mean_field, reward, next_state, next_mean_field, done)
- [x] `push()` æ–¹æ³•
- [x] `sample()` æ–¹æ³•

#### PyTorchChargingValueFunction æ‰©å±•
- [x] `compute_mean_field()` æ–¹æ³•
- [x] `batch_get_q_value_with_mean_field()` æ–¹æ³•
- [x] `update_agent_action_distribution()` æ–¹æ³•
- [x] é‚»å±…å®šä¹‰ï¼šæ¬§æ°è·ç¦» â‰¤ neighbor_radius
- [x] æ²¡æœ‰é‚»å±…æ—¶è¿”å›å‡åŒ€åˆ†å¸ƒ

## 4. æ½œåœ¨é—®é¢˜æ£€æŸ¥

### 1. åŠ¨ä½œç»´åº¦ä¸€è‡´æ€§
- âš ï¸ éœ€è¦ç¡®è®¤ï¼š`action_dim` åœ¨ä¸åŒåœ°æ–¹æ˜¯å¦ä¸€è‡´
  - MeanFieldQNetwork: `action_dim` å‚æ•°
  - PyTorchChargingValueFunction: ç®€åŒ–ä¸º 3 (assign, idle, charge)
  - å®é™…ç¯å¢ƒï¼šå¯èƒ½æœ‰æ›´å¤šåŠ¨ä½œç±»å‹

**å»ºè®®**: ç»Ÿä¸€ `action_dim` å®šä¹‰

### 2. Mean Field åˆå§‹åŒ–
- âœ… æ²¡æœ‰é‚»å±…æ—¶ä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
- âœ… æ²¡æœ‰å†å²åŠ¨ä½œæ—¶ä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
- âš ï¸ éœ€è¦ç¡®ä¿åœ¨ç¬¬ä¸€ä¸ª episode å¼€å§‹æ—¶åˆå§‹åŒ– `agent_action_distributions`

### 3. åŠ¨ä½œåˆ†å¸ƒæ›´æ–°
- âš ï¸ éœ€è¦åœ¨æ¯ä¸ª step åè°ƒç”¨ `update_agent_action_distribution()`
- å»ºè®®åœ¨ Environment.step() æˆ– simulate_motion() ä¸­æ·»åŠ 

### 4. ç½‘ç»œè¾“å…¥ç»´åº¦
- âš ï¸ PyTorchPathBasedNetwork æ˜¯å¦æ”¯æŒ `forward_with_mean_field()` æ–¹æ³•ï¼Ÿ
- å½“å‰å®ç°åœ¨ `batch_get_q_value_with_mean_field()` ä¸­æœ‰å›é€€æœºåˆ¶

### 5. æ€§èƒ½è€ƒè™‘
- âœ… æ‰¹é‡è®¡ç®—æé«˜æ•ˆç‡
- âœ… é‚»å±…è®¡ç®—ä½¿ç”¨ç©ºé—´ç´¢å¼•ï¼ˆåŸºäºåŠå¾„ï¼‰
- âš ï¸ å¤§è§„æ¨¡åœºæ™¯ä¸‹å¯èƒ½éœ€è¦ä¼˜åŒ–é‚»å±…æœç´¢ï¼ˆKD-treeï¼‰

## 5. ä½¿ç”¨ç¤ºä¾‹

### ç¯å¢ƒé…ç½®
```python
# åœ¨ Environment åˆå§‹åŒ–å
env = ChargingIntegratedEnvironment(...)

# è®¾ç½® value function ä½¿ç”¨ mean field
value_function = PyTorchChargingValueFunction(...)
env.set_value_function(value_function)
```

### æ‰¹é‡è¯„ä¼°
```python
# å‡†å¤‡ vehicle-request pairs
vehicle_request_pairs = [
    (vehicle_id_1, request_1),
    (vehicle_id_2, request_2),
    ...
]

# ä½¿ç”¨ mean field æ‰¹é‡è¯„ä¼°
q_values = env.batch_evaluate_service_options_meanfield(
    vehicle_request_pairs,
    ifEVQvalue=False
)
```

### è®­ç»ƒå¾ªç¯
```python
for episode in range(num_episodes):
    state = env.reset()
    
    for step in range(max_steps):
        # 1. è®¡ç®— mean field
        mean_field = value_function.compute_mean_field(env, agent_id)
        
        # 2. é€‰æ‹©åŠ¨ä½œ
        action, q_values, action_probs = agent.select_action(
            state_features, mean_field, training=True
        )
        
        # 3. æ‰§è¡ŒåŠ¨ä½œ
        next_state, reward, done, info = env.step(action)
        
        # 4. æ›´æ–°åŠ¨ä½œåˆ†å¸ƒ
        value_function.update_agent_action_distribution(
            agent_id, action_probs
        )
        
        # 5. è®¡ç®—ä¸‹ä¸€ä¸ª mean field
        next_mean_field = value_function.compute_mean_field(env, agent_id)
        
        # 6. å­˜å‚¨ç»éªŒ
        agent.store_transition(
            state, action, mean_field, 
            reward, next_state, next_mean_field, done
        )
        
        # 7. è®­ç»ƒ
        loss = agent.train_step(batch_size=64)
```

## 6. æµ‹è¯•å»ºè®®

### å•å…ƒæµ‹è¯•
1. æµ‹è¯• `compute_mean_field()` è¿”å›æ­£ç¡®ç»´åº¦
2. æµ‹è¯•æ²¡æœ‰é‚»å±…æ—¶è¿”å›å‡åŒ€åˆ†å¸ƒ
3. æµ‹è¯• `batch_get_q_value_with_mean_field()` æ‰¹é‡å¤§å°ä¸€è‡´
4. æµ‹è¯• MeanFieldQNetwork forward pass

### é›†æˆæµ‹è¯•
1. æµ‹è¯• `batch_evaluate_service_options_meanfield()` å®Œæ•´æµç¨‹
2. æµ‹è¯•ä¸ Gurobi optimizer çš„é›†æˆ
3. æ¯”è¾ƒ mean field æ–¹æ³• vs æ™®é€šæ–¹æ³•çš„æ€§èƒ½

### éªŒè¯æµ‹è¯•
1. æ£€æŸ¥ Q å€¼æ˜¯å¦æ”¶æ•›
2. æ£€æŸ¥ mean field æ˜¯å¦éšè®­ç»ƒå˜åŒ–
3. æ£€æŸ¥å¤šæ™ºèƒ½ä½“åè°ƒæ•ˆæœ

## 7. æ€§èƒ½ä¼˜åŒ–å»ºè®®

### å½“å‰å®ç°
- âœ… æ‰¹é‡è®¡ç®—
- âœ… ç¼“å­˜åŠ¨ä½œåˆ†å¸ƒ
- âœ… GPU åŠ é€Ÿ

### å¯èƒ½çš„ä¼˜åŒ–
1. **é‚»å±…æœç´¢ä¼˜åŒ–**
   - ä½¿ç”¨ç©ºé—´ç´¢å¼•ï¼ˆKD-tree, Grid-basedï¼‰
   - é¢„è®¡ç®—é‚»å±…å…³ç³»

2. **Mean Field ç¼“å­˜**
   - å¦‚æœä½ç½®ä¸å˜ï¼Œç¼“å­˜ mean field
   - å¢é‡æ›´æ–°è€Œéé‡æ–°è®¡ç®—

3. **å¹¶è¡Œè®¡ç®—**
   - å¤šä¸ª agent çš„ mean field å¹¶è¡Œè®¡ç®—
   - æ‰¹é‡å¤„ç†æ‰€æœ‰ agents

## 8. å‚è€ƒæ–‡çŒ®

- Yang et al. (2018). "Mean Field Multi-Agent Reinforcement Learning"
- Lowe et al. (2017). "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments"
- Foerster et al. (2018). "Counterfactual Multi-Agent Policy Gradients"

## 9. æ€»ç»“

### å·²å®Œæˆ âœ…
- Mean Field Q-Network æ¶æ„
- æ‰¹é‡ mean field Qå€¼è®¡ç®—
- é‚»å±…åŠ¨ä½œåˆ†å¸ƒè®¡ç®—
- ç»éªŒå›æ”¾æœºåˆ¶
- è®­ç»ƒæ›´æ–°è§„åˆ™

### éœ€è¦å®Œå–„ âš ï¸
- åŠ¨ä½œç»´åº¦ç»Ÿä¸€
- åŠ¨ä½œåˆ†å¸ƒæ›´æ–°é›†æˆåˆ°ç¯å¢ƒ step
- PyTorchPathBasedNetwork çš„ mean field æ”¯æŒ
- å¤§è§„æ¨¡åœºæ™¯çš„æ€§èƒ½ä¼˜åŒ–

### å»ºè®®ä¸‹ä¸€æ­¥ ğŸ“‹
1. è¿è¡Œç®€å•æµ‹è¯•éªŒè¯åŸºæœ¬åŠŸèƒ½
2. æ·»åŠ åŠ¨ä½œåˆ†å¸ƒæ›´æ–°åˆ°ç¯å¢ƒå¾ªç¯
3. ç»Ÿä¸€åŠ¨ä½œç©ºé—´å®šä¹‰
4. æ€§èƒ½åŸºå‡†æµ‹è¯•
5. ä¸ç°æœ‰æ–¹æ³•å¯¹æ¯”å®éªŒ
