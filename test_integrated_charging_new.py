"""
Integrated Test: Vehicle Charging Behavior Integration Test using src folder components
"""
from datetime import datetime
import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
import numpy as np
import matplotlib.pyplot as plt
import random
from pathlib import Path
import time
from collections import defaultdict, deque
import pandas as pd
from datetime import datetime
from src.ChargingIntegrationVisualization import ChargingIntegrationVisualization
# Set matplotlib Chinese font support
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False  # Fix minus sign display issue

# å¯¼å…¥é…ç½®ç®¡ç†å™¨
from config.config_manager import ConfigManager, get_config, get_training_config, get_sampling_config

from src.Environment import Environment
from src.LearningAgent import LearningAgent
from src.Action import Action, ChargingAction, ServiceAction
from src.Request import Request
from src.charging_station import ChargingStationManager, ChargingStation
from src.CentralAgent import CentralAgent
from src.ValueFunction_pytorch import PyTorchChargingValueFunction
from src.Environment import ChargingIntegratedEnvironment
from src.SpatialVisualization import SpatialVisualization
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque
print("âœ“ Successfully imported core components from src folder")
USE_SRC_COMPONENTS = True


def load_q_network_checkpoint(value_function, checkpoint_path):
    """
    åŠ è½½å·²ä¿å­˜çš„Q-networkæ£€æŸ¥ç‚¹
    
    Args:
        value_function: PyTorchChargingValueFunctionå®ä¾‹
        checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸåŠ è½½
    """
    try:
        if not os.path.exists(checkpoint_path):
            print(f"âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            return False
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(checkpoint_path, map_location=value_function.device)
        
        # æ¢å¤ç½‘ç»œå‚æ•°
        if 'network_state_dict' in checkpoint:
            value_function.network.load_state_dict(checkpoint['network_state_dict'])
        
        if 'target_network_state_dict' in checkpoint:
            value_function.target_network.load_state_dict(checkpoint['target_network_state_dict'])
        
        # æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€
        if 'optimizer_state_dict' in checkpoint:
            value_function.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # æ¢å¤å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€
        if 'scheduler_state_dict' in checkpoint:
            value_function.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        # æ¢å¤è®­ç»ƒæ­¥æ•°
        if 'training_step' in checkpoint:
            value_function.training_step = checkpoint['training_step']
        
        # æ¢å¤æŸå¤±å†å²
        if 'training_losses' in checkpoint:
            value_function.training_losses = checkpoint['training_losses']
        
        # æ¢å¤Qå€¼å†å²
        if 'q_values_history' in checkpoint:
            value_function.q_values_history = checkpoint['q_values_history']
        
        episode = checkpoint.get('episode', 0)
        buffer_size = checkpoint.get('experience_buffer_size', 0)
        
        print(f"âœ“ æˆåŠŸåŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
        print(f"  - Episode: {episode}")
        print(f"  - Training step: {value_function.training_step}")
        print(f"  - Experience buffer size: {buffer_size}")
        
        return True
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        return False


def save_q_network_checkpoint(value_function, episode, checkpoint_dir="checkpoints/q_networks"):
    """
    ä¿å­˜Q-networkæ£€æŸ¥ç‚¹çš„é€šç”¨å‡½æ•°
    
    Args:
        value_function: PyTorchChargingValueFunctionå®ä¾‹
        episode: å½“å‰episodeæ•°
        checkpoint_dir: ä¿å­˜ç›®å½•
    
    Returns:
        dict: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    import os
    
    if value_function is None:
        print("âŒ Value functionä¸ºç©ºï¼Œæ— æ³•ä¿å­˜")
        return {}
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # å®šä¹‰ä¿å­˜è·¯å¾„
    paths = {
        'main_network': os.path.join(checkpoint_dir, f"q_network_episode_{episode}.pth"),
        'target_network': os.path.join(checkpoint_dir, f"target_network_episode_{episode}.pth"),
        'full_state': os.path.join(checkpoint_dir, f"full_state_episode_{episode}.pth")
    }
    
    try:
        # ä¿å­˜ä¸»Q-networkå‚æ•°
        torch.save(value_function.network.state_dict(), paths['main_network'])
        
        # ä¿å­˜target networkå‚æ•°
        torch.save(value_function.target_network.state_dict(), paths['target_network'])
        
        # ä¿å­˜å®Œæ•´çŠ¶æ€
        full_state = {
            'episode': episode,
            'training_step': value_function.training_step,
            'network_state_dict': value_function.network.state_dict(),
            'target_network_state_dict': value_function.target_network.state_dict(),
            'optimizer_state_dict': value_function.optimizer.state_dict(),
            'scheduler_state_dict': value_function.scheduler.state_dict(),
            'experience_buffer_size': len(value_function.experience_buffer),
            'training_losses': value_function.training_losses[-100:] if value_function.training_losses else [],
            'q_values_history': value_function.q_values_history[-100:] if value_function.q_values_history else []
        }
        torch.save(full_state, paths['full_state'])
        
        print(f"âœ“ Episode {episode}: æˆåŠŸä¿å­˜Q-networkæ£€æŸ¥ç‚¹")
        print(f"  - Q-network: {paths['main_network']}")
        print(f"  - Target network: {paths['target_network']}")
        print(f"  - Full state: {paths['full_state']}")
        print(f"  - Training step: {value_function.training_step}")
        print(f"  - Experience buffer size: {len(value_function.experience_buffer)}")
        
        return paths
        
    except Exception as e:
        print(f"âŒ Episode {episode}: ä¿å­˜ç½‘ç»œå¤±è´¥: {e}")
        return {}


def list_available_checkpoints(checkpoint_dir="checkpoints/q_networks"):
    """
    åˆ—å‡ºå¯ç”¨çš„æ£€æŸ¥ç‚¹æ–‡ä»¶
    
    Args:
        checkpoint_dir: æ£€æŸ¥ç‚¹ç›®å½•
        
    Returns:
        list: å¯ç”¨çš„å®Œæ•´çŠ¶æ€æ£€æŸ¥ç‚¹åˆ—è¡¨
    """
    import os
    import glob
    
    if not os.path.exists(checkpoint_dir):
        print(f"âŒ æ£€æŸ¥ç‚¹ç›®å½•ä¸å­˜åœ¨: {checkpoint_dir}")
        return []
    
    # å¯»æ‰¾å®Œæ•´çŠ¶æ€æ–‡ä»¶
    pattern = os.path.join(checkpoint_dir, "full_state_episode_*.pth")
    checkpoints = glob.glob(pattern)
    
    # æå–episodeç¼–å·å¹¶æ’åº
    checkpoint_info = []
    for checkpoint in checkpoints:
        try:
            basename = os.path.basename(checkpoint)
            episode_str = basename.replace("full_state_episode_", "").replace(".pth", "")
            episode = int(episode_str)
            checkpoint_info.append((episode, checkpoint))
        except ValueError:
            continue
    
    # æŒ‰episodeæ’åº
    checkpoint_info.sort(key=lambda x: x[0])
    
    if checkpoint_info:
        print(f"âœ“ æ‰¾åˆ° {len(checkpoint_info)} ä¸ªæ£€æŸ¥ç‚¹:")
        for episode, path in checkpoint_info:
            print(f"  - Episode {episode}: {path}")
    else:
        print(f"âŒ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶åœ¨: {checkpoint_dir}")
    
    return checkpoint_info

def set_random_seeds(seed=42):
    """
    è®¾ç½®æ‰€æœ‰éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­ï¼Œç¡®ä¿å®éªŒçš„å¯é‡å¤æ€§
    
    Args:
        seed (int): éšæœºæ•°ç§å­ï¼Œé»˜è®¤ä¸º42
    """
    # Pythonå†…ç½®randomæ¨¡å—
    random.seed(seed)
    
    # NumPyéšæœºæ•°ç”Ÿæˆå™¨
    np.random.seed(seed)
    
    # PyTorchéšæœºæ•°ç”Ÿæˆå™¨  
    torch.manual_seed(seed)
    
    # å¦‚æœä½¿ç”¨CUDAï¼Œè®¾ç½®CUDAéšæœºæ•°ç§å­
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        # ç¡®ä¿CUDAæ“ä½œçš„ç¡®å®šæ€§ï¼ˆå¯èƒ½ä¼šå½±å“æ€§èƒ½ï¼‰
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    
    print(f"âœ“ Random seeds set to {seed} for all generators (Python, NumPy, PyTorch)")


def run_charging_integration_test(adpvalue,num_episodes,use_intense_requests,assignmentgurobi,batch_size=256, num_vehicles = 10):
    """Run charging integration test with EV/AEV analysis"""
    print("=== Starting Enhanced Charging Behavior Integration Test ===")
    
    # è®¾ç½®å…¨å±€éšæœºæ•°ç§å­ï¼Œç¡®ä¿è½¦è¾†åˆå§‹åŒ–ä¸€è‡´
    set_random_seeds(seed=42)
    
    # Create environment with significantly more complexity for better learning
    num_vehicles = num_vehicles
    num_stations = 4
    env = ChargingIntegratedEnvironment(
        num_vehicles=num_vehicles, 
        num_stations=num_stations, 
        random_seed=42,
        use_intense_requests=use_intense_requests
    )
    
    print("âœ“ Fixed initial state setup: Vehicle positions and battery levels will be identical across all episodes")
    print("âœ“ Request generation will vary by episode for learning progression")
    
    # Initialize neural network-based ValueFunction for decision making only if needed
    # Use PyTorchChargingValueFunction with neural network only when ADP > 0 and assignmentgurobi is True
    use_neural_network = adpvalue > 0 and assignmentgurobi
    
    if use_neural_network:
        value_function = PyTorchChargingValueFunction(
            grid_size=env.grid_size, 
            num_vehicles=num_vehicles,
            device='cuda' if torch.cuda.is_available() else 'cpu',  # Use GPU if available
            episode_length=env.episode_length,  # ä¼ é€’æ­£ç¡®çš„episodeé•¿åº¦
            max_requests=10000,  # è®¾ç½®åˆç†çš„æœ€å¤§è¯·æ±‚æ•°
            env=env  # ä¼ é€’ç¯å¢ƒå¼•ç”¨
        )
        # Set the value function in the environment for Q-value calculation
        env.set_value_function(value_function)
    else:
        value_function = None
        
    env.adp_value = adpvalue
    env.assignmentgurobi = assignmentgurobi
    # Exploration parameters for enhanced learning with complex environment
    exploration_episodes = max(1, num_episodes // 2)  # Half episodes for exploration  
    epsilon_start = 0.4  # Higher exploration for complex environment
    epsilon_end = 0.1   # End with 10% random actions
    epsilon_decay = (epsilon_start - epsilon_end) / exploration_episodes
    
    # Enhanced training parameters for complex environment
    training_frequency = 2
    warmup_steps = 100     # Increased warmup for complex environment
    
    print(f"âœ“ Initialized environment with {num_vehicles} vehicles and {num_stations} charging stations")
    if use_neural_network:
        print(f"âœ“ Initialized PyTorchChargingValueFunction with neural network")
        print(f"   - Network parameters: {sum(p.numel() for p in value_function.network.parameters())}")
        print(f"âœ“ Enhanced exploration strategy: {exploration_episodes} episodes with epsilon {epsilon_start:.2f} â†’ {epsilon_end:.2f}")
        print(f"   - Training frequency: every {training_frequency} steps after {warmup_steps} warmup steps")
        print(f"   - Using device: {value_function.device}")
    else:
        print(f"âœ“ Neural network training disabled (ADP={adpvalue}, AssignmentGurobi={assignmentgurobi})")
        print(f"   - Running without neural network training")
    
    # Display vehicle type distribution
    ev_count = sum(1 for v in env.vehicles.values() if v['type'] == 'EV')
    aev_count = sum(1 for v in env.vehicles.values() if v['type'] == 'AEV')
    print(f"âœ“ Vehicle distribution: {ev_count} EV vehicles, {aev_count} AEV vehicles")
    
    # Test parameters
    num_episodes = num_episodes
    results = {
        'Idle_average': [],
        'episode_rewards': [],
        'charging_events': [],
        'episode_detailed_stats': [],  # New: detailed stats for each episode
        'vehicle_visit_stats': [],     # New: vehicle visit patterns for each episode
        'battery_levels': [],
        'environment_stats': [],
        'value_function_losses': [],
        'qvalue_losses': []  # Added: to store all training losses
    }
    
    for episode in range(num_episodes):
        # ä¸ºæ¯ä¸ªepisodeè®¾ç½®è¯·æ±‚ç”Ÿæˆä¸“ç”¨çš„ç§å­ï¼Œç¡®ä¿è¯·æ±‚åºåˆ—çš„å¤šæ ·æ€§
        # ä½†ä¿æŒä¸åŒADPå€¼ä¸‹ç›¸åŒepisodeçš„è¯·æ±‚åºåˆ—ä¸€è‡´
        episode_seed = 32 + episode  # åŸºç¡€ç§å­42åŠ ä¸Šepisodeç¼–å·
        env.set_request_generation_seed(episode_seed)
        print(f"Episode {episode + 1}: Request generation seed set to {episode_seed}")
        
        current_epsilon = max(epsilon_end, epsilon_start - episode * epsilon_decay)
        use_exploration = False
        
        # Reset environment
        states = env.reset()
        episode_reward = 0
        episode_charging_events = []
        episode_losses = []
        
        Idle_list = []
        for step in range(env.episode_length):
            # Generate actions using ValueFunction
            actions = {}
            states_for_training = []
            actions_for_training = []
            current_requests = list(env.active_requests.values())
            actions, storeactions = env.simulate_motion(agents=[], current_requests=current_requests, rebalance=True)
            next_states, rewards, dur_rewards, done, info = env.step(actions,storeactions)
            # Debug: Output step statistics every 100 steps
            if step % 25 == 0:
                stats = env.get_stats()
                active_requests = len(env.active_requests) if hasattr(env, 'active_requests') else 0
                assigned_vehicles = len([v for v in env.vehicles.values() if v['assigned_request'] is not None])
                charging_vehicles = len([v for v in env.vehicles.values() if v['charging_station'] is not None])
                onboard = len([v for v in env.vehicles.values() if v['passenger_onboard'] is not None])
                idlecar = len([v for v in env.vehicles.values() if  v.get('idle_target') is not None ])
                waitcar = len([v for v in env.vehicles.values() if  v.get('is_stationary') is True ])
                movecharge = len([v for v in env.vehicles.values() if v.get('charging_target') is not None])
                target_location_v = len([v for v in env.vehicles.values() if v.get('target_location') is not None])
                idle_vehicles = len([v for v in env.vehicles.values() 
                                   if v['assigned_request'] is None and v['passenger_onboard'] is None and v['charging_station'] is None and v['target_location'] is None])
                step_reward = sum(rewards.values())
                print(f"Step {step}: Active requests: {active_requests}, Assigned: {assigned_vehicles}, Onboard: {onboard}, Charging: {charging_vehicles}, Idle: {idlecar}, waitcar: {waitcar}, movecharge: {movecharge}, Idle Vehicles: {idle_vehicles}, Step reward: {step_reward:.2f}")
                Idle_list.append(idle_vehicles)
                # Neural network monitoring (if using neural network)
                if use_neural_network and hasattr(value_function, 'training_losses') and value_function.training_losses:
                    recent_loss = value_function.training_losses[-1] if value_function.training_losses else 0.0
                    buffer_size = len(value_function.experience_buffer)
                    training_step = value_function.training_step
                    
                    # Sample some Q-values to show the actual raw values used by Gurobi
                    if buffer_size > 0:
                        # Get a sample Q-value to demonstrate what Gurobi actually uses
                        sample_vehicle_id = list(env.vehicles.keys())[0] if env.vehicles else 0
                        sample_location = list(env.vehicles.values())[0]['location'] if env.vehicles else 0
                        sample_battery = list(env.vehicles.values())[0]['battery'] if env.vehicles else 1.0
                        
                        try:
                            # Test different action types - these are the raw Q-values Gurobi uses
                            idle_q = value_function.get_idle_q_value(sample_vehicle_id, sample_location, sample_battery, current_time=step)
                            assign_q = value_function.get_q_value(sample_vehicle_id, "assign_1", sample_location, sample_location+1, current_time=step, battery_level=sample_battery)
                            charge_q = value_function.get_q_value(sample_vehicle_id, "charge_1", sample_location, sample_location+5, current_time=step, battery_level=sample_battery)
                            
                            print(f"  Neural Network Status:")
                            print(f"    Training step: {training_step}, Buffer: {buffer_size}, Recent loss: {recent_loss:.4f}")
                            print(f"    Raw Q-values (no normalization): Idle={idle_q:.3f}, Assign={assign_q:.3f}, Charge={charge_q:.3f}")
                            print(f"    Note: Gurobi uses these raw Q-values directly in optimization objective")
                            
                            # æ·»åŠ ç»éªŒæ•°æ®åˆ†æ
                            if step > 100 and step % 100 == 0:  # æ¯100æ­¥åˆ†æä¸€æ¬¡
                                exp_analysis = value_function.analyze_experience_data()
                                if exp_analysis:
                                    reward_stats = exp_analysis['reward_stats']
                                    action_stats = exp_analysis['action_stats']
                                    print(f"    ğŸ“Š Experience Data Analysis (last 100 steps):")
                                    print(f"      Reward Distribution: +{reward_stats['positive_ratio']:.1%} | 0{reward_stats['neutral_ratio']:.1%} | -{reward_stats['negative_ratio']:.1%}")
                                    print(f"      Mean Rewards: Overall={reward_stats['mean_reward']:.2f}, Assign={action_stats['assign_mean_reward']:.2f}, Charge={action_stats['charge_mean_reward']:.2f}, Idle={action_stats['idle_mean_reward']:.2f}")
                                    print(f"      Action Success Rates: Assign={action_stats['assign_positive_ratio']:.1%}, Charge={action_stats['charge_positive_ratio']:.1%}, Idle={action_stats['idle_positive_ratio']:.1%}")
                                    
                        except Exception as e:
                            print(f"  Neural Network Status: Training step: {training_step}, Buffer: {buffer_size}, Recent loss: {recent_loss:.4f}")
                            print(f"    Error getting sample Q-values: {e}")
                else:
                    print(f"  Neural Network: {'Not training yet' if use_neural_network else 'Disabled'}")
            
            # Note: Q-learning experience storage is now handled automatically in env.step()
            # This ensures consistency between traditional Q-table and neural network training
            
            # Enhanced training: much more frequent training for better learning (only if using neural network)
            if use_neural_network and len(value_function.experience_buffer) >= warmup_steps:
                # Train more frequently based on our new parameters
                if step % training_frequency == 0:
                    training_loss = value_function.train_step(batch_size=batch_size)  # Larger batch
                    if training_loss > 0:
                        episode_losses.append(training_loss)
                
            episode_reward += sum(rewards.values())
            episode_charging_events.extend(info.get('charging_events', []))
            
            if done:
                break
        results['Idle_average'].append(sum(Idle_list)/len(Idle_list) if Idle_list else 0)
        results['episode_rewards'].append(episode_reward)
        results['charging_events'].extend(episode_charging_events)
        results['value_function_losses'].append(np.mean(episode_losses) if episode_losses else 0.0)
        results['qvalue_losses'].extend(episode_losses)  # Fixed: extend instead of assign
        # Record environment statistics
        stats = env.get_stats()
        results['active_requests'] = stats['active_requests']
        results['environment_stats'].append(stats)
        results['battery_levels'].append(stats['average_battery'])
        results['completed_requests'] = stats['completed_requests']
        results['avg_requestvalue'] = stats['completed_orders_req']
        # Collect detailed episode statistics
        episode_stats = env.get_episode_stats()
        episode_stats['episode_number'] = episode + 1
        episode_stats['episode_reward'] = episode_reward
        episode_stats['charging_events_count'] = len(episode_charging_events)
        
        # Output rebalancing assignment statistics
        rebalancing_calls = episode_stats.get('total_rebalancing_calls', 0)
        total_assignments = episode_stats.get('total_rebalancing_assignments', 0)
        avg_assignments = episode_stats.get('avg_rebalancing_assignments_per_call', 0)
        avg_whole = episode_stats.get('avg_rebalancing_assignments_per_whole', 0)
        print(f"Episode {episode + 1} Completed:")
        print(f"  Reward: {episode_reward:.2f}")
        print(f"  Orders: Total={episode_stats['total_orders']}, Accepted={episode_stats['accepted_orders']}, Completed={episode_stats['completed_orders']}, Rejected={episode_stats['rejected_orders']}")
        print(f"  Battery: {episode_stats['avg_battery_level']:.2f}")
        print(f"  Rebalancing: Calls={rebalancing_calls}, Total Assignments={total_assignments}, Avg Assignments={avg_assignments:.2f}, Avg Rebalance Whole={avg_whole:.2f}")

        # Add neural network Q-value summary
        if use_neural_network:
            idle_q = episode_stats.get('sample_idle_q_value', 0.0)
            assign_q = episode_stats.get('sample_assign_q_value', 0.0)
            charge_q = episode_stats.get('sample_charge_q_value', 0.0)
            nn_loss = episode_stats.get('neural_network_loss', 0.0)
            print(f"  Neural Network: Loss={nn_loss:.4f}, Q-values(Gurobi): Idle={idle_q:.3f}, Assign={assign_q:.3f}, Charge={charge_q:.3f}")
        # Only record neural network metrics if using neural network
        if use_neural_network:
            episode_stats['neural_network_loss'] = np.mean(episode_losses) if episode_losses else 0.0
            episode_stats['neural_network_loss_std'] = np.std(episode_losses) if episode_losses else 0.0
            episode_stats['training_steps_in_episode'] = len(episode_losses)
            
            # Sample Q-values for different action types (actual values used by Gurobi)
            if len(value_function.experience_buffer) > 0:
                try:
                    sample_vehicle_id = list(env.vehicles.keys())[0] if env.vehicles else 0
                    sample_location = list(env.vehicles.values())[0]['location'] if env.vehicles else 0
                    sample_battery = list(env.vehicles.values())[0]['battery'] if env.vehicles else 1.0
                    
                    # Get sample Q-values for statistics
                    idle_q = value_function.get_idle_q_value(sample_vehicle_id, sample_location, sample_battery, current_time=env.current_time)
                    assign_q = value_function.get_q_value(sample_vehicle_id, "assign_1", sample_location, sample_location+1, current_time=env.current_time, battery_level=sample_battery)
                    charge_q = value_function.get_q_value(sample_vehicle_id, "charge_1", sample_location, sample_location+5, current_time=env.current_time, battery_level=sample_battery)
                    
                    episode_stats['sample_idle_q_value'] = idle_q
                    episode_stats['sample_assign_q_value'] = assign_q
                    episode_stats['sample_charge_q_value'] = charge_q
                    
                except Exception as e:
                    episode_stats['sample_idle_q_value'] = 0.0
                    episode_stats['sample_assign_q_value'] = 0.0
                    episode_stats['sample_charge_q_value'] = 0.0
            else:
                episode_stats['sample_idle_q_value'] = 0.0
                episode_stats['sample_assign_q_value'] = 0.0
                episode_stats['sample_charge_q_value'] = 0.0
        else:
            episode_stats['neural_network_loss'] = 0.0
            episode_stats['neural_network_loss_std'] = 0.0
            episode_stats['training_steps_in_episode'] = 0
            episode_stats['sample_idle_q_value'] = 0.0
            episode_stats['sample_assign_q_value'] = 0.0
            episode_stats['sample_charge_q_value'] = 0.0
        results['episode_detailed_stats'].append(episode_stats)
        
        # Analyze charging usage history for this episode
        if 'charging_usage_history' in episode_stats and episode_stats['charging_usage_history']:
            charging_history = episode_stats['charging_usage_history']
            avg_usage = sum(h['vehicles_per_station'] for h in charging_history) / len(charging_history)
            max_usage = max(h['vehicles_per_station'] for h in charging_history)
            min_usage = min(h['vehicles_per_station'] for h in charging_history)
            print(f"  Charging History: {len(charging_history)} time steps, Avg: {avg_usage:.2f}, Max: {max_usage:.2f}, Min: {min_usage:.2f} vehicles/station")
        
        # Analyze vehicle visit patterns for this episode
        vehicle_visit_stats = analyze_vehicle_visit_patterns(env)
        results['vehicle_visit_stats'].append(vehicle_visit_stats)
        

    print("\n=== Integration Test Complete ===")
    if use_neural_network:
        print(f"âœ“ Neural Network ValueFunction trained over {num_episodes} episodes")
        print(f"âœ“ Final average training loss: {np.mean(results['value_function_losses']):.4f}")
        print(f"âœ“ Neural network has {sum(p.numel() for p in value_function.network.parameters())} parameters")
    else:
        print(f"âœ“ Test completed without neural network training")
        print(f"âœ“ Used traditional Q-table approach")
    
    # Create results directory for analysis - choose directory based on assignmentgurobi
    if assignmentgurobi:
        results_dir = Path("results/integrated_tests")
    else:
        results_dir = Path("results/integrated_tests_h")
    results_dir.mkdir(parents=True, exist_ok=True)
    print(f"âœ“ Results will be saved to: {results_dir}")
    
    # Save detailed episode statistics to Excel including vehicle visit patterns
    excel_path, spatial_path = save_episode_stats_to_excel(env, results['episode_detailed_stats'], results_dir, results.get('vehicle_visit_stats'))
    
    # Store file paths in results for reference
    results['excel_path'] = excel_path
    results['spatial_image_path'] = spatial_path
    
    return results, env


# =============================================================================
# NEW WORKFLOW: EV-AEV Separate Q-Network Training
# =============================================================================

def run_ev_aev_separate_training(adpvalue, num_episodes, use_intense_requests, batch_size=256, num_vehicles=10):
    """
    æ–°çš„è®­ç»ƒ workflow: EV å’Œ AEV åˆ†å¼€è®­ç»ƒ
    
    Workflow:
    1. ç”Ÿæˆè®¢å•åå…ˆå¯¹ EV åˆ†é…
    2. EV æ‹’ç»çš„è®¢å• + å‰©ä½™è®¢å•åˆ†é…ç»™ AEV
    3. æ‹’ç»è®¢å•çš„ EV æœ‰æƒ©ç½šæ—¶é—´ï¼ˆå†·å´æœŸï¼‰
    4. EV æŒ‰æ¦‚ç‡ç§»åŠ¨åˆ°å……ç”µç«™æˆ–å…¶ä»–çƒ­ç‚¹åŒºåŸŸ
    5. EV Q-network åªè®­ç»ƒè®¢å•åˆ†é…çš„ Q-value
    6. AEV Q-network è®­ç»ƒæ‰€æœ‰åŠ¨ä½œï¼ˆåˆ†é…ã€å……ç”µã€idleï¼‰
    
    Args:
        adpvalue: ADP ç³»æ•°
        num_episodes: è®­ç»ƒå›åˆæ•°
        use_intense_requests: æ˜¯å¦ä½¿ç”¨é›†ä¸­å¼è¯·æ±‚ç”Ÿæˆ
        batch_size: è®­ç»ƒæ‰¹æ¬¡å¤§å°
        num_vehicles: è½¦è¾†æ€»æ•°
    """
    print("=" * 70)
    print("ğŸš— NEW WORKFLOW: EV-AEV Separate Q-Network Training")
    print("=" * 70)
    print("ğŸ“‹ Workflow Description:")
    print("   1. Orders first assigned to EV vehicles")
    print("   2. Rejected orders + remaining orders â†’ AEV vehicles")
    print("   3. Rejecting EVs get penalty cooldown time")
    print("   4. EVs probabilistically move to charging/hotspots")
    print("   5. EV Q-network: only trains on order assignment")
    print("   6. AEV Q-network: trains on all actions")
    print("-" * 70)
    
    # è®¾ç½®éšæœºç§å­
    set_random_seeds(seed=42)
    
    # åˆ›å»ºç¯å¢ƒ
    num_stations = 4
    env = ChargingIntegratedEnvironment(
        num_vehicles=num_vehicles,
        num_stations=num_stations,
        random_seed=42,
        use_intense_requests=use_intense_requests
    )
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"âœ“ Using device: {device}")
    
    # åˆ†ç¦» EV å’Œ AEV è½¦è¾†
    ev_vehicles = {vid: v for vid, v in env.vehicles.items() if v['type'] == 1}
    aev_vehicles = {vid: v for vid, v in env.vehicles.items() if v['type'] == 2}
    print(f"âœ“ Vehicle distribution: {len(ev_vehicles)} EV, {len(aev_vehicles)} AEV")
    
    # åˆ›å»ºåˆ†å¼€çš„ Q-networks
    use_neural_network = adpvalue > 0
    
    if use_neural_network:
        # EV Q-network: åªç”¨äºè®¢å•åˆ†é…å†³ç­–
        ev_value_function = PyTorchChargingValueFunction(
            grid_size=env.grid_size,
            num_vehicles=len(ev_vehicles),
            device=device,
            episode_length=env.episode_length,
            max_requests=10000,
            env=env
        )
        
        # AEV Q-network: ç”¨äºæ‰€æœ‰å†³ç­–ï¼ˆåˆ†é…ã€å……ç”µã€idleï¼‰
        aev_value_function = PyTorchChargingValueFunction(
            grid_size=env.grid_size,
            num_vehicles=len(aev_vehicles),
            device=device,
            episode_length=env.episode_length,
            max_requests=10000,
            env=env
        )
        
        # è®¾ç½®ç¯å¢ƒçš„ value_functionï¼ˆç”¨äºå…¼å®¹ç°æœ‰ä»£ç ï¼‰
        env.set_value_function(aev_value_function)
        
        print(f"âœ“ EV Q-network initialized (assignment only)")
        print(f"   Parameters: {sum(p.numel() for p in ev_value_function.network.parameters())}")
        print(f"âœ“ AEV Q-network initialized (full actions)")
        print(f"   Parameters: {sum(p.numel() for p in aev_value_function.network.parameters())}")
    else:
        ev_value_function = None
        aev_value_function = None
        print(f"âœ“ Neural network training disabled")
    
    env.adp_value = adpvalue
    
    # EV æƒ©ç½šå‚æ•°
    ev_rejection_penalty_time = 3  # æ‹’ç»åçš„å†·å´æ—¶é—´æ­¥æ•°
    ev_rejection_cooldown = {}  # {vehicle_id: remaining_cooldown_steps}
    
    # EV ç§»åŠ¨æ¦‚ç‡å‚æ•°
    ev_charging_probability = 0.3  # ç§»åŠ¨åˆ°å……ç”µç«™çš„æ¦‚ç‡
    ev_hotspot_probability = 0.5   # ç§»åŠ¨åˆ°çƒ­ç‚¹åŒºåŸŸçš„æ¦‚ç‡
    # å‰©ä½™æ¦‚ç‡ï¼šéšæœºç§»åŠ¨
    
    # è®­ç»ƒå‚æ•°
    exploration_episodes = max(1, num_episodes // 2)
    epsilon_start = 0.4
    epsilon_end = 0.1
    epsilon_decay = (epsilon_start - epsilon_end) / exploration_episodes
    warmup_steps = 100
    training_frequency = 2
    
    # ç»“æœå­˜å‚¨
    results = {
        'episode_rewards': [],
        'ev_rewards': [],
        'aev_rewards': [],
        'ev_assignments': [],
        'ev_rejections': [],
        'aev_assignments': [],
        'ev_losses': [],
        'aev_losses': [],
        'completed_orders': [],
        'episode_detailed_stats': [],
        'vehicle_visit_stats': []
    }
    
    for episode in range(num_episodes):
        # è®¾ç½®æ¯ä¸ª episode çš„è¯·æ±‚ç”Ÿæˆç§å­
        episode_seed = 42 + episode
        env.set_request_generation_seed(episode_seed)
        
        current_epsilon = max(epsilon_end, epsilon_start - episode * epsilon_decay)
        
        # é‡ç½®ç¯å¢ƒ
        states = env.reset()
        
        # é‡ç½®å†·å´è®¡æ—¶å™¨
        ev_rejection_cooldown = {vid: 0 for vid in ev_vehicles.keys()}
        
        episode_reward = 0
        ev_episode_reward = 0
        aev_episode_reward = 0
        ev_assignment_count = 0
        ev_rejection_count = 0
        aev_assignment_count = 0
        ev_episode_losses = []
        aev_episode_losses = []
        
        print(f"\n{'='*60}")
        print(f"Episode {episode + 1}/{num_episodes}")
        print(f"{'='*60}")
        
        for step in range(env.episode_length):
            # ç”Ÿæˆæ–°è¯·æ±‚
            if use_intense_requests:
                new_requests = env._generate_intense_requests()
            else:
                new_requests = env._generate_random_requests()
            
            current_requests = list(env.active_requests.values())
            
            # ========================================
            # PHASE 1: EV Assignment (Priority)
            # ========================================
            ev_actions = {}
            ev_assigned_requests = set()
            rejected_requests = []
            
            # è·å–å¯ç”¨çš„ EVï¼ˆä¸åœ¨å†·å´æœŸï¼‰
            available_evs = [vid for vid, v in ev_vehicles.items() 
                           if ev_rejection_cooldown.get(vid, 0) <= 0
                           and env.vehicles[vid]['assigned_request'] is None
                           and env.vehicles[vid]['passenger_onboard'] is None
                           and env.vehicles[vid]['charging_station'] is None]
            
            # æŒ‰è·ç¦»æ’åºè¯·æ±‚ç»™æ¯ä¸ª EV
            for ev_id in available_evs:
                if not current_requests:
                    break
                    
                ev_vehicle = env.vehicles[ev_id]
                ev_loc = ev_vehicle['location']
                ev_battery = ev_vehicle['battery']
                
                # æŒ‰è·ç¦»æ’åºå¯ç”¨è¯·æ±‚
                sorted_requests = sorted(
                    [r for r in current_requests if r.request_id not in ev_assigned_requests],
                    key=lambda r: abs(r.pickup - ev_loc)
                )
                
                if not sorted_requests:
                    continue
                
                # é€‰æ‹©æœ€è¿‘çš„è¯·æ±‚
                best_request = sorted_requests[0]
                
                # æ£€æŸ¥ç”µé‡æ˜¯å¦è¶³å¤Ÿå®Œæˆè®¢å•
                pickup_distance = abs(best_request.pickup - ev_loc)
                dropoff_distance = abs(best_request.dropoff - best_request.pickup)
                total_distance = pickup_distance + dropoff_distance
                battery_needed = total_distance * env.battery_consum
                
                if ev_battery < battery_needed + 0.1:  # ä¿ç•™10%ç”µé‡ä½™é‡
                    # ç”µé‡ä¸è¶³ï¼Œç§»åŠ¨åˆ°å……ç”µç«™
                    ev_actions[ev_id] = ('charge', None)
                    continue
                
                # å°è¯•åˆ†é…è®¢å•
                if env._should_reject_request(ev_id, best_request):
                    # EV æ‹’ç»è®¢å•
                    ev_rejection_count += 1
                    rejected_requests.append(best_request)
                    
                    # è®¾ç½®æ‹’ç»æƒ©ç½šå†·å´æ—¶é—´
                    ev_rejection_cooldown[ev_id] = ev_rejection_penalty_time
                    
                    # å­˜å‚¨æ‹’ç»ç»éªŒåˆ° EV Q-networkï¼ˆè´Ÿå¥–åŠ±ï¼‰
                    if use_neural_network and ev_value_function is not None:
                        _store_ev_rejection_experience(
                            ev_value_function, ev_id, ev_vehicle, 
                            best_request, env, rejection_penalty=-5.0
                        )
                    
                    # æŒ‰æ¦‚ç‡å†³å®š EV ä¸‹ä¸€æ­¥åŠ¨ä½œ
                    rand_val = random.random()
                    if rand_val < ev_charging_probability:
                        ev_actions[ev_id] = ('charge', None)
                    elif rand_val < ev_charging_probability + ev_hotspot_probability:
                        ev_actions[ev_id] = ('hotspot', None)
                    else:
                        ev_actions[ev_id] = ('random_move', None)
                else:
                    # EV æ¥å—è®¢å•
                    ev_assignment_count += 1
                    ev_assigned_requests.add(best_request.request_id)
                    ev_actions[ev_id] = ('assign', best_request)
                    
                    # å­˜å‚¨åˆ†é…ç»éªŒåˆ° EV Q-networkï¼ˆæ­£å¥–åŠ±ï¼‰
                    if use_neural_network and ev_value_function is not None:
                        _store_ev_assignment_experience(
                            ev_value_function, ev_id, ev_vehicle,
                            best_request, env, assignment_reward=best_request.final_value
                        )
            
            # ========================================
            # PHASE 2: AEV Assignment (Remaining + Rejected)
            # ========================================
            aev_actions = {}
            
            # åˆå¹¶å‰©ä½™è®¢å•å’Œè¢«æ‹’ç»çš„è®¢å•
            remaining_requests = [r for r in current_requests 
                                if r.request_id not in ev_assigned_requests]
            remaining_requests.extend(rejected_requests)
            
            # è·å–å¯ç”¨çš„ AEV
            available_aevs = [vid for vid, v in aev_vehicles.items()
                            if env.vehicles[vid]['assigned_request'] is None
                            and env.vehicles[vid]['passenger_onboard'] is None
                            and env.vehicles[vid]['charging_station'] is None]
            
            # AEV ä½¿ç”¨ Q-value é€‰æ‹©æœ€ä¼˜è®¢å•
            for aev_id in available_aevs:
                if not remaining_requests:
                    break
                    
                aev_vehicle = env.vehicles[aev_id]
                aev_loc = aev_vehicle['location']
                aev_battery = aev_vehicle['battery']
                
                # æŒ‰ Q-value æ’åºè¯·æ±‚ï¼ˆå¦‚æœæœ‰ç¥ç»ç½‘ç»œï¼‰
                if use_neural_network and aev_value_function is not None:
                    request_q_values = []
                    for req in remaining_requests:
                        q_val = aev_value_function.get_assignment_q_value(
                            aev_id, req.request_id, aev_loc, req.pickup,
                            env.current_time, len(available_aevs), len(remaining_requests),
                            aev_battery, req.final_value
                        )
                        request_q_values.append((req, q_val))
                    
                    # æŒ‰ Q-value é™åºæ’åº
                    request_q_values.sort(key=lambda x: x[1], reverse=True)
                    sorted_requests = [r for r, _ in request_q_values]
                else:
                    # å¯å‘å¼ï¼šæŒ‰ä»·å€¼/è·ç¦»æ¯”æ’åº
                    sorted_requests = sorted(
                        remaining_requests,
                        key=lambda r: r.final_value / (abs(r.pickup - aev_loc) + 1),
                        reverse=True
                    )
                
                if not sorted_requests:
                    continue
                
                # é€‰æ‹©æœ€ä¼˜è¯·æ±‚
                best_request = sorted_requests[0]
                
                # æ£€æŸ¥ç”µé‡
                pickup_distance = abs(best_request.pickup - aev_loc)
                dropoff_distance = abs(best_request.dropoff - best_request.pickup)
                total_distance = pickup_distance + dropoff_distance
                battery_needed = total_distance * env.battery_consum
                
                if aev_battery < battery_needed + 0.1:
                    # ç”µé‡ä¸è¶³ï¼Œé€‰æ‹©å……ç”µ
                    aev_actions[aev_id] = ('charge', None)
                else:
                    # AEV åˆ†é…è®¢å•ï¼ˆAEV ä¸ä¼šæ‹’ç»ï¼‰
                    aev_assignment_count += 1
                    remaining_requests.remove(best_request)
                    aev_actions[aev_id] = ('assign', best_request)
            
            # ========================================
            # PHASE 3: Execute Actions
            # ========================================
            all_actions = {}
            storeactions = {}
            
            # æ‰§è¡Œ EV åŠ¨ä½œ
            for ev_id, (action_type, action_data) in ev_actions.items():
                action = _create_action_from_type(env, ev_id, action_type, action_data)
                if action:
                    all_actions[ev_id] = action
                    storeactions[ev_id] = (action_type, action_data)
            
            # æ‰§è¡Œ AEV åŠ¨ä½œ
            for aev_id, (action_type, action_data) in aev_actions.items():
                action = _create_action_from_type(env, aev_id, action_type, action_data)
                if action:
                    all_actions[aev_id] = action
                    storeactions[aev_id] = (action_type, action_data)
            
            # å¤„ç†æ²¡æœ‰åŠ¨ä½œçš„è½¦è¾†ï¼ˆä½¿ç”¨ç¯å¢ƒçš„ simulate_motionï¼‰
            unassigned_vehicles = set(env.vehicles.keys()) - set(all_actions.keys())
            if unassigned_vehicles:
                # è®©ç¯å¢ƒå¤„ç†å‰©ä½™è½¦è¾†
                env_actions, env_storeactions = env.simulate_motion(
                    agents=[], current_requests=remaining_requests, rebalance=True
                )
                for vid in unassigned_vehicles:
                    if vid in env_actions:
                        all_actions[vid] = env_actions[vid]
                        storeactions[vid] = env_storeactions.get(vid)
            
            # æ‰§è¡Œç¯å¢ƒæ­¥è¿›
            next_states, rewards, dur_rewards, done, info = env.step(all_actions, storeactions)
            
            # æ›´æ–°å†·å´è®¡æ—¶å™¨
            for ev_id in ev_rejection_cooldown:
                if ev_rejection_cooldown[ev_id] > 0:
                    ev_rejection_cooldown[ev_id] -= 1
            
            # ç´¯è®¡å¥–åŠ±
            for vid, reward in rewards.items():
                episode_reward += reward
                if vid in ev_vehicles:
                    ev_episode_reward += reward
                else:
                    aev_episode_reward += reward
            
            # ========================================
            # PHASE 4: Training
            # ========================================
            if use_neural_network and step >= warmup_steps and step % training_frequency == 0:
                # è®­ç»ƒ EV Q-networkï¼ˆåªç”¨åˆ†é…ç»éªŒï¼‰
                if len(ev_value_function.experience_buffer) >= batch_size:
                    ev_loss = ev_value_function.train_step(batch_size=batch_size)
                    if ev_loss > 0:
                        ev_episode_losses.append(ev_loss)
                
                # è®­ç»ƒ AEV Q-networkï¼ˆæ‰€æœ‰ç»éªŒï¼‰
                if len(aev_value_function.experience_buffer) >= batch_size:
                    aev_loss = aev_value_function.train_step(batch_size=batch_size)
                    if aev_loss > 0:
                        aev_episode_losses.append(aev_loss)
            
            # è¾“å‡ºçŠ¶æ€
            if step % 25 == 0:
                idle_count = len([v for v in env.vehicles.values() 
                                if v['assigned_request'] is None 
                                and v['passenger_onboard'] is None 
                                and v['charging_station'] is None])
                print(f"Step {step}: Requests={len(env.active_requests)}, "
                      f"EV_Assign={ev_assignment_count}, EV_Reject={ev_rejection_count}, "
                      f"AEV_Assign={aev_assignment_count}, Idle={idle_count}")
            
            if done:
                break
        
        # è®°å½• episode ç»“æœ
        results['episode_rewards'].append(episode_reward)
        results['ev_rewards'].append(ev_episode_reward)
        results['aev_rewards'].append(aev_episode_reward)
        results['ev_assignments'].append(ev_assignment_count)
        results['ev_rejections'].append(ev_rejection_count)
        results['aev_assignments'].append(aev_assignment_count)
        results['ev_losses'].append(np.mean(ev_episode_losses) if ev_episode_losses else 0.0)
        results['aev_losses'].append(np.mean(aev_episode_losses) if aev_episode_losses else 0.0)
        
        stats = env.get_stats()
        results['completed_orders'].append(stats.get('completed_orders', 0))
        
        # æ”¶é›†è¯¦ç»†ç»Ÿè®¡
        episode_stats = env.get_episode_stats()
        episode_stats['ev_assignments'] = ev_assignment_count
        episode_stats['ev_rejections'] = ev_rejection_count
        episode_stats['aev_assignments'] = aev_assignment_count
        episode_stats['ev_reward'] = ev_episode_reward
        episode_stats['aev_reward'] = aev_episode_reward
        results['episode_detailed_stats'].append(episode_stats)
        
        # è½¦è¾†è®¿é—®æ¨¡å¼
        vehicle_visit_stats = analyze_vehicle_visit_patterns(env)
        results['vehicle_visit_stats'].append(vehicle_visit_stats)
        
        print(f"\nEpisode {episode + 1} Summary:")
        print(f"  Total Reward: {episode_reward:.2f} (EV: {ev_episode_reward:.2f}, AEV: {aev_episode_reward:.2f})")
        print(f"  Assignments: EV={ev_assignment_count}, AEV={aev_assignment_count}")
        print(f"  EV Rejections: {ev_rejection_count}")
        print(f"  Completed Orders: {stats.get('completed_orders', 0)}")
        if use_neural_network:
            print(f"  Losses: EV={np.mean(ev_episode_losses) if ev_episode_losses else 0:.4f}, "
                  f"AEV={np.mean(aev_episode_losses) if aev_episode_losses else 0:.4f}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if use_neural_network and episode % 10 == 0:
            checkpoint_dir = "checkpoints/ev_aev_separate"
            os.makedirs(checkpoint_dir, exist_ok=True)
            torch.save(ev_value_function.network.state_dict(), 
                      f"{checkpoint_dir}/ev_network_ep{episode}.pth")
            torch.save(aev_value_function.network.state_dict(), 
                      f"{checkpoint_dir}/aev_network_ep{episode}.pth")
            print(f"  âœ“ Saved checkpoints")
    
    print("\n" + "=" * 70)
    print("ğŸ‰ EV-AEV Separate Training Complete!")
    print("=" * 70)
    print(f"Total Episodes: {num_episodes}")
    print(f"Average Reward: {np.mean(results['episode_rewards']):.2f}")
    print(f"Average EV Assignments: {np.mean(results['ev_assignments']):.1f}")
    print(f"Average EV Rejections: {np.mean(results['ev_rejections']):.1f}")
    print(f"Average AEV Assignments: {np.mean(results['aev_assignments']):.1f}")
    print(f"Average Completed Orders: {np.mean(results['completed_orders']):.1f}")
    
    # ä¿å­˜ç»“æœ
    results_dir = Path("results/ev_aev_separate")
    results_dir.mkdir(parents=True, exist_ok=True)
    
    excel_path, spatial_path = save_episode_stats_to_excel(
        env, results['episode_detailed_stats'], results_dir, 
        results.get('vehicle_visit_stats')
    )
    results['excel_path'] = excel_path
    results['spatial_image_path'] = spatial_path
    
    return results, env


def _store_ev_rejection_experience(value_function, vehicle_id, vehicle, request, env, rejection_penalty=-5.0):
    """å­˜å‚¨ EV æ‹’ç»è®¢å•çš„ç»éªŒ"""
    experience = {
        'vehicle_id': vehicle_id,
        'vehicle_location': vehicle['location'],
        'target_location': request.pickup,
        'current_time': env.current_time,
        'other_vehicles': len([v for v in env.vehicles.values() if v['assigned_request'] is None]),
        'num_requests': len(env.active_requests),
        'battery_level': vehicle['battery'],
        'next_battery_level': vehicle['battery'],
        'request_value': request.final_value,
        'action_type': 'assign_rejected',
        'reward': rejection_penalty,
        'done': False
    }
    value_function.experience_buffer.append(experience)


def _store_ev_assignment_experience(value_function, vehicle_id, vehicle, request, env, assignment_reward):
    """å­˜å‚¨ EV æˆåŠŸåˆ†é…è®¢å•çš„ç»éªŒ"""
    experience = {
        'vehicle_id': vehicle_id,
        'vehicle_location': vehicle['location'],
        'target_location': request.pickup,
        'current_time': env.current_time,
        'other_vehicles': len([v for v in env.vehicles.values() if v['assigned_request'] is None]),
        'num_requests': len(env.active_requests),
        'battery_level': vehicle['battery'],
        'next_battery_level': vehicle['battery'] - abs(request.dropoff - request.pickup) * env.battery_consum,
        'request_value': request.final_value,
        'action_type': 'assign',
        'reward': assignment_reward,
        'done': False
    }
    value_function.experience_buffer.append(experience)


def _create_action_from_type(env, vehicle_id, action_type, action_data):
    """æ ¹æ®åŠ¨ä½œç±»å‹åˆ›å»ºåŠ¨ä½œå¯¹è±¡"""
    vehicle = env.vehicles[vehicle_id]
    
    if action_type == 'assign' and action_data is not None:
        # åˆ†é…è®¢å•
        request = action_data
        env._assign_request_to_vehicle(vehicle_id, request.request_id)
        # è®¾ç½®ç›®æ ‡ä½ç½®ä¸º pickup
        pickup_x = request.pickup % env.grid_size
        pickup_y = request.pickup // env.grid_size
        vehicle['target_location'] = (pickup_x, pickup_y)
        return ServiceAction(vehicle_id=vehicle_id, requests={request})
    
    elif action_type == 'charge':
        # ç§»åŠ¨åˆ°å……ç”µç«™
        if hasattr(env, 'charging_manager') and env.charging_manager.stations:
            # æ‰¾æœ€è¿‘çš„å……ç”µç«™
            vehicle_loc = vehicle['location']
            nearest_station = min(
                env.charging_manager.stations.values(),
                key=lambda s: abs(s.location - vehicle_loc)
            )
            env._move_vehicle_to_charging_station(vehicle_id, nearest_station.id)
            return ChargingAction(vehicle_id=vehicle_id, charging_station_id=nearest_station.id)
        return None
    
    elif action_type == 'hotspot':
        # ç§»åŠ¨åˆ°çƒ­ç‚¹åŒºåŸŸ
        if hasattr(env, 'hotspot_locations') and env.hotspot_locations:
            vehicle_loc = vehicle['location']
            vehicle_x = vehicle_loc % env.grid_size
            vehicle_y = vehicle_loc // env.grid_size
            
            # æ‰¾æœ€è¿‘çš„çƒ­ç‚¹
            nearest_hotspot = min(
                env.hotspot_locations,
                key=lambda h: abs(h[0] - vehicle_x) + abs(h[1] - vehicle_y)
            )
            vehicle['target_location'] = nearest_hotspot
            vehicle['idle_target'] = nearest_hotspot[1] * env.grid_size + nearest_hotspot[0]
            return Action(vehicle_id=vehicle_id, action_type='idle')
        return None
    
    elif action_type == 'random_move':
        # éšæœºç§»åŠ¨
        new_x = random.randint(0, env.grid_size - 1)
        new_y = random.randint(0, env.grid_size - 1)
        vehicle['target_location'] = (new_x, new_y)
        vehicle['idle_target'] = new_y * env.grid_size + new_x
        return Action(vehicle_id=vehicle_id, action_type='idle')
    
    return None


def run_charging_integration_test_threshold(adpvalue,num_episodes,use_intense_requests,assignmentgurobi,batch_size=256,heuristic_battery_threshold = 0.5, num_vehicles = 10):
    """Run charging integration test with EV/AEV analysis"""
    print("=== Starting Enhanced Charging Behavior Integration Test ===")
    
    # è®¾ç½®å…¨å±€éšæœºæ•°ç§å­ï¼Œç¡®ä¿è½¦è¾†åˆå§‹åŒ–ä¸€è‡´
    set_random_seeds(seed=42)
    
    # Create environment with significantly more complexity for better learning
    num_vehicles = num_vehicles
    num_stations = 4
    env = ChargingIntegratedEnvironment(
        num_vehicles=num_vehicles, 
        num_stations=num_stations, 
        random_seed=42  # ä¼ å…¥ç§å­ç¡®ä¿ç¯å¢ƒåˆå§‹åŒ–çš„ä¸€è‡´æ€§
    )
    
    print("âœ“ Fixed initial state setup: Vehicle positions and battery levels will be identical across all episodes")
    print("âœ“ Request generation will vary by episode for learning progression")
    
    # Initialize neural network-based ValueFunction for decision making only if needed
    # Use PyTorchChargingValueFunction with neural network only when ADP > 0 and assignmentgurobi is True
    use_neural_network = adpvalue > 0 and assignmentgurobi
    
    if use_neural_network:
        value_function = PyTorchChargingValueFunction(
            grid_size=env.grid_size, 
            num_vehicles=num_vehicles,
            device='cuda' if torch.cuda.is_available() else 'cpu',  # Use GPU if available
            episode_length=env.episode_length,  # ä¼ é€’æ­£ç¡®çš„episodeé•¿åº¦
            max_requests=10000,  # è®¾ç½®åˆç†çš„æœ€å¤§è¯·æ±‚æ•°
            env=env  # ä¼ é€’ç¯å¢ƒå¼•ç”¨
        )
        # Set the value function in the environment for Q-value calculation
        env.set_value_function(value_function)
    else:
        value_function = None
        
    env.adp_value = adpvalue
    env.use_intense_requests = use_intense_requests
    env.assignmentgurobi = assignmentgurobi
    env.heuristic_battery_threshold = heuristic_battery_threshold
    # Exploration parameters for enhanced learning with complex environment
    exploration_episodes = max(1, num_episodes // 2)  # Half episodes for exploration  
    epsilon_start = 0.4  # Higher exploration for complex environment
    epsilon_end = 0.1   # End with 10% random actions
    epsilon_decay = (epsilon_start - epsilon_end) / exploration_episodes
    
    # Enhanced training parameters for complex environment
    training_frequency = 2
    warmup_steps = 100     # Increased warmup for complex environment
    
    print(f"âœ“ Initialized environment with {num_vehicles} vehicles and {num_stations} charging stations")
    if use_neural_network:
        print(f"âœ“ Initialized PyTorchChargingValueFunction with neural network")
        print(f"   - Network parameters: {sum(p.numel() for p in value_function.network.parameters())}")
        print(f"âœ“ Enhanced exploration strategy: {exploration_episodes} episodes with epsilon {epsilon_start:.2f} â†’ {epsilon_end:.2f}")
        print(f"   - Training frequency: every {training_frequency} steps after {warmup_steps} warmup steps")
        print(f"   - Using device: {value_function.device}")
    else:
        print(f"âœ“ Neural network training disabled (ADP={adpvalue}, AssignmentGurobi={assignmentgurobi})")
        print(f"   - Running without neural network training")
    
    # Display vehicle type distribution
    ev_count = sum(1 for v in env.vehicles.values() if v['type'] == 'EV')
    aev_count = sum(1 for v in env.vehicles.values() if v['type'] == 'AEV')
    print(f"âœ“ Vehicle distribution: {ev_count} EV vehicles, {aev_count} AEV vehicles")
    
    # Test parameters
    num_episodes = num_episodes
    results = {
        'Idle_average': [],
        'episode_rewards': [],
        'charging_events': [],
        'episode_detailed_stats': [],  # New: detailed stats for each episode
        'vehicle_visit_stats': [],     # New: vehicle visit patterns for each episode
        'battery_levels': [],
        'environment_stats': [],
        'value_function_losses': [],
        'qvalue_losses': []  # Added: to store all training losses
    }
    
    for episode in range(num_episodes):
        # ä¸ºæ¯ä¸ªepisodeè®¾ç½®è¯·æ±‚ç”Ÿæˆä¸“ç”¨çš„ç§å­ï¼Œç¡®ä¿è¯·æ±‚åºåˆ—çš„å¤šæ ·æ€§
        # ä½†ä¿æŒä¸åŒADPå€¼ä¸‹ç›¸åŒepisodeçš„è¯·æ±‚åºåˆ—ä¸€è‡´
        episode_seed = 32 + episode  # åŸºç¡€ç§å­42åŠ ä¸Šepisodeç¼–å·
        env.set_request_generation_seed(episode_seed)
        print(f"Episode {episode + 1}: Request generation seed set to {episode_seed}")
        
        current_epsilon = max(epsilon_end, epsilon_start - episode * epsilon_decay)
        use_exploration = False
        
        # Reset environment
        states = env.reset()
        episode_reward = 0
        episode_charging_events = []
        episode_losses = []
        
        Idle_list = []

        if episode % 10 == 0:
            # ä¿å­˜Q-networkå’Œtarget networkå‚æ•°åˆ°æœ¬åœ°
            if use_neural_network and value_function is not None:
                saved_paths = save_q_network_checkpoint(value_function, episode)
                if not saved_paths:
                    print(f"âŒ Episode {episode}: ä¿å­˜ç½‘ç»œå¤±è´¥")
            else:
                print(f"Episode {episode}: Neural network not available for saving")
        
        for step in range(env.episode_length):
            # Generate actions using ValueFunction
            actions = {}
            states_for_training = []
            actions_for_training = []
            current_requests = list(env.active_requests.values())
            actions, storeactions = env.simulate_motion(agents=[], current_requests=current_requests, rebalance=True)
            next_states, rewards, dur_rewards, done, info = env.step(actions,storeactions)
            # Debug: Output step statistics every 100 steps
            if step % 25 == 0:
                stats = env.get_stats()
                active_requests = len(env.active_requests) if hasattr(env, 'active_requests') else 0
                assigned_vehicles = len([v for v in env.vehicles.values() if v['assigned_request'] is not None])
                charging_vehicles = len([v for v in env.vehicles.values() if v['charging_station'] is not None])
                onboard = len([v for v in env.vehicles.values() if v['passenger_onboard'] is not None])
                idlecar = len([v for v in env.vehicles.values() if  v.get('idle_target') is not None ])
                waitcar = len([v for v in env.vehicles.values() if  v.get('is_stationary') is True ])
                movecharge = len([v for v in env.vehicles.values() if v.get('charging_target') is not None])
                target_location_v = len([v for v in env.vehicles.values() if v.get('target_location') is not None])
                idle_vehicles = len([v for v in env.vehicles.values() 
                                   if v['assigned_request'] is None and v['passenger_onboard'] is None and v['charging_station'] is None and v['target_location'] is None])
                step_reward = sum(rewards.values())
                print(f"Step {step}: Active requests: {active_requests}, Assigned: {assigned_vehicles}, Onboard: {onboard}, Charging: {charging_vehicles}, Idle: {idlecar}, waitcar: {waitcar}, movecharge: {movecharge}, Idle Vehicles: {idle_vehicles}, Step reward: {step_reward:.2f}")
                Idle_list.append(idle_vehicles)
                # Neural network monitoring (if using neural network)
                if use_neural_network and hasattr(value_function, 'training_losses') and value_function.training_losses:
                    recent_loss = value_function.training_losses[-1] if value_function.training_losses else 0.0
                    buffer_size = len(value_function.experience_buffer)
                    training_step = value_function.training_step
                    
                    # Sample some Q-values to show the actual raw values used by Gurobi
                    if buffer_size > 0:
                        # Get a sample Q-value to demonstrate what Gurobi actually uses
                        sample_vehicle_id = list(env.vehicles.keys())[0] if env.vehicles else 0
                        sample_location = list(env.vehicles.values())[0]['location'] if env.vehicles else 0
                        sample_battery = list(env.vehicles.values())[0]['battery'] if env.vehicles else 1.0
                        
                        try:
                            # Test different action types - these are the raw Q-values Gurobi uses
                            idle_q = value_function.get_idle_q_value(sample_vehicle_id, sample_location, sample_battery, current_time=step)
                            assign_q = value_function.get_q_value(sample_vehicle_id, "assign_1", sample_location, sample_location+1, current_time=step, battery_level=sample_battery)
                            charge_q = value_function.get_q_value(sample_vehicle_id, "charge_1", sample_location, sample_location+5, current_time=step, battery_level=sample_battery)
                            
                            print(f"  Neural Network Status:")
                            print(f"    Training step: {training_step}, Buffer: {buffer_size}, Recent loss: {recent_loss:.4f}")
                            print(f"    Raw Q-values (no normalization): Idle={idle_q:.3f}, Assign={assign_q:.3f}, Charge={charge_q:.3f}")
                            print(f"    Note: Gurobi uses these raw Q-values directly in optimization objective")
                            
                            # æ·»åŠ ç»éªŒæ•°æ®åˆ†æ
                            if step > 100 and step % 100 == 0:  # æ¯100æ­¥åˆ†æä¸€æ¬¡
                                exp_analysis = value_function.analyze_experience_data()
                                if exp_analysis:
                                    reward_stats = exp_analysis['reward_stats']
                                    action_stats = exp_analysis['action_stats']
                                    print(f"    ğŸ“Š Experience Data Analysis (last 100 steps):")
                                    print(f"      Reward Distribution: +{reward_stats['positive_ratio']:.1%} | 0{reward_stats['neutral_ratio']:.1%} | -{reward_stats['negative_ratio']:.1%}")
                                    print(f"      Mean Rewards: Overall={reward_stats['mean_reward']:.2f}, Assign={action_stats['assign_mean_reward']:.2f}, Charge={action_stats['charge_mean_reward']:.2f}, Idle={action_stats['idle_mean_reward']:.2f}")
                                    print(f"      Action Success Rates: Assign={action_stats['assign_positive_ratio']:.1%}, Charge={action_stats['charge_positive_ratio']:.1%}, Idle={action_stats['idle_positive_ratio']:.1%}")
                                    
                        except Exception as e:
                            print(f"  Neural Network Status: Training step: {training_step}, Buffer: {buffer_size}, Recent loss: {recent_loss:.4f}")
                            print(f"    Error getting sample Q-values: {e}")
                else:
                    print(f"  Neural Network: {'Not training yet' if use_neural_network else 'Disabled'}")
            
            # Note: Q-learning experience storage is now handled automatically in env.step()
            # This ensures consistency between traditional Q-table and neural network training
            
            # Enhanced training: much more frequent training for better learning (only if using neural network)
            if use_neural_network and len(value_function.experience_buffer) >= warmup_steps:
                # Train more frequently based on our new parameters
                if step % training_frequency == 0:
                    training_loss = value_function.train_step(batch_size=batch_size)  # Larger batch
                    if training_loss > 0:
                        episode_losses.append(training_loss)
                
            episode_reward += sum(rewards.values())
            episode_charging_events.extend(info.get('charging_events', []))
            
            if done:
                break
        results['Idle_average'].append(sum(Idle_list)/len(Idle_list) if Idle_list else 0)
        results['episode_rewards'].append(episode_reward)
        results['charging_events'].extend(episode_charging_events)
        results['value_function_losses'].append(np.mean(episode_losses) if episode_losses else 0.0)
        results['qvalue_losses'].extend(episode_losses)  # Fixed: extend instead of assign
        # Record environment statistics
        stats = env.get_stats()
        results['active_requests'] = stats['active_requests']
        results['environment_stats'].append(stats)
        results['battery_levels'].append(stats['average_battery'])
        results['completed_requests'] = stats['completed_requests']
        # Collect detailed episode statistics
        episode_stats = env.get_episode_stats()
        episode_stats['episode_number'] = episode + 1
        episode_stats['episode_reward'] = episode_reward
        episode_stats['charging_events_count'] = len(episode_charging_events)
        
        # Output rebalancing assignment statistics
        rebalancing_calls = episode_stats.get('total_rebalancing_calls', 0)
        total_assignments = episode_stats.get('total_rebalancing_assignments', 0)
        avg_assignments = episode_stats.get('avg_rebalancing_assignments_per_call', 0)
        avg_whole = episode_stats.get('avg_rebalancing_assignments_per_whole', 0)
        print(f"Episode {episode + 1} Completed:")
        print(f"  Reward: {episode_reward:.2f}")
        print(f"  Orders: Total={episode_stats['total_orders']}, Accepted={episode_stats['accepted_orders']}, Completed={episode_stats['completed_orders']}, Rejected={episode_stats['rejected_orders']}")
        print(f"  Battery: {episode_stats['avg_battery_level']:.2f}")
        print(f"  Rebalancing: Calls={rebalancing_calls}, Total Assignments={total_assignments}, Avg Assignments={avg_assignments:.2f}, Avg Rebalance Whole={avg_whole:.2f}")

        # Add neural network Q-value summary
        if use_neural_network:
            idle_q = episode_stats.get('sample_idle_q_value', 0.0)
            assign_q = episode_stats.get('sample_assign_q_value', 0.0)
            charge_q = episode_stats.get('sample_charge_q_value', 0.0)
            nn_loss = episode_stats.get('neural_network_loss', 0.0)
            print(f"  Neural Network: Loss={nn_loss:.4f}, Q-values(Gurobi): Idle={idle_q:.3f}, Assign={assign_q:.3f}, Charge={charge_q:.3f}")
        # Only record neural network metrics if using neural network
        if use_neural_network:
            episode_stats['neural_network_loss'] = np.mean(episode_losses) if episode_losses else 0.0
            episode_stats['neural_network_loss_std'] = np.std(episode_losses) if episode_losses else 0.0
            episode_stats['training_steps_in_episode'] = len(episode_losses)
            
            # Sample Q-values for different action types (actual values used by Gurobi)
            if len(value_function.experience_buffer) > 0:
                try:
                    sample_vehicle_id = list(env.vehicles.keys())[0] if env.vehicles else 0
                    sample_location = list(env.vehicles.values())[0]['location'] if env.vehicles else 0
                    sample_battery = list(env.vehicles.values())[0]['battery'] if env.vehicles else 1.0
                    
                    # Get sample Q-values for statistics
                    idle_q = value_function.get_idle_q_value(sample_vehicle_id, sample_location, sample_battery, current_time=env.current_time)
                    assign_q = value_function.get_q_value(sample_vehicle_id, "assign_1", sample_location, sample_location+1, current_time=env.current_time, battery_level=sample_battery)
                    charge_q = value_function.get_q_value(sample_vehicle_id, "charge_1", sample_location, sample_location+5, current_time=env.current_time, battery_level=sample_battery)
                    
                    episode_stats['sample_idle_q_value'] = idle_q
                    episode_stats['sample_assign_q_value'] = assign_q
                    episode_stats['sample_charge_q_value'] = charge_q
                    
                except Exception as e:
                    episode_stats['sample_idle_q_value'] = 0.0
                    episode_stats['sample_assign_q_value'] = 0.0
                    episode_stats['sample_charge_q_value'] = 0.0
            else:
                episode_stats['sample_idle_q_value'] = 0.0
                episode_stats['sample_assign_q_value'] = 0.0
                episode_stats['sample_charge_q_value'] = 0.0
        else:
            episode_stats['neural_network_loss'] = 0.0
            episode_stats['neural_network_loss_std'] = 0.0
            episode_stats['training_steps_in_episode'] = 0
            episode_stats['sample_idle_q_value'] = 0.0
            episode_stats['sample_assign_q_value'] = 0.0
            episode_stats['sample_charge_q_value'] = 0.0
        results['episode_detailed_stats'].append(episode_stats)
        
        # Analyze charging usage history for this episode
        if 'charging_usage_history' in episode_stats and episode_stats['charging_usage_history']:
            charging_history = episode_stats['charging_usage_history']
            avg_usage = sum(h['vehicles_per_station'] for h in charging_history) / len(charging_history)
            max_usage = max(h['vehicles_per_station'] for h in charging_history)
            min_usage = min(h['vehicles_per_station'] for h in charging_history)
            print(f"  Charging History: {len(charging_history)} time steps, Avg: {avg_usage:.2f}, Max: {max_usage:.2f}, Min: {min_usage:.2f} vehicles/station")
        
        # Analyze vehicle visit patterns for this episode
        vehicle_visit_stats = analyze_vehicle_visit_patterns(env)
        results['vehicle_visit_stats'].append(vehicle_visit_stats)
        

    print("\n=== Integration Test Complete ===")
    if use_neural_network:
        print(f"âœ“ Neural Network ValueFunction trained over {num_episodes} episodes")
        print(f"âœ“ Final average training loss: {np.mean(results['value_function_losses']):.4f}")
        print(f"âœ“ Neural network has {sum(p.numel() for p in value_function.network.parameters())} parameters")
    else:
        print(f"âœ“ Test completed without neural network training")
        print(f"âœ“ Used traditional Q-table approach")
    
    # Create results directory for analysis - choose directory based on assignmentgurobi
    if assignmentgurobi:
        results_dir = Path("results/integrated_tests")
    else:
        results_dir = Path("results/integrated_tests_h")
    results_dir.mkdir(parents=True, exist_ok=True)
    print(f"âœ“ Results will be saved to: {results_dir}")
    
    # Save detailed episode statistics to Excel including vehicle visit patterns
    excel_path, spatial_path = save_episode_stats_to_excel(env, results['episode_detailed_stats'], results_dir, results.get('vehicle_visit_stats'))
    
    # Store file paths in results for reference
    results['excel_path'] = excel_path
    results['spatial_image_path'] = spatial_path
    
    return results, env








def analyze_vehicle_visit_patterns(env):
    """Analyze vehicle visit patterns and identify most frequently visited locations"""
    vehicle_visit_stats = {}
    
    # Define hotspot locations for reference
    hotspots = [
        (env.grid_size // 4, env.grid_size // 4),           # Bottom-left hotspot
        (3 * env.grid_size // 4, env.grid_size // 4),       # Bottom-right hotspot
        (env.grid_size // 2, 3 * env.grid_size // 4)        # Top-center hotspot
    ]
    
    for vehicle_id, vehicle in env.vehicles.items():
        # Get position history for this vehicle
        position_history = env.vehicle_position_history.get(vehicle_id, [])
        
        if not position_history:
            # If no history, use current position
            current_coords = vehicle['coordinates']
            location_counts = {str(current_coords): 1}
        else:
            # Count visits to each location
            location_counts = {}
            for entry in position_history:
                coords_str = str(entry['coords'])
                location_counts[coords_str] = location_counts.get(coords_str, 0) + 1
        
        # Find most visited location
        if location_counts:
            most_visited_location = max(location_counts, key=location_counts.get)
            most_visited_coords = eval(most_visited_location)
            visit_count = location_counts[most_visited_location]
            
            # Calculate location diversity (number of unique locations visited)
            unique_locations = len(location_counts)
            total_visits = sum(location_counts.values())
            diversity_score = unique_locations / total_visits if total_visits > 0 else 0
            
            # Calculate average distance from hotspots
            avg_distance_from_hotspots = 0
            hotspot_visits = 0
            for location_str, count in location_counts.items():
                coords = eval(location_str)
                min_distance_to_hotspot = min(
                    abs(coords[0] - hx) + abs(coords[1] - hy) 
                    for hx, hy in hotspots
                )
                avg_distance_from_hotspots += min_distance_to_hotspot * count
                
                # Check if in hotspot area (within 2 grid units)
                if min_distance_to_hotspot <= 2:
                    hotspot_visits += count
            
            avg_distance_from_hotspots = avg_distance_from_hotspots / total_visits if total_visits > 0 else 0
            hotspot_time_percentage = (hotspot_visits / total_visits * 100) if total_visits > 0 else 0
            
            # Get top 3 most visited locations
            sorted_locations = sorted(location_counts.items(), key=lambda x: x[1], reverse=True)
            top_3_locations = [f"{loc}({count})" for loc, count in sorted_locations[:3]]
            
            vehicle_visit_stats[vehicle_id] = {
                'vehicle_type': vehicle['type'],
                'most_visited_location': most_visited_location,
                'most_visited_coords': most_visited_coords,
                'visit_count': visit_count,
                'unique_locations': unique_locations,
                'diversity_score': round(diversity_score, 3),
                'avg_distance_from_hotspots': round(avg_distance_from_hotspots, 2),
                'hotspot_time_percentage': round(hotspot_time_percentage, 1),
                'top_3_locations': ', '.join(top_3_locations),
                'location_counts': location_counts
            }
        else:
            # Fallback for vehicles with no data
            vehicle_visit_stats[vehicle_id] = {
                'vehicle_type': vehicle['type'],
                'most_visited_location': 'N/A',
                'most_visited_coords': vehicle['coordinates'],
                'visit_count': 0,
                'unique_locations': 0,
                'diversity_score': 0.0,
                'avg_distance_from_hotspots': 0.0,
                'hotspot_time_percentage': 0.0,
                'top_3_locations': 'N/A',
                'location_counts': {}
            }
    
    return vehicle_visit_stats


def print_vehicle_visit_summary(vehicle_visit_stats_list):
    """Print summary of vehicle visit patterns across all episodes"""
    if not vehicle_visit_stats_list:
        print("âš  No vehicle visit data available")
        return
    
    print("\n" + "="*60)
    print("ğŸš— è½¦è¾†è®¿é—®æ¨¡å¼æ€»ç»“")
    print("="*60)
    
    # Aggregate statistics across all episodes
    all_vehicles_data = {}
    location_popularity = {}
    
    for episode_visits in vehicle_visit_stats_list:
        for vehicle_id, visit_info in episode_visits.items():
            if vehicle_id not in all_vehicles_data:
                all_vehicles_data[vehicle_id] = {
                    'vehicle_type': visit_info['vehicle_type'],
                    'total_visits': 0,
                    'total_unique_locations': 0,
                    'total_hotspot_time': 0,
                    'episodes_count': 0
                }
            
            data = all_vehicles_data[vehicle_id]
            data['total_visits'] += visit_info['visit_count']
            data['total_unique_locations'] += visit_info['unique_locations']
            data['total_hotspot_time'] += visit_info['hotspot_time_percentage']
            data['episodes_count'] += 1
            
            # Track location popularity
            for location, count in visit_info.get('location_counts', {}).items():
                if location not in location_popularity:
                    location_popularity[location] = 0
                location_popularity[location] += count
    
    # Vehicle type analysis
    ev_vehicles = {vid: data for vid, data in all_vehicles_data.items() if data['vehicle_type'] == 'EV'}
    aev_vehicles = {vid: data for vid, data in all_vehicles_data.items() if data['vehicle_type'] == 'AEV'}
    
    print(f"ğŸ“ˆ è½¦è¾†ç±»å‹ç»Ÿè®¡:")
    print(f"   EVè½¦è¾†æ•°é‡: {len(ev_vehicles)}")
    print(f"   AEVè½¦è¾†æ•°é‡: {len(aev_vehicles)}")
    
    # Calculate averages
    if ev_vehicles:
        avg_ev_hotspot_time = np.mean([data['total_hotspot_time'] / data['episodes_count'] 
                                      for data in ev_vehicles.values()])
        print(f"   EVå¹³å‡çƒ­ç‚¹åŒºåŸŸæ—¶é—´: {avg_ev_hotspot_time:.1f}%")
    
    if aev_vehicles:
        avg_aev_hotspot_time = np.mean([data['total_hotspot_time'] / data['episodes_count'] 
                                       for data in aev_vehicles.values()])
        print(f"   AEVå¹³å‡çƒ­ç‚¹åŒºåŸŸæ—¶é—´: {avg_aev_hotspot_time:.1f}%")
    
    # Most popular locations
    if location_popularity:
        print(f"\nğŸ“ æœ€å—æ¬¢è¿çš„ä½ç½® (å‰10å):")
        sorted_locations = sorted(location_popularity.items(), key=lambda x: x[1], reverse=True)
        for i, (location, visits) in enumerate(sorted_locations[:10], 1):
            coords = eval(location) if isinstance(location, str) and '(' in location else location
            print(f"   {i:2d}. {coords}: {visits} æ¬¡è®¿é—®")
    
    # Vehicle mobility analysis
    print(f"\nğŸš› è½¦è¾†ç§»åŠ¨æ€§åˆ†æ:")
    if all_vehicles_data:
        avg_unique_locations = np.mean([data['total_unique_locations'] / data['episodes_count'] 
                                       for data in all_vehicles_data.values()])
        avg_visits_per_episode = np.mean([data['total_visits'] / data['episodes_count'] 
                                         for data in all_vehicles_data.values()])
        
        print(f"   å¹³å‡æ¯episodeè®¿é—®çš„ä¸åŒä½ç½®æ•°: {avg_unique_locations:.1f}")
        print(f"   å¹³å‡æ¯episodeæ€»è®¿é—®æ¬¡æ•°: {avg_visits_per_episode:.1f}")
        
        # Identify most and least mobile vehicles
        mobility_scores = {vid: data['total_unique_locations'] / data['episodes_count'] 
                          for vid, data in all_vehicles_data.items()}
        
        most_mobile = max(mobility_scores, key=mobility_scores.get)
        least_mobile = min(mobility_scores, key=mobility_scores.get)
        
        print(f"   æœ€æ´»è·ƒè½¦è¾†: Vehicle {most_mobile} ({mobility_scores[most_mobile]:.1f} ä¸ªä¸åŒä½ç½®/episode)")
        print(f"   æœ€ä¸æ´»è·ƒè½¦è¾†: Vehicle {least_mobile} ({mobility_scores[least_mobile]:.1f} ä¸ªä¸åŒä½ç½®/episode)")


def save_episode_stats_to_excel(env, episode_stats, results_dir, vehicle_visit_stats=None):
    """Save detailed episode statistics to Excel file including vehicle visit patterns, ADP values, and spatial analysis"""
    if not episode_stats:
        print("âš  No episode statistics to save")
        return
    
    # Create DataFrame from episode statistics
    df = pd.DataFrame(episode_stats)
    
    # Extract ADP value and demand pattern information
    adpvalue = getattr(env, 'adp_value', 1.0)
    demand_pattern = "intense" if getattr(env, 'use_intense_requests', True) else "random"
    charging_penalty = getattr(env, 'charging_penalty', 2.0)
    unserved_penalty = getattr(env, 'unserved_penalty', 1.5)
    
    # Add timestamp to filename
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    excel_filename = f"episode_statistics_adp{adpvalue}_demand{demand_pattern}_{env.heuristic_battery_threshold}_{timestamp}.xlsx"
    excel_path = results_dir / excel_filename
    
    # Generate spatial visualization
    spatial_image_path = results_dir / f"spatial_analysis_adp{adpvalue}_demand{demand_pattern}_{timestamp}.png"
    
    try:
        # Create Excel writer with multiple sheets
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            # Main statistics sheet
            df.to_excel(writer, sheet_name='Episode_Statistics', index=False)
            
            # ADP Configuration sheet
            adp_config_data = {
                'Parameter': [
                    'ADP_Value',
                    'Demand_Pattern',
                    'Charging_Penalty',
                    'Unserved_Penalty',
                    'Grid_Size',
                    'Number_of_Vehicles',
                    'Number_of_Stations',
                    'Episode_Length',
                    'Request_Generation_Rate',
                    'Vehicle_Types',
                    'Hotspot_Configuration'
                ],
                'Value': [
                    adpvalue,
                    demand_pattern,
                    charging_penalty,
                    unserved_penalty,
                    env.grid_size,
                    env.num_vehicles,
                    env.num_stations,
                    env.episode_length,
                    env.request_generation_rate,
                    f"EV: {sum(1 for v in env.vehicles.values() if v['type'] == 'EV')}, AEV: {sum(1 for v in env.vehicles.values() if v['type'] == 'AEV')}",
                    "3 hotspots with weights [0.6, 0.3, 0.1]" if demand_pattern == "intense" else "Random distribution"
                ],
                'Description': [
                    'Weight for Q-value contribution in optimization',
                    'Request generation pattern (intense=hotspots, random=uniform)',
                    'Penalty coefficient for charging actions',
                    'Penalty coefficient for unserved requests',
                    'Size of the simulation grid',
                    'Total number of vehicles in simulation',
                    'Total number of charging stations',
                    'Length of each episode in time steps',
                    'Probability of generating new request each step',
                    'Distribution of vehicle types',
                    'Spatial distribution pattern for request generation'
                ]
            }
            
            adp_config_df = pd.DataFrame(adp_config_data)
            adp_config_df.to_excel(writer, sheet_name='ADP_Configuration', index=False)
            
            # Demand Pattern Analysis sheet
            if hasattr(env, 'request_generation_history') and env.request_generation_history:
                demand_data = []
                hotspot_counts = {0: 0, 1: 0, 2: 0}  # Track requests per hotspot
                
                for req_info in env.request_generation_history:
                    hotspot_idx = req_info.get('hotspot_idx', -1)
                    if hotspot_idx in hotspot_counts:
                        hotspot_counts[hotspot_idx] += 1
                    
                    demand_data.append({
                        'Pickup_X': req_info['pickup_coords'][0],
                        'Pickup_Y': req_info['pickup_coords'][1],
                        'Dropoff_X': req_info['dropoff_coords'][0],
                        'Dropoff_Y': req_info['dropoff_coords'][1],
                        'Hotspot_Index': hotspot_idx,
                        'Generation_Time': req_info['time']
                    })
                
                if demand_data:
                    demand_df = pd.DataFrame(demand_data)
                    demand_df.to_excel(writer, sheet_name='Demand_Pattern', index=False)
                    
                    # Hotspot statistics
                    total_requests = len(demand_data)
                    hotspot_stats = []
                    for hotspot_id, count in hotspot_counts.items():
                        percentage = (count / total_requests * 100) if total_requests > 0 else 0
                        hotspot_stats.append({
                            'Hotspot_ID': hotspot_id,
                            'Request_Count': count,
                            'Percentage': f"{percentage:.1f}%",
                            'Expected_Percentage': ['60%', '30%', '10%'][hotspot_id] if hotspot_id < 3 else 'N/A'
                        })
                    
                    hotspot_stats_df = pd.DataFrame(hotspot_stats)
                    hotspot_stats_df.to_excel(writer, sheet_name='Hotspot_Statistics', index=False)
            
            # Vehicle Visit Patterns sheet
            if vehicle_visit_stats:
                visit_data = []
                for episode_idx, episode_visits in enumerate(vehicle_visit_stats):
                    for vehicle_id, visit_info in episode_visits.items():
                        visit_data.append({
                            'Episode': episode_idx + 1,
                            'Vehicle_ID': vehicle_id,
                            'Vehicle_Type': visit_info.get('vehicle_type', 'Unknown'),
                            'Most_Visited_Location': visit_info.get('most_visited_location', 'N/A'),
                            'Most_Visited_Coords': visit_info.get('most_visited_coords', 'N/A'),
                            'Visit_Count': visit_info.get('visit_count', 0),
                            'Total_Unique_Locations': visit_info.get('unique_locations', 0),
                            'Location_Diversity_Score': visit_info.get('diversity_score', 0.0),
                            'Average_Distance_from_Hotspots': visit_info.get('avg_distance_from_hotspots', 0.0),
                            'Time_in_Hotspot_Areas_%': visit_info.get('hotspot_time_percentage', 0.0),
                            'Top_3_Visited_Locations': visit_info.get('top_3_locations', 'N/A')
                        })
                
                if visit_data:
                    visit_df = pd.DataFrame(visit_data)
                    visit_df.to_excel(writer, sheet_name='Vehicle_Visit_Patterns', index=False)
                
                # Location Heatmap Summary
                location_summary = {}
                for episode_visits in vehicle_visit_stats:
                    for vehicle_id, visit_info in episode_visits.items():
                        for location, count in visit_info.get('location_counts', {}).items():
                            if location not in location_summary:
                                location_summary[location] = {'total_visits': 0, 'vehicles_visited': set()}
                            location_summary[location]['total_visits'] += count
                            location_summary[location]['vehicles_visited'].add(vehicle_id)
                
                if location_summary:
                    heatmap_data = []
                    for location, info in location_summary.items():
                        coords = eval(location) if isinstance(location, str) and '(' in location else location
                        heatmap_data.append({
                            'Location_Coords': coords,
                            'Total_Visits': info['total_visits'],
                            'Unique_Vehicles_Visited': len(info['vehicles_visited']),
                            'Average_Visits_per_Vehicle': info['total_visits'] / len(info['vehicles_visited']) if info['vehicles_visited'] else 0
                        })
                    
                    heatmap_df = pd.DataFrame(heatmap_data)
                    heatmap_df = heatmap_df.sort_values('Total_Visits', ascending=False)
                    heatmap_df.to_excel(writer, sheet_name='Location_Heatmap', index=False)
            
            # Summary statistics sheet
            summary_stats = {
                'Metric': [
                    'Total Episodes',
                    'Average Orders per Episode',
                    'Average Accepted Orders per Episode',
                    'Average Rejected Orders per Episode',
                    'Overall Rejection Rate (%)',
                    'Average Battery Level',
                    'Total EV Vehicles',
                    'Total AEV Vehicles',
                    'EV Rejection Rate (%)',
                    'AEV Rejection Rate (%)',
                    'Average Neural Network Loss',
                    'Neural Network Loss Std Dev',
                    'Average Training Steps per Episode'
                ],
                'Value': [
                    len(df),
                    df['total_orders'].mean(),
                    df['accepted_orders'].mean(),
                    df['rejected_orders'].mean(),
                    (df['rejected_orders'].sum() / df['total_orders'].sum() * 100) if df['total_orders'].sum() > 0 else 0,
                    df['avg_battery_level'].mean(),
                    df['ev_count'].iloc[0] if not df.empty else 0,
                    df['aev_count'].iloc[0] if not df.empty else 0,
                    (df['ev_rejected'].sum() / (df['accepted_orders'].sum() + df['ev_rejected'].sum()) * 100) if (df['accepted_orders'].sum() + df['ev_rejected'].sum()) > 0 else 0,
                    (df['aev_rejected'].sum() / (df['accepted_orders'].sum() + df['aev_rejected'].sum()) * 100) if (df['accepted_orders'].sum() + df['aev_rejected'].sum()) > 0 else 0,
                    df['neural_network_loss'].mean() if 'neural_network_loss' in df.columns else 0,
                    df['neural_network_loss_std'].mean() if 'neural_network_loss_std' in df.columns else 0,
                    df['training_steps_in_episode'].mean() if 'training_steps_in_episode' in df.columns else 0
                ]
            }
            
            summary_df = pd.DataFrame(summary_stats)
            summary_df.to_excel(writer, sheet_name='Summary', index=False)
            
            # Vehicle type comparison sheet
            if not df.empty:
                vehicle_comparison = pd.DataFrame({
                    'Vehicle_Type': ['EV', 'AEV'],
                    'Count': [df['ev_count'].iloc[0], df['aev_count'].iloc[0]],
                    'Total_Rejected_Orders': [df['ev_rejected'].sum(), df['aev_rejected'].sum()],
                    'Rejection_Rate_%': [
                        (df['ev_rejected'].sum() / (df['accepted_orders'].sum() + df['ev_rejected'].sum()) * 100) if (df['accepted_orders'].sum() + df['ev_rejected'].sum()) > 0 else 0,
                        (df['aev_rejected'].sum() / (df['accepted_orders'].sum() + df['aev_rejected'].sum()) * 100) if (df['accepted_orders'].sum() + df['aev_rejected'].sum()) > 0 else 0
                    ]
                })
                vehicle_comparison.to_excel(writer, sheet_name='Vehicle_Comparison', index=False)
        
        # Generate and save spatial visualization
        try:
            spatial_viz = SpatialVisualization(env.grid_size)
            success = spatial_viz.create_comprehensive_spatial_plot(
                env=env, 
                save_path=spatial_image_path,
                adpvalue=adpvalue,
                demand_pattern=demand_pattern
            )
            
            if success:
                print(f"âœ“ Spatial visualization saved: {spatial_image_path}")
            else:
                print(f"âš  Failed to generate spatial visualization")
            
        except Exception as e:
            print(f"âš  Error generating spatial visualization: {e}")
        
        print(f"âœ“ Episode statistics saved to Excel: {excel_path}")
        print(f"  - Episode_Statistics: Detailed data for each episode")
        print(f"  - ADP_Configuration: System parameters and settings")
        print(f"  - Demand_Pattern: Request generation analysis")
        print(f"  - Hotspot_Statistics: Hotspot performance metrics")
        print(f"  - Summary: Overall performance metrics")
        print(f"  - Vehicle_Comparison: EV vs AEV performance comparison")
        if vehicle_visit_stats:
            print(f"  - Vehicle_Visit_Patterns: Individual vehicle movement analysis")
            print(f"  - Location_Heatmap: Aggregated location popularity")
        
        return excel_path, spatial_image_path
        print(f"  - Summary: Overall performance metrics")
        print(f"  - Vehicle_Comparison: EV vs AEV performance comparison")
        
    except Exception as e:
        print(f"âŒ Error saving Excel file: {e}")
        # Save as CSV as backup
        csv_path = results_dir / f"episode_statistics_{timestamp}.csv"
        df.to_csv(csv_path, index=False)
        print(f"âœ“ Backup saved as CSV: {csv_path}")


def analyze_results(results):
    """Analyze test results including EV/AEV behavior"""
    print("\n=== Enhanced Results Analysis ===")
    
    # Basic statistics
    total_episodes = len(results['episode_rewards'])
    avg_reward = np.mean(results['episode_rewards'])
    total_charging = len(results['charging_events'])
    avg_battery = np.mean(results['battery_levels'])
    
    print(f"Total episodes: {total_episodes}")
    print(f"Average reward: {avg_reward:.2f}")
    print(f"Total charging events: {total_charging}")
    print(f"Average battery level: {avg_battery:.2f}")
    
    # Vehicle type analysis
    if 'environment_stats' in results and results['environment_stats']:
        latest_stats = results['environment_stats'][-1]
        ev_count = latest_stats.get('ev_count', 0)
        aev_count = latest_stats.get('aev_count', 0)
        total_rejected = latest_stats.get('total_rejected_requests', 0)
        ev_rejected = latest_stats.get('ev_rejected_requests', 0)
        aev_rejected = latest_stats.get('aev_rejected_requests', 0)
        
        print(f"\nVehicle Type Analysis:")
        print(f"  EV vehicles: {ev_count}")
        print(f"  AEV vehicles: {aev_count}")
        print(f"  Total rejected requests: {total_rejected}")
        print(f"  EV rejected requests: {ev_rejected}")
        print(f"  AEV rejected requests: {aev_rejected}")
        
        if ev_count > 0:
            ev_rejection_rate = ev_rejected / max(1, ev_rejected + latest_stats.get('completed_requests', 0))
            print(f"  EV rejection rate: {ev_rejection_rate:.2%}")
        
        if aev_count > 0:
            aev_rejection_rate = aev_rejected / max(1, aev_rejected + latest_stats.get('completed_requests', 0))
            print(f"  AEV rejection rate: {aev_rejection_rate:.2%}")
    
    # Request fulfillment analysis
    if 'environment_stats' in results and results['environment_stats']:
        completed_requests = sum(stats.get('completed_requests', 0) for stats in results['environment_stats'])
        total_earnings = sum(stats.get('total_earnings', 0) for stats in results['environment_stats'])
        avg_fulfillment = np.mean([stats.get('request_fulfillment_rate', 0) for stats in results['environment_stats']])
        
        print(f"\nRequest Fulfillment Analysis:")
        print(f"  Total completed requests: {completed_requests}")
        print(f"  Total earnings: {total_earnings:.2f}")
        print(f"  Average fulfillment rate: {avg_fulfillment:.2%}")
    
    # Charging behavior analysis
    if results['charging_events']:
        station_usage = defaultdict(int)
        duration_stats = []
        
        for event in results['charging_events']:
            station_usage[event['station_id']] += 1
            duration_stats.append(event['duration'])
        
        print(f"\nCharging Station Usage Statistics:")
        for station_id, count in station_usage.items():
            print(f"  Station {station_id}: {count} times")
        
        print(f"Average charging duration: {np.mean(duration_stats):.1f}")
        print(f"Max charging duration: {max(duration_stats)}")
        print(f"Min charging duration: {min(duration_stats)}")
    
    # Analyze charging usage history across all episodes
    if 'episode_detailed_stats' in results:
        all_usage_data = []
        for episode_stats in results['episode_detailed_stats']:
            if 'charging_usage_history' in episode_stats and episode_stats['charging_usage_history']:
                for usage_point in episode_stats['charging_usage_history']:
                    all_usage_data.append(usage_point['vehicles_per_station'])
        
        if all_usage_data:
            print(f"\nOverall Charging Station Usage Analysis:")
            print(f"  Total time steps recorded: {len(all_usage_data)}")
            print(f"  Average vehicles per station: {np.mean(all_usage_data):.3f}")
            print(f"  Maximum vehicles per station: {max(all_usage_data):.3f}")
            print(f"  Minimum vehicles per station: {min(all_usage_data):.3f}")
            print(f"  Standard deviation: {np.std(all_usage_data):.3f}")
        else:
            print(f"\nNo charging usage history data found across episodes")
    
    # Learning curve analysis
    improvement = 0
    if len(results['episode_rewards']) > 10:
        early_rewards = results['episode_rewards'][:10]
        late_rewards = results['episode_rewards'][-10:]
        improvement = np.mean(late_rewards) - np.mean(early_rewards)
        print(f"\nReward improvement: {improvement:.2f}")
        
        if improvement > 0:
            print("âœ“ Shows learning improvement trend")
        else:
            print("âš  Learning effectiveness needs improvement")
    
    # Battery management assessment
    if results['battery_levels']:
        min_battery = min(results['battery_levels'])
        max_battery = max(results['battery_levels'])
        battery_stability = np.std(results['battery_levels'])
        
        print(f"\nBattery Management Analysis:")
        print(f"  Lowest average battery: {min_battery:.2f}")
        print(f"  Highest average battery: {max_battery:.2f}")
        print(f"  Battery stability (std dev): {battery_stability:.3f}")
        
        if min_battery > 0.2:
            print("âœ“ Good battery management, no severe low battery issues")
        else:
            print("âš  Risk of critically low battery levels")
    
    return {
        'avg_reward': avg_reward,
        'total_charging': total_charging,
        'avg_battery': avg_battery,
        'improvement': improvement,
        'min_battery': min_battery if results['battery_levels'] else 0,
        'battery_stability': battery_stability if results['battery_levels'] else 0
    }


def visualize_integrated_results(env,results, assignmentgurobi=True):
    """å¯è§†åŒ–é›†æˆæµ‹è¯•ç»“æœ"""
    print("\n=== ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ ===")
    
    try:
        # åˆ›å»ºå¯è§†åŒ–å™¨
        visualizer = ChargingIntegrationVisualization(figsize=(15, 10))
        
        # ä¿å­˜è·¯å¾„ - æ ¹æ®assignmentgurobié€‰æ‹©ç›®å½•
        if assignmentgurobi:
            results_dir = Path("results/integrated_tests")
        else:
            results_dir = Path("results/integrated_tests_h")
        results_dir.mkdir(parents=True, exist_ok=True)
        
        adpvalue = getattr(env, 'adp_value', 1.0)
        plot_path = results_dir / f"integrated_charging_results_{adpvalue}.png"
        fig1 = visualizer.plot_integrated_results(results,  save_path=str(plot_path))

        # ç”Ÿæˆç­–ç•¥åˆ†æå›¾è¡¨
        strategy_plot_path = results_dir / f"charging_strategy_analysis_{adpvalue}.png"
        fig2 = visualizer.plot_charging_strategy_analysis(results, save_path=str(strategy_plot_path))

        print(f"âœ“ ä¸»è¦ç»“æœå›¾è¡¨å·²ä¿å­˜è‡³: {plot_path}")
        print(f"âœ“ ç­–ç•¥åˆ†æå›¾è¡¨å·²ä¿å­˜è‡³: {strategy_plot_path}")
        
        # å…³é—­å›¾è¡¨ä»¥é‡Šæ”¾å†…å­˜
        plt.close(fig1)
        plt.close(fig2)
        
        return True
            
    except Exception as e:
        print(f"âŒ å¯è§†åŒ–å¤±è´¥: {e}")
        return False


def generate_integration_report(results, analysis, assignmentgurobi=True):
    """ç”Ÿæˆé›†æˆæµ‹è¯•æŠ¥å‘Š"""
    print("\n=== ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š ===")
    
    # æ ¹æ®assignmentgurobié€‰æ‹©ç›®å½•
    if assignmentgurobi:
        results_dir = Path("results/integrated_tests")
    else:
        results_dir = Path("results/integrated_tests_h")
    results_dir.mkdir(parents=True, exist_ok=True)
    
    report_path = results_dir / "integration_report.txt"
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("å……ç”µè¡Œä¸ºé›†æˆæµ‹è¯•æŠ¥å‘Š\n")
        f.write("=" * 50 + "\n\n")
        
        f.write("æµ‹è¯•æ¦‚å†µ:\n")
        f.write(f"- æ€»å›åˆæ•°: {len(results['episode_rewards'])}\n")
        f.write(f"- å¹³å‡å¥–åŠ±: {analysis['avg_reward']:.2f}\n")
        f.write(f"- æ€»å……ç”µæ¬¡æ•°: {analysis['total_charging']}\n")
        f.write(f"- å¹³å‡ç”µæ± ç”µé‡: {analysis['avg_battery']:.2f}\n")
        f.write(f"- å¥–åŠ±æ”¹è¿›: {analysis['improvement']:.2f}\n")

        f.write(f"- æœ€ä½ç”µé‡: {analysis['min_battery']:.2f}\n")
        f.write(f"- ç”µé‡ç¨³å®šæ€§: {analysis['battery_stability']:.3f}\n\n")
        
        f.write("å……ç”µè¡Œä¸ºè¯„ä¼°:\n")
        if analysis['total_charging'] > 0:
            f.write("âœ“ å……ç”µåŠŸèƒ½æ­£å¸¸å·¥ä½œ\n")
            avg_charging_per_episode = analysis['total_charging'] / len(results['episode_rewards'])
            f.write(f"âœ“ å¹³å‡æ¯å›åˆ {avg_charging_per_episode:.1f} æ¬¡å……ç”µ\n")
            
            if avg_charging_per_episode > 3:
                f.write("âœ“ å……ç”µé¢‘ç‡åˆç†\n")
            else:
                f.write("âš  å……ç”µé¢‘ç‡å¯èƒ½åä½\n")
        else:
            f.write("âŒ æœªæ£€æµ‹åˆ°å……ç”µè¡Œä¸º\n")
        
        if analysis['avg_battery'] > 0.4:
            f.write("âœ“ ç”µæ± ç®¡ç†ä¼˜ç§€\n")
        elif analysis['avg_battery'] > 0.25:
            f.write("âœ“ ç”µæ± ç®¡ç†è‰¯å¥½\n")
        else:
            f.write("âš  ç”µæ± ç®¡ç†éœ€è¦æ”¹è¿›\n")
        
        if analysis['improvement'] > 5:
            f.write("âœ“ å­¦ä¹ æ•ˆæœæ˜¾è‘—\n")
        elif analysis['improvement'] > 0:
            f.write("âœ“ å­¦ä¹ æ•ˆæœè‰¯å¥½\n")
        else:
            f.write("âš  å­¦ä¹ æ•ˆæœå¾…æ”¹è¿›\n")
        
        f.write(f"\nå……ç”µç­–ç•¥è´¨é‡è¯„ä¼°:\n")
        if analysis['min_battery'] > 0.15:
            f.write("âœ“ å¾ˆå¥½åœ°é¿å…äº†ç”µé‡å±æœº\n")
        else:
            f.write("âš  å­˜åœ¨ç”µé‡ç®¡ç†é£é™©\n")
            
        if analysis['battery_stability'] < 0.1:
            f.write("âœ“ ç”µé‡ç®¡ç†ç¨³å®š\n")
        else:
            f.write("âš  ç”µé‡æ³¢åŠ¨è¾ƒå¤§\n")
    
    print(f"âœ“ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")


def testtimeperformance(carnumlist):
    import time
    num_episodes = 100
    use_intense_requests = False
    config_manager = ConfigManager()
    print("ğŸ“‹ åŠ è½½é…ç½®å‚æ•°...")
    config_manager.print_config('training')
    config_manager.print_config('environment')
    
    # ä»é…ç½®è·å–å‚æ•°
    training_config = get_training_config()
    env_config = config_manager.get_environment_config()
    
    heuristictime = []
    ILPtimelist = []
    ADPtimelist = []
    for carnum in carnumlist:
        adpvalue = 0
        assignmentgurobi =False
        start_time = time.time()
        results, env = run_charging_integration_test_threshold(adpvalue,num_episodes,use_intense_requests,assignmentgurobi,batch_size=256, num_vehicles = carnum)
        end_time = time.time()
        
        heuristictime.append(end_time - start_time)
    np.savetxt("heuristictime.txt",heuristictime)
    assignmentgurobi =True
    for carnum in carnumlist:
        start_time = time.time()
        results, env = run_charging_integration_test(0, num_episodes=num_episodes, use_intense_requests=use_intense_requests, assignmentgurobi=assignmentgurobi, num_vehicles = carnum)
        end_time = time.time()
        ILPtimelist.append(end_time - start_time)
    np.savetxt("ILPtimelist.txt",ILPtimelist)   
    # for carnum in carnumlist:
    #     start_time = time.time()
    #     results, env = run_charging_integration_test(0.1, num_episodes=num_episodes, use_intense_requests=use_intense_requests, assignmentgurobi=assignmentgurobi, num_vehicles = carnum)
    #     end_time = time.time()
    #     ADPtimelist.append(end_time - start_time)
    # np.savetxt("ADPtimelist.txt",ADPtimelist)   
        
def main():


    print("ğŸš—âš¡ å……ç”µè¡Œä¸ºé›†æˆæµ‹è¯•ç¨‹åº")
    print("ä½¿ç”¨srcæ–‡ä»¶å¤¹ä¸­çš„Environmentå’Œå……ç”µç»„ä»¶")
    print("-" * 60)

    # åŠ è½½é…ç½®
    config_manager = ConfigManager()
    print("ğŸ“‹ åŠ è½½é…ç½®å‚æ•°...")
    config_manager.print_config('training')
    config_manager.print_config('environment')
    
    # ä»é…ç½®è·å–å‚æ•°
    training_config = get_training_config()
    env_config = config_manager.get_environment_config()
    charge_threshold = [0.3+i*0.1 for i in range(6)]
    use_intense_requests = False
    try:
        # ä»é…ç½®è·å–è®­ç»ƒå‚æ•°
        num_episodes = 100
        print(f"ğŸ“Š ä½¿ç”¨é…ç½®å‚æ•°: episodes={num_episodes}")
        
        # carnumlist = [i*5 for i in range(1,6)]
        # testtimeperformance(carnumlist)
        
        
        batch_size = training_config.get('batch_size', 256)
        # adpvalue = 0
        # assignmentgurobi =False
        # # for charge_th in charge_threshold:
        # charge_th = 0.5
        # results, env = run_charging_integration_test_threshold(adpvalue,num_episodes,use_intense_requests,assignmentgurobi,batch_size=256, heuristic_battery_threshold = charge_th)



        # print("\n" + "="*60)
        assignmentgurobi =True
        results_folder = "results/integrated_tests/" if assignmentgurobi else "results/integrated_tests_h/"
        print(f"ğŸ“ è¯·æ£€æŸ¥ {results_folder} æ–‡ä»¶å¤¹ä¸­çš„è¯¦ç»†ç»“æœ")
        print("="*60)
        adplist = [1]
        for adpvalue in adplist:
            assignment_type = "Gurobi" if assignmentgurobi else "Heuristic"
            print(f"\nâš¡ å¼€å§‹é›†æˆæµ‹è¯• (ADP={adpvalue}, Assignment={assignment_type})")
            results, env = run_charging_integration_test(adpvalue, num_episodes=num_episodes, use_intense_requests=use_intense_requests, assignmentgurobi=assignmentgurobi)

            # åˆ†æç»“æœ
            analysis = analyze_results(results)
            
            # ç”Ÿæˆå¯è§†åŒ–
            success = visualize_integrated_results(env, results, assignmentgurobi=assignmentgurobi)
            
            # ç©ºé—´åˆ†å¸ƒå¯è§†åŒ–å·²åœ¨Excelå¯¼å‡ºä¸­ç”Ÿæˆ
            print(f"\nğŸ—ºï¸  ç©ºé—´åˆ†å¸ƒåˆ†æå·²å®Œæˆï¼Œå›¾åƒè·¯å¾„: {results.get('spatial_image_path', 'N/A')}")
            
            # ç”Ÿæˆä¼ ç»Ÿçš„ç©ºé—´åˆ†å¸ƒåˆ†æï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰
            spatial_viz = SpatialVisualization(env.grid_size)
            spatial_analysis = spatial_viz.analyze_spatial_patterns(env, adp_value=adpvalue)
            spatial_viz.print_spatial_analysis(spatial_analysis)
            
            # ç”ŸæˆæŠ¥å‘Š
            generate_integration_report(results, analysis, assignmentgurobi=assignmentgurobi)
            
            # è¾“å‡ºè½¦è¾†è®¿é—®æ¨¡å¼æ€»ç»“
            print_vehicle_visit_summary(results.get('vehicle_visit_stats', []))
            
            print("\n" + "="*60)
            print(f"ğŸ‰ é›†æˆæµ‹è¯•å®Œæˆ! (ADP={adpvalue}, {assignment_type})")
            print("ğŸ“Š ç»“æœæ‘˜è¦:")
            print(f"   - å¹³å‡å¥–åŠ±: {analysis['avg_reward']:.2f}")
            print(f"   - å……ç”µæ¬¡æ•°: {analysis['total_charging']}")
            print(f"   - å¹³å‡ç”µé‡: {analysis['avg_battery']:.2f}")
            print(f"   - å¥–åŠ±æ”¹è¿›: {analysis['improvement']:.2f}")
            
            if success:
                print("ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨ç”ŸæˆæˆåŠŸ")
            
            results_folder = "results/integrated_tests/" if assignmentgurobi else "results/integrated_tests_h/"
            print(f"ğŸ“ è¯·æ£€æŸ¥ {results_folder} æ–‡ä»¶å¤¹ä¸­çš„è¯¦ç»†ç»“æœ")
            print("="*60)
        
        # ========================================
        # NEW WORKFLOW: EV-AEV Separate Training
        # ========================================
        # å–æ¶ˆä¸‹é¢çš„æ³¨é‡Šä»¥è¿è¡Œæ–°çš„ EV-AEV åˆ†å¼€è®­ç»ƒ workflow
        # print("\n" + "="*70)
        # print("ğŸš— å¼€å§‹æ–° Workflow: EV-AEV åˆ†å¼€è®­ç»ƒ")
        # print("="*70)
        # ev_aev_results, ev_aev_env = run_ev_aev_separate_training(
        #     adpvalue=1,
        #     num_episodes=num_episodes,
        #     use_intense_requests=use_intense_requests,
        #     batch_size=batch_size,
        #     num_vehicles=10
        # )
        # print(f"ğŸ“ EV-AEV åˆ†å¼€è®­ç»ƒç»“æœä¿å­˜åœ¨: results/ev_aev_separate/")
            
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()



# ä½¿ç”¨ç¤ºä¾‹å‡½æ•°
def example_usage_with_checkpoints():
    """
    å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ£€æŸ¥ç‚¹ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½çš„ç¤ºä¾‹
    """
    print("="*60)
    print("ğŸ“š Q-Networkæ£€æŸ¥ç‚¹ä¿å­˜å’ŒåŠ è½½ä½¿ç”¨ç¤ºä¾‹")
    print("="*60)
    
    print("\n1. åˆ—å‡ºå¯ç”¨çš„æ£€æŸ¥ç‚¹:")
    print("   checkpoints = list_available_checkpoints()")
    
    print("\n2. æ‰‹åŠ¨ä¿å­˜æ£€æŸ¥ç‚¹:")
    print("   # åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­")
    print("   if episode % 10 == 0 and use_neural_network:")
    print("       saved_paths = save_q_network_checkpoint(value_function, episode)")
    
    print("\n3. åŠ è½½æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ:")
    print("   # åœ¨åˆ›å»ºvalue_functionä¹‹å")
    print("   checkpoint_path = 'checkpoints/q_networks/full_state_episode_50.pth'")
    print("   success = load_q_network_checkpoint(value_function, checkpoint_path)")
    
    print("\n4. åœ¨æµ‹è¯•å‡½æ•°ä¸­è‡ªåŠ¨ä¿å­˜:")
    print("   # å½“å‰å·²é›†æˆï¼Œæ¯10ä¸ªepisodeè‡ªåŠ¨ä¿å­˜")
    print("   # run_charging_integration_test_threshold(...)")
    
    print("\n5. æ£€æŸ¥ç‚¹æ–‡ä»¶ç»“æ„:")
    print("   checkpoints/q_networks/")
    print("   â”œâ”€â”€ q_network_episode_X.pth         # ä¸»ç½‘ç»œæƒé‡")
    print("   â”œâ”€â”€ target_network_episode_X.pth    # ç›®æ ‡ç½‘ç»œæƒé‡")
    print("   â””â”€â”€ full_state_episode_X.pth        # å®Œæ•´è®­ç»ƒçŠ¶æ€")
    
    print("\nâœ“ æ£€æŸ¥ç‚¹åŠŸèƒ½å·²é›†æˆåˆ°æµ‹è¯•æµç¨‹ä¸­!")


def load_and_continue_training_example():
    """
    ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒçš„å®Œæ•´ç¤ºä¾‹
    """
    print("\n" + "="*50)
    print("ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒç¤ºä¾‹")
    print("="*50)
    
    # åˆ—å‡ºå¯ç”¨æ£€æŸ¥ç‚¹
    checkpoints = list_available_checkpoints()
    
    if checkpoints:
        # é€‰æ‹©æœ€æ–°çš„æ£€æŸ¥ç‚¹
        latest_episode, latest_checkpoint = checkpoints[-1]
        print(f"\nğŸ“‚ æœ€æ–°æ£€æŸ¥ç‚¹: Episode {latest_episode}")
        print(f"   è·¯å¾„: {latest_checkpoint}")
        
        print("\nğŸ’¡ è¦ä»æ­¤æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼Œè¯·:")
        print("1. åœ¨main()å‡½æ•°ä¸­æ·»åŠ æ£€æŸ¥ç‚¹åŠ è½½é€»è¾‘")
        print("2. è®¾ç½®checkpoint_pathä¸ºä¸Šè¿°è·¯å¾„")
        print("3. è¿è¡Œæµ‹è¯•ï¼Œç½‘ç»œå°†è‡ªåŠ¨ä»æ£€æŸ¥ç‚¹æ¢å¤")
    else:
        print("\nğŸ“­ æš‚æ— å¯ç”¨æ£€æŸ¥ç‚¹")
        print("   è¿è¡Œå‡ ä¸ªepisodeåä¼šè‡ªåŠ¨ç”Ÿæˆæ£€æŸ¥ç‚¹æ–‡ä»¶")


if __name__ == "__main__":
    # åœ¨ä¸»å‡½æ•°è¿è¡Œå‰æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
    example_usage_with_checkpoints()
    load_and_continue_training_example()
    
    # è¿è¡Œä¸»æµ‹è¯•
    main()